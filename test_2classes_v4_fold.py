#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SAMé€šç”¨ç—…ç¶æ‰¹é‡æµ‹è¯•è„šæœ¬ - æµ‹è¯•é›†æ•´ä½“æŒ‡æ ‡è®¡ç®—
åŠŸèƒ½ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹æ•´ä¸ªæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œè®¡ç®—ç—…ç¶Dice/IoUå’Œæ•´ä½“mDice/mIoUã€‚
è‡ªåŠ¨è¿‡æ»¤æµ‹è¯•é›†ï¼Œåªä¿ç•™åŒ…å«æŒ‡å®šIDçš„å›¾åƒï¼Œå¹¶ç»Ÿè®¡è¿‡æ»¤åæ•°é‡ã€‚
ä¸è¿›è¡Œå¯è§†åŒ–ï¼Œåªè¾“å‡ºæŒ‡æ ‡ç»“æœã€‚
"""

import os
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from segment_anything import sam_model_registry
from collections import defaultdict

# ===== ğŸ’ª é…ç½®ç±»ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰ =====
class TestConfig:
    IMAGE_SIZE = 1024
    NUM_CLASSES = 2
    BATCH_SIZE = 1  
    NUM_WORKERS = 4
    SAM_MODEL_TYPE = "vit_b"
    PIXEL_MEAN = [123.675, 116.28, 103.53]
    PIXEL_STD = [58.395, 57.12, 57.375]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    LESION_ID = 29  
    LESION_NAME = "å£°å¸¦ç™½æ–‘"  
    
    ID_MAPPING = {
        0: 0,          
        LESION_ID: 1,  
    }
    CLASS_NAMES = ["èƒŒæ™¯", LESION_NAME]
    
    LESION_CLASSES = [1]  # ç—…ç¶ç±»åˆ«

config = TestConfig()

class DSCEnhancedSAMModel(torch.nn.Module):
    """DSCå¢å¼ºSAMæ¨¡å‹ - ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´"""
    
    def __init__(self, sam_model, num_classes):
        super().__init__()
        self.sam = sam_model
        self.num_classes = num_classes
        
        # å¢å¼ºçš„åˆ†å‰²å¤´ - ä¸ºDSCä¼˜åŒ–
        self.segmentation_head = torch.nn.Sequential(
            torch.nn.Conv2d(256, 128, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(128),
            torch.nn.ReLU(inplace=True),
            torch.nn.Dropout2d(0.1),  # æ·»åŠ dropouté˜²è¿‡æ‹Ÿåˆ
            
            torch.nn.Conv2d(128, 64, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(64),
            torch.nn.ReLU(inplace=True),
            torch.nn.Dropout2d(0.05),
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            torch.nn.Conv2d(64, 32, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(32),
            torch.nn.ReLU(inplace=True),
            
            torch.nn.Conv2d(32, num_classes, kernel_size=1)
        )
        
        # å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶
        self.attention = torch.nn.Sequential(
            torch.nn.Conv2d(256, 64, kernel_size=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(64, 16, kernel_size=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(16, 1, kernel_size=1),
            torch.nn.Sigmoid()
        )
        
        # å¢å¼ºè¾¹ç•Œç»†åŒ–æ¨¡å— - ä¸“é—¨å¯¹æŠ—è¿‡åº¦åˆ†å‰²
        self.boundary_refine = torch.nn.Sequential(
            torch.nn.Conv2d(num_classes, 32, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(32),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(32, 16, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(16),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(16, num_classes, kernel_size=1),
            torch.nn.Tanh()  # ä½¿ç”¨tanhé™åˆ¶è¾“å‡ºèŒƒå›´
        )
        
        # å¢å¼ºè¾¹ç•Œæ”¶ç¼©æ¨¡å— - ä¸“é—¨å¯¹æŠ—HD=52.26pxä¸¥é‡åç§»
        self.boundary_contract = torch.nn.Sequential(
            torch.nn.Conv2d(num_classes, 16, kernel_size=5, padding=2),
            torch.nn.BatchNorm2d(16),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(16, 8, kernel_size=3, padding=1),
            torch.nn.Sigmoid(),
            torch.nn.Conv2d(8, num_classes, kernel_size=1)
        )
    
    def forward(self, images):
        batch_size = images.shape[0]
        
        image_embeddings = self.sam.image_encoder(images)
        
        attention_map = self.attention(image_embeddings)
        enhanced_features = image_embeddings * attention_map
        
        segmentation_logits = self.segmentation_head(enhanced_features)
        
        # è¾¹ç•Œç»†åŒ–å’Œæ”¶ç¼© - æ¿€è¿›ä¼˜åŒ–è¾¹ç•Œç²¾ç¡®æ€§
        boundary_refinement = self.boundary_refine(segmentation_logits)
        refined_logits = segmentation_logits + 0.06 * boundary_refinement  # è¿›ä¸€æ­¥é™ä½ç»†åŒ–æƒé‡
        
        # å¼ºåŒ–è¾¹ç•Œæ”¶ç¼© - å¯¹æŠ—HD=52.26pxä¸¥é‡åç§» 
        boundary_contraction = self.boundary_contract(refined_logits)
        refined_logits = refined_logits - 0.12 * boundary_contraction  # å¤§å¹…å¢åŠ æ”¶ç¼©åŠ›åº¦ (0.05â†’0.12)
        
        refined_logits = F.interpolate(
            refined_logits,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        
        return refined_logits

# ===== ğŸ¯ æµ‹è¯•æ•°æ®é›†ç±»ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— å¢å¼º/é‡‡æ ·ï¼‰ =====
class TestDataset(Dataset):
    def __init__(self, images_dir, masks_dir):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.image_files = [f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"æ‰¾åˆ° {len(self.image_files)} ä¸ªæµ‹è¯•å›¾åƒ")
        
        # è‡ªåŠ¨è¿‡æ»¤ï¼Œåªä¿ç•™åŒ…å«æŒ‡å®šLESION_IDçš„å›¾åƒ
        self.filter_lesion_only()
    
    def filter_lesion_only(self):
        """è¿‡æ»¤æµ‹è¯•é›†ï¼Œåªä¿ç•™æ©ç ä¸­åŒ…å«æŒ‡å®šLESION_IDçš„å›¾åƒï¼Œå¹¶ç»Ÿè®¡æ•°é‡"""
        filtered_files = []
        for file in tqdm(self.image_files, desc=f"è¿‡æ»¤æµ‹è¯•é›†ï¼ˆåªä¿ç•™å«ID={config.LESION_ID}çš„å›¾åƒï¼‰"):
            mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
            mask_path = os.path.join(self.masks_dir, mask_file)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if mask is not None and config.LESION_ID in np.unique(mask):
                filtered_files.append(file)
        
        original_count = len(self.image_files)
        self.image_files = filtered_files
        filtered_count = len(self.image_files)
        
        print(f"è¿‡æ»¤å‰æµ‹è¯•é›†æ•°é‡: {original_count}")
        print(f"è¿‡æ»¤åæµ‹è¯•é›†æ•°é‡ï¼ˆå«ID={config.LESION_ID}ï¼‰: {filtered_count}")
        if filtered_count == 0:
            print(f"è­¦å‘Šï¼šè¿‡æ»¤åæµ‹è¯•é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦åŒ…å«ID={config.LESION_ID}ã€‚")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        image_file = self.image_files[idx]
        image_path = os.path.join(self.images_dir, image_file)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        mask_file = image_file.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_path = os.path.join(self.masks_dir, mask_file)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        # åº”ç”¨IDæ˜ å°„ï¼ˆå…¶ä»–ç±»è½¬ä¸ºèƒŒæ™¯ï¼‰
        mapped_mask = np.zeros_like(mask)
        for original_id, new_id in config.ID_MAPPING.items():
            mapped_mask[mask == original_id] = new_id
        unknown_mask = ~np.isin(mask, list(config.ID_MAPPING.keys()))
        mapped_mask[unknown_mask] = 0
        
        # è°ƒæ•´å¤§å°
        image = cv2.resize(image, (config.IMAGE_SIZE, config.IMAGE_SIZE))
        mask = cv2.resize(mapped_mask, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)
        
        # è½¬æ¢ä¸ºtensor
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        mask = torch.from_numpy(mask).long()
        
        # æ ‡å‡†åŒ–
        mean = torch.tensor(config.PIXEL_MEAN).view(3, 1, 1) / 255.0
        std = torch.tensor(config.PIXEL_STD).view(3, 1, 1) / 255.0
        image = (image - mean) / std
        
        return image, mask

# ===== ğŸ“Š è¯„ä¼°æŒ‡æ ‡è®¡ç®—ç±» =====
class ComprehensiveMetrics:
    """ç»¼åˆè¯„ä¼°æŒ‡æ ‡è®¡ç®—ç±» - æ”¯æŒ2ç±»è¯„ä¼°"""
    
    def __init__(self):
        self.class_dices = defaultdict(list)
        self.class_ious = defaultdict(list)
    
    def compute_metrics(self, pred: np.ndarray, target: np.ndarray, cls_id: int) -> tuple:
        """è®¡ç®—å•ä¸ªç±»çš„Diceå’ŒIoU"""
        pred_mask = (pred == cls_id)
        target_mask = (target == cls_id)
        
        intersection = np.logical_and(pred_mask, target_mask).sum()
        pred_sum = pred_mask.sum()
        target_sum = target_mask.sum()
        
        if pred_sum + target_sum == 0:
            return 1.0, 1.0
        
        if pred_sum == 0 or target_sum == 0:
            return 0.0, 0.0
        
        dice = 2.0 * intersection / (pred_sum + target_sum)
        iou = intersection / (pred_sum + target_sum - intersection)
        
        return float(dice), float(iou)
    
    def update(self, pred: np.ndarray, target: np.ndarray):
        """æ›´æ–°è¯„ä¼°æŒ‡æ ‡"""
        for cls_id in range(config.NUM_CLASSES):
            dice, iou = self.compute_metrics(pred, target, cls_id)
            self.class_dices[cls_id].append(dice)
            self.class_ious[cls_id].append(iou)
    
    def get_metrics(self):
        metrics = {
            'per_class': {},
            'overall': {}
        }
        
        # æ¯ä¸ªç±»çš„å¹³å‡
        for cls_id in range(config.NUM_CLASSES):
            if self.class_dices[cls_id]:
                mean_dice = np.mean(self.class_dices[cls_id])
                mean_iou = np.mean(self.class_ious[cls_id])
            else:
                mean_dice = 0.0
                mean_iou = 0.0
            
            metrics['per_class'][cls_id] = {
                'dice': mean_dice,
                'iou': mean_iou
            }
        
        # æ•´ä½“ mDice å’Œ mIoU (æ‰€æœ‰ç±»çš„å¹³å‡)
        mdice = np.mean([metrics['per_class'][cls]['dice'] for cls in range(config.NUM_CLASSES)])
        miou = np.mean([metrics['per_class'][cls]['iou'] for cls in range(config.NUM_CLASSES)])
        
        metrics['overall'] = {
            'mDice': mdice,
            'mIoU': miou
        }
        
        # ç—…ç¶ç‰¹å®š (class 1)
        metrics['lesion'] = metrics['per_class'].get(1, {'dice': 0.0, 'iou': 0.0})
        
        return metrics

# ===== ğŸ” åŠ è½½æ¨¡å‹ =====
def load_model(model_path, sam_checkpoint):
    sam = sam_model_registry[config.SAM_MODEL_TYPE](checkpoint=sam_checkpoint)
    model = DSCEnhancedSAMModel(sam, config.NUM_CLASSES)
    checkpoint = torch.load(model_path, map_location=config.DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(config.DEVICE)
    model.eval()
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    return model

# ===== ä¸»å‡½æ•°ï¼šæ‰¹é‡æµ‹è¯•å¹¶è®¡ç®—æŒ‡æ ‡ =====
def main():
    # åœ¨IDEä¸­ç¼–è¾‘ä»¥ä¸‹è·¯å¾„å˜é‡ï¼Œç„¶åç›´æ¥è¿è¡Œè„šæœ¬
    test_images_dir = "/root/autodl-tmp/SAM/12classes_lesion/test/images"  
    test_masks_dir = "/root/autodl-tmp/SAM/12classes_lesion/test/masks"    
    model_path = "/root/autodl-tmp/SAM/results/models/dsc_enhanced_sdbb_4/models/best_model_overall_dice.pth"  
    sam_checkpoint = "/root/autodl-tmp/SAM/pre_models/sam_vit_b_01ec64.pth"  
    
    # åŠ è½½æ¨¡å‹
    model = load_model(model_path, sam_checkpoint)
    
    # åŠ è½½æ•°æ®é›†
    test_dataset = TestDataset(test_images_dir, test_masks_dir)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS, shuffle=False)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    metrics_calc = ComprehensiveMetrics()
    
    print(f"\nå¼€å§‹æµ‹è¯•ï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬...")
    
    with torch.no_grad():
        for images, masks in tqdm(test_loader, desc="æµ‹è¯•è¿›åº¦"):
            images = images.to(config.DEVICE)
            masks = masks.cpu().numpy()  # [B, H, W]
            
            logits = model(images)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1).cpu().numpy()  # [B, H, W]
            
            for i in range(len(preds)):
                pred = preds[i]
                target = masks[i]
                
                # æ›´æ–°è¯„ä¼°æŒ‡æ ‡
                metrics_calc.update(pred, target)
    
    # è·å–è¯„ä¼°ç»“æœ
    results = metrics_calc.get_metrics()
    
    print(f"\n{'='*60}")
    print(f"ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“")
    print(f"{'='*60}")
    
    # ç—…ç¶æŒ‡æ ‡
    print(f"\nğŸ“Š ç—…ç¶ ({config.LESION_NAME}) æŒ‡æ ‡:")
    print(f"  Dice: {results['lesion']['dice']:.4f}")
    print(f"  IoU: {results['lesion']['iou']:.4f}")
    
    # æ•´ä½“æŒ‡æ ‡
    print(f"\nğŸ† æ•´ä½“å¹³å‡æŒ‡æ ‡:")
    print(f"  mDice: {results['overall']['mDice']:.4f}")
    print(f"  mIoU: {results['overall']['mIoU']:.4f}")
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•å®Œæˆï¼æ€»æ ·æœ¬æ•°: {len(test_dataset)}")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()