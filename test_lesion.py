#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ SAM ViT-Bå£°å¸¦åˆ†å‰²æµ‹è¯•è„šæœ¬ - è€å“¥å®šåˆ¶ç‰ˆ ğŸ”¥
åŒ…å«è¯¦ç»†çš„ç»“æœè§£é‡Šå’Œæ›´å¥½çš„å¯è§†åŒ–
æ”¯æŒï¼šèƒŒæ™¯ã€å·¦å£°å¸¦ã€å³å£°å¸¦ã€å£°å¸¦å°ç»“ã€å£°å¸¦ç™½æ–‘ã€å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
ä¸train_sam_optimized.pyå®Œå…¨å…¼å®¹ï¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡ï¼Œé¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# SAMæ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼‰
class EnhancedSAMModel(nn.Module):
    """è€å“¥çš„å¼ºåŒ–SAMæ¨¡å‹ - ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼"""
    
    def __init__(self, sam_model, num_classes):
        super().__init__()
        self.sam = sam_model
        self.num_classes = num_classes
        
        # ğŸ”¥ å¤šå°ºåº¦ç‰¹å¾èåˆåˆ†å‰²å¤´ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        self.segmentation_head = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šç‰¹å¾æå–
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # ç¬¬äºŒå±‚ï¼šç‰¹å¾ç»†åŒ–
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # ç¬¬ä¸‰å±‚ï¼šåˆ†ç±»è¾“å‡º
            nn.Conv2d(64, num_classes, kernel_size=1)
        )
        
        # ğŸ¯ æ³¨æ„åŠ›æ¨¡å— - è®©æ¨¡å‹ä¸»åŠ¨å…³æ³¨ç—…ç¶ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        self.attention = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # å†»ç»“SAMçš„éƒ¨åˆ†å‚æ•°ï¼Œåªå¾®è°ƒå…³é”®éƒ¨åˆ†
        self.freeze_sam_components()
        
        print("ğŸ¯ EnhancedSAMModel loaded (compatible with trained model)")
    
    def freeze_sam_components(self):
        """å†»ç»“SAMçš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒå¿…è¦çš„éƒ¨åˆ†"""
        # å†»ç»“image_encoderçš„å‰é¢å‡ å±‚
        layers = list(self.sam.image_encoder.children())
        for i, layer in enumerate(layers[:-3]):  # åªè§£å†»æœ€å3å±‚
            for param in layer.parameters():
                param.requires_grad = False
    
    def forward(self, images):
        batch_size = images.shape[0]
        
        # SAMå›¾åƒç¼–ç 
        image_embeddings = self.sam.image_encoder(images)
        
        # ğŸ¯ æ³¨æ„åŠ›å¢å¼º
        attention_map = self.attention(image_embeddings)
        enhanced_features = image_embeddings * attention_map
        
        # å¤šå°ºåº¦åˆ†å‰²
        segmentation_logits = self.segmentation_head(enhanced_features)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        segmentation_logits = F.interpolate(
            segmentation_logits,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        
        # è™šæ‹ŸIoUé¢„æµ‹ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        iou_predictions = torch.ones(batch_size, 1).to(images.device) * 0.8
        
        return segmentation_logits, iou_predictions

def test_single_image_sam(model_path, image_path, save_dir="/root/autodl-tmp/SAM/results/predictions/lesion"):
    """ğŸ”¥ SAM ViT-Bä¼˜åŒ–æ¨¡å‹6ç±»åˆ†å‰²å•å¼ å›¾ç‰‡æµ‹è¯• - è€å“¥å®šåˆ¶ç‰ˆ"""
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Using device: {device}")
    
    # åŠ è½½SAMæ¨¡å‹
    print(f"ğŸ“‚ Loading SAM model: {model_path}")
    try:
        from segment_anything import sam_model_registry
        
        # åˆ›å»ºSAMåŸºç¡€æ¨¡å‹
        sam = sam_model_registry["vit_b"](checkpoint=None)  # å…ˆåˆ›å»ºæ¶æ„
        
        # åˆ›å»ºæˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹
        model = EnhancedSAMModel(sam, 6).to(device)  # 6ç±»è¾“å‡º
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½ä¿¡æ¯
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            print(f"âœ… Model performance: mIoU={metrics.get('mIoU', 0):.4f}")
        
    except ImportError:
        print("âŒ segment_anything not found, installing...")
        os.system("pip install segment-anything")
        from segment_anything import sam_model_registry
        sam = sam_model_registry["vit_b"](checkpoint=None)  # ä½¿ç”¨ViT-bæ¨¡å‹
        model = EnhancedSAMModel(sam, 6).to(device)
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
    
    # è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
    print(f"ğŸ” Processing image: {image_path}")
    image = cv2.imread(str(image_path), cv2.IMREAD_COLOR)
    if image is None:
        raise ValueError(f"Cannot read image: {image_path}")
    
    original_size = image.shape[:2]
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # SAMæ ‡å‡†é¢„å¤„ç†ï¼ˆ1024x1024ï¼‰
    target_size = 1024
    image_resized = cv2.resize(image_rgb, (target_size, target_size))
    
    # æ ‡å‡†åŒ–ï¼ˆSAMä½¿ç”¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    pixel_mean = np.array([123.675, 116.28, 103.53])
    pixel_std = np.array([58.395, 57.12, 57.375])
    image_normalized = (image_resized - pixel_mean) / pixel_std
    
    # è½¬æ¢ä¸ºtensor
    image_tensor = torch.from_numpy(image_normalized).float().permute(2, 0, 1).unsqueeze(0).to(device)
    
    # é¢„æµ‹
    print("ğŸ”® Running inference...")
    with torch.no_grad():
        outputs, iou_preds = model(image_tensor)
        
        # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
        outputs = F.interpolate(outputs, size=original_size, mode='bilinear', align_corners=False)
        
        # è·å–é¢„æµ‹ç»“æœ
        pred_probs = F.softmax(outputs, dim=1).squeeze(0).cpu().numpy()
        pred_mask = np.argmax(pred_probs, axis=0).astype(np.uint8)
    
    # 4ç±»åˆ«å®šä¹‰ï¼ˆåªæ˜¾ç¤ºç—…ç¶ï¼Œéšè—å·¦å³å£°å¸¦ï¼‰
    class_names = [
        'Background',      # ID: 0
        'Left',   # ID: 1 (éšè—æ˜¾ç¤º)
        'Right',  # ID: 2 (éšè—æ˜¾ç¤º)
        'Vocal Nodules',   # ID: 105 -> 3 å£°å¸¦å°ç»“
        'Vocal Leukoplakia', # ID: 23 -> 4 å£°å¸¦ç™½æ–‘
        'Vocal Papilloma'  # ID: 146 -> 5 å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
    ]
    
    # æ˜¾ç¤ºç”¨çš„ç±»åˆ«åç§°ï¼ˆåªåŒ…å«ç—…ç¶ï¼‰
    display_class_names = [
        'Background',
        'Vocal Nodules',    # å£°å¸¦å°ç»“
        'Vocal Leukoplakia', # å£°å¸¦ç™½æ–‘
        'Vocal Papilloma'   # å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
    ]
    
    # é¢œè‰²æ˜ å°„ï¼šèƒŒæ™¯+3ç§ç—…ç¶
    display_colors = [
        [0, 0, 0],        # èƒŒæ™¯-é»‘è‰²
        [255, 255, 0],    # å£°å¸¦å°ç»“-é»„è‰²
        [255, 0, 255],    # å£°å¸¦ç™½æ–‘-æ´‹çº¢è‰²
        [0, 255, 255],    # å£°å¸¦ä¹³å¤´çŠ¶ç˜¤-é’è‰²
    ]
    
    # å®Œæ•´çš„ç±»åˆ«åˆ°æ˜¾ç¤ºç±»åˆ«çš„æ˜ å°„
    class_to_display = {
        0: 0,  # èƒŒæ™¯ -> èƒŒæ™¯
        1: 0,  # å·¦å£°å¸¦ -> èƒŒæ™¯ï¼ˆéšè—ï¼‰
        2: 0,  # å³å£°å¸¦ -> èƒŒæ™¯ï¼ˆéšè—ï¼‰
        3: 1,  # å£°å¸¦å°ç»“ -> æ˜¾ç¤ºç±»åˆ«1
        4: 2,  # å£°å¸¦ç™½æ–‘ -> æ˜¾ç¤ºç±»åˆ«2
        5: 3,  # å£°å¸¦ä¹³å¤´çŠ¶ç˜¤ -> æ˜¾ç¤ºç±»åˆ«3
    }
    
    # åˆ›å»ºå½©è‰²åˆ†å‰²å›¾ï¼ˆåªæ˜¾ç¤ºç—…ç¶ï¼‰
    display_mask = np.zeros_like(pred_mask)
    for original_class, display_class in class_to_display.items():
        display_mask[pred_mask == original_class] = display_class
    
    colored_mask = np.zeros((*display_mask.shape, 3), dtype=np.uint8)
    for i, color in enumerate(display_colors):
        colored_mask[display_mask == i] = color
    
    # åˆ›å»ºå åŠ å›¾
    overlay = cv2.addWeighted(image_rgb, 0.7, colored_mask, 0.3, 0)
    
    # ç»Ÿè®¡å„ç±»åˆ«åƒç´ æ•°ï¼ˆåªç»Ÿè®¡ç—…ç¶ç±»åˆ«ï¼‰
    unique, counts = np.unique(display_mask, return_counts=True)
    total_pixels = display_mask.size
    
    print("\nğŸ“Š Lesion Segmentation Statistics:")
    for cls, count in zip(unique, counts):
        percentage = count / total_pixels * 100
        if cls == 0:
            print(f"  {display_class_names[cls]}: {count:,} pixels ({percentage:.2f}%)")
        else:
            print(f"  {display_class_names[cls]}: {count:,} pixels ({percentage:.3f}%)")
    
    # é¢å¤–ç»Ÿè®¡åŸå§‹ç±»åˆ«ä¸­çš„å£°å¸¦ä¿¡æ¯ï¼ˆä»…ç”¨äºå†…éƒ¨ç»Ÿè®¡ï¼‰
    original_unique, original_counts = np.unique(pred_mask, return_counts=True)
    vocal_fold_pixels = 0
    for cls, count in zip(original_unique, original_counts):
        if cls in [1, 2]:  # å·¦å³å£°å¸¦
            vocal_fold_pixels += count
    
    if vocal_fold_pixels > 0:
        vocal_fold_percentage = vocal_fold_pixels / total_pixels * 100
        print(f"  [Hidden] Vocal Folds Total: {vocal_fold_pixels:,} pixels ({vocal_fold_percentage:.2f}%)")
    
    # åˆ›å»ºå¯è§†åŒ–å›¾
    fig = plt.figure(figsize=(20, 16))
    filename = Path(image_path).name
    fig.suptitle(f'ğŸ”¥ SAM ViT-B Vocal Lesion Segmentation Analysis - è€å“¥ä¼˜åŒ–ç‰ˆ: {filename}', fontsize=18, fontweight='bold')
    
    # åˆ›å»ºç½‘æ ¼å¸ƒå±€ - 3è¡Œ4åˆ—
    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.2)
    
    # ç¬¬ä¸€è¡Œï¼šåŸºæœ¬ç»“æœ
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(image_rgb)
    ax1.set_title('Original Image', fontsize=14, fontweight='bold')
    ax1.axis('off')
    
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.imshow(colored_mask)
    ax2.set_title('Segmentation Result', fontsize=14, fontweight='bold')
    ax2.axis('off')
    
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.imshow(overlay)
    ax3.set_title('Overlay (70% Original + 30% Mask)', fontsize=14, fontweight='bold')
    ax3.axis('off')
    
    # æ·»åŠ å›¾ä¾‹ï¼ˆåªæ˜¾ç¤ºç—…ç¶ç±»åˆ«ï¼‰
    ax4 = fig.add_subplot(gs[0, 3])
    ax4.axis('off')
    legend_elements = []
    for i, (name, color) in enumerate(zip(display_class_names, display_colors)):
        legend_elements.append(patches.Patch(color=np.array(color)/255, label=name))
    ax4.legend(handles=legend_elements, loc='center', fontsize=12, title='Lesion Legend', title_fontsize=14)
    ax4.set_title('Color Legend', fontsize=14, fontweight='bold')
    
    # ç¬¬äºŒè¡Œï¼šç—…ç¶ç½®ä¿¡åº¦åˆ†æ
    lesion_indices = [3, 4, 5]  # å£°å¸¦å°ç»“ã€å£°å¸¦ç™½æ–‘ã€å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
    lesion_titles = [
        'Vocal Nodules Confidence',
        'Vocal Leukoplakia Confidence', 
        'Vocal Papilloma Confidence'
    ]
    
    for i in range(3):
        ax = fig.add_subplot(gs[1, i])
        lesion_idx = lesion_indices[i]
        im = ax.imshow(pred_probs[lesion_idx], cmap='hot', vmin=0, vmax=1)
        ax.set_title(lesion_titles[i], fontsize=12, fontweight='bold')
        ax.axis('off')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.tick_params(labelsize=10)
        cbar.set_label('Confidence', fontsize=10)
    
    # ç¬¬äºŒè¡Œå³ä¾§ï¼šç—…ç¶ç»Ÿè®¡ä¿¡æ¯
    ax = fig.add_subplot(gs[1, 3])
    ax.axis('off')
    
    # è®¡ç®—æœ€å¤§ç½®ä¿¡åº¦ç”¨äºç»Ÿè®¡
    max_confidence = np.max(pred_probs, axis=0)
    
    stats_text = "Lesion Detection Statistics:\n\n"
    
    # åªç»Ÿè®¡ç—…ç¶ç±»åˆ«
    for cls, count in zip(unique, counts):
        if cls > 0:  # è·³è¿‡èƒŒæ™¯
            percentage = count / total_pixels * 100
            original_cls = [k for k, v in class_to_display.items() if v == cls][0]
            avg_conf = pred_probs[original_cls][pred_mask == original_cls].mean() if count > 0 else 0
            stats_text += f"{display_class_names[cls]}:\n"
            stats_text += f"  Area: {percentage:.4f}%\n"
            stats_text += f"  Confidence: {avg_conf:.3f}\n\n"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç—…ç¶è¢«æ£€æµ‹åˆ°
    lesion_detected = any(cls > 0 and counts[unique == cls][0] > total_pixels * 0.0001 for cls in unique)
    
    if not lesion_detected:
        stats_text += "No significant lesions detected.\n\n"
    
    # æ·»åŠ æ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡ï¼ˆä»…ç—…ç¶åŒºåŸŸï¼‰
    lesion_mask = display_mask > 0
    if np.any(lesion_mask):
        lesion_conf = max_confidence[lesion_mask].mean()
        stats_text += f"Lesion Avg Confidence: {lesion_conf:.3f}\n"
    
    stats_text += f"Image Size: {original_size[1]}Ã—{original_size[0]}"
    
    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))
    ax.set_title('Lesion Statistics', fontsize=14, fontweight='bold')
    
    # ç¬¬ä¸‰è¡Œï¼šèƒŒæ™¯å’Œæ•´ä½“åˆ†æ
    analysis_titles = [
        'Background Confidence',
        'Max Lesion Confidence',
        'Lesion Probability Map'
    ]
    
    # ç¬¬ä¸‰è¡Œç¬¬1åˆ—ï¼šèƒŒæ™¯ç½®ä¿¡åº¦
    ax = fig.add_subplot(gs[2, 0])
    im = ax.imshow(pred_probs[0], cmap='gray', vmin=0, vmax=1)
    ax.set_title(analysis_titles[0], fontsize=12, fontweight='bold')
    ax.axis('off')
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.ax.tick_params(labelsize=10)
    cbar.set_label('Confidence', fontsize=10)
    
    # ç¬¬ä¸‰è¡Œç¬¬2åˆ—ï¼šæœ€å¤§ç—…ç¶ç½®ä¿¡åº¦
    ax = fig.add_subplot(gs[2, 1])
    lesion_max_conf = np.max(pred_probs[3:6], axis=0)  # 3ä¸ªç—…ç¶ç±»åˆ«çš„æœ€å¤§ç½®ä¿¡åº¦
    im = ax.imshow(lesion_max_conf, cmap='hot', vmin=0, vmax=1)
    ax.set_title(analysis_titles[1], fontsize=12, fontweight='bold')
    ax.axis('off')
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.ax.tick_params(labelsize=10)
    cbar.set_label('Confidence', fontsize=10)
    
    # ç¬¬ä¸‰è¡Œç¬¬3åˆ—ï¼šç—…ç¶æ¦‚ç‡åˆ†å¸ƒå›¾
    ax = fig.add_subplot(gs[2, 2])
    # ç»„åˆæ‰€æœ‰ç—…ç¶ç±»åˆ«çš„æ¦‚ç‡
    combined_lesion_prob = np.sum(pred_probs[3:6], axis=0)
    im = ax.imshow(combined_lesion_prob, cmap='plasma', vmin=0, vmax=1)
    ax.set_title(analysis_titles[2], fontsize=12, fontweight='bold')
    ax.axis('off')
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.ax.tick_params(labelsize=10)
    cbar.set_label('Probability', fontsize=10)
    
    # ç¬¬ä¸‰è¡Œå³ä¾§ï¼šè¯¦ç»†è¯Šæ–­æŠ¥å‘Š
    ax = fig.add_subplot(gs[2, 3])
    ax.axis('off')
    
    report_text = "ğŸ¥ DIAGNOSTIC REPORT:\n\n"
    
    # è®¡ç®—æ¯ç§ç—…ç¶çš„é¢ç§¯å’Œç½®ä¿¡åº¦
    for cls in [1, 2, 3]:  # display_maskä¸­çš„ç—…ç¶ç±»åˆ«
        if cls in unique:
            count = counts[unique == cls][0]
            percentage = count / total_pixels * 100
            original_cls = [k for k, v in class_to_display.items() if v == cls][0]
            avg_conf = pred_probs[original_cls][pred_mask == original_cls].mean()
            max_conf = pred_probs[original_cls][pred_mask == original_cls].max() if count > 0 else 0
            
            if percentage > 0.001:  # é˜ˆå€¼é™ä½åˆ°0.001%
                report_text += f"âœ“ {display_class_names[cls]} DETECTED:\n"
                report_text += f"  Coverage: {percentage:.4f}%\n"
                report_text += f"  Confidence: {avg_conf:.3f} (max: {max_conf:.3f})\n"
                if percentage > 0.1:
                    report_text += f"  âš ï¸ Significant finding!\n"
                report_text += "\n"
    
    # æ•´ä½“è¯„ä¼°
    total_lesion_area = sum(counts[unique == cls][0] for cls in [1, 2, 3] if cls in unique)
    total_lesion_percentage = total_lesion_area / total_pixels * 100
    
    if total_lesion_percentage > 0.01:
        report_text += f"ğŸ“Š TOTAL LESION AREA: {total_lesion_percentage:.3f}%\n"
        if total_lesion_percentage > 1.0:
            report_text += "ğŸ”´ High lesion burden detected\n"
        elif total_lesion_percentage > 0.1:
            report_text += "ğŸŸ¡ Moderate lesion presence\n"
        else:
            report_text += "ğŸŸ¢ Minor lesion presence\n"
    else:
        report_text += "âœ… NO SIGNIFICANT LESIONS\n"
    
    ax.text(0.05, 0.95, report_text, transform=ax.transAxes, fontsize=11,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9))
    ax.set_title('Medical Report', fontsize=14, fontweight='bold')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    result_path = save_dir / f"{Path(image_path).stem}_sam_analysis.png"
    plt.savefig(result_path, dpi=150, bbox_inches='tight', facecolor='white')
    print(f"ğŸ’¾ Analysis saved: {result_path}")
    
    plt.show()
    
    return pred_mask, pred_probs, colored_mask, overlay

if __name__ == "__main__":
    # æµ‹è¯•å‚æ•° - ä½¿ç”¨æ–°è®­ç»ƒçš„SAMæ¨¡å‹è·¯å¾„
    model_path = "autodl-tmp/SAM/results/models/run_3/models/best_model.pth"
    
    # è·å–å›¾åƒè·¯å¾„
    import sys
    if len(sys.argv) > 1:
        image_path = sys.argv[1]
    else:
         # é»˜è®¤æµ‹è¯•å›¾ç‰‡
        image_path = "autodl-tmp/SAM/sdbb+rtzl.jpg"
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        print("Please complete SAM training first or specify correct model path")
        print("Expected model path: /root/autodl-tmp/SAM/results/models/best_model.pth")
    elif not os.path.exists(image_path):
        print(f"âŒ Image file not found: {image_path}")
        print("Please specify correct image path")
    else:
        try:
            print("="*80)
            print("ğŸ”¥ SAM ViT-B VOCAL LESION SEGMENTATION ANALYSIS - è€å“¥ä¼˜åŒ–ç‰ˆ")
            print("="*80)
            print(f"ğŸ“‚ Model: {model_path}")
            print(f"ğŸ–¼ï¸  Image: {image_path}")
            print("\nğŸ“‹ Understanding the Results:")
            print("â€¢ Row 1: Original image, lesion segmentation, overlay, and color legend")
            print("â€¢ Row 2: Individual lesion confidence maps + detection statistics")  
            print("â€¢ Row 3: Background analysis, max lesion confidence, probability map + medical report")
            print("â€¢ Color coding: Yellow=Vocal Nodules, Magenta=Vocal Leukoplakia, Cyan=Vocal Papilloma")
            print("â€¢ Note: Left and right vocal folds are hidden in this analysis")
            print("â€¢ Confidence interpretation: Bright=high confidence, Dark=low confidence")
            print("="*80)
            
            test_single_image_sam(model_path, image_path)
            print("âœ… SAM lesion segmentation analysis completed successfully!")
        except Exception as e:
            print(f"âŒ Error during testing: {e}")
            import traceback
            traceback.print_exc() 