#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SAMé€šç”¨ç—…ç¶æµ‹è¯•è„šæœ¬ - å•å¼ å›¾åƒå¯è§†åŒ–
åŠŸèƒ½ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹å•å¼ å›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œå¹¶å¯è§†åŒ–ç»“æœï¼š
1. åŸå›¾åƒ
2. é¢„æµ‹mask overlayåˆ°åŸå›¾åƒï¼ˆåŠé€æ˜çº¢è‰²è¡¨ç¤ºæŒ‡å®šç—…ç¶ï¼‰
3. ç—…ç¶è¾¹ç¼˜å‹¾ç”»ï¼ˆç»¿è‰²è½®å»“ï¼‰
"""

import os
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from segment_anything import sam_model_registry

# ===== ğŸ’ª é…ç½®ç±»ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰ =====
class TestConfig:
    IMAGE_SIZE = 1024
    NUM_CLASSES = 2
    SAM_MODEL_TYPE = "vit_b"
    PIXEL_MEAN = [123.675, 116.28, 103.53]
    PIXEL_STD = [58.395, 57.12, 57.375]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç”¨æˆ·ä¿®æ”¹åŒºï¼šæŒ‡å®šç—…ç¶IDå’Œåç§°
    LESION_ID = 29  
    LESION_NAME = "å£°å¸¦ç™½æ–‘" 
    
    ID_MAPPING = {
        0: 0,          # èƒŒæ™¯
        LESION_ID: 1,  # æŒ‡å®šç—…ç¶
    }

config = TestConfig()

# ===== ğŸš€ æ¨¡å‹ç±»ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼Œç®€åŒ–ç‰ˆï¼‰ =====
class EnhancedSAMModel(torch.nn.Module):
    """DSCå¢å¼ºSAMæ¨¡å‹ - åŸºäºåŸæœ‰æ¶æ„ä¼˜åŒ–"""
    
    def __init__(self, sam_model, num_classes):
        super().__init__()
        self.sam = sam_model
        self.num_classes = num_classes
        
        # å¢å¼ºçš„åˆ†å‰²å¤´ - ä¸ºDSCä¼˜åŒ–
        self.segmentation_head = torch.nn.Sequential(
            torch.nn.Conv2d(256, 128, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(128),
            torch.nn.ReLU(inplace=True),
            torch.nn.Dropout2d(0.1),  # æ·»åŠ dropouté˜²è¿‡æ‹Ÿåˆ
            
            torch.nn.Conv2d(128, 64, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(64),
            torch.nn.ReLU(inplace=True),
            torch.nn.Dropout2d(0.05),
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            torch.nn.Conv2d(64, 32, kernel_size=3, padding=1),
            torch.nn.BatchNorm2d(32),
            torch.nn.ReLU(inplace=True),
            
            torch.nn.Conv2d(32, num_classes, kernel_size=1)
        )
        
        # å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶
        self.attention = torch.nn.Sequential(
            torch.nn.Conv2d(256, 64, kernel_size=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(64, 16, kernel_size=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(16, 1, kernel_size=1),
            torch.nn.Sigmoid()
        )
        
        # è¾¹ç•Œç»†åŒ–æ¨¡å—
        self.boundary_refine = torch.nn.Sequential(
            torch.nn.Conv2d(num_classes, 16, kernel_size=3, padding=1),
            torch.nn.ReLU(inplace=True),
            torch.nn.Conv2d(16, num_classes, kernel_size=1),
            torch.nn.Tanh()  # ä½¿ç”¨tanhé™åˆ¶è¾“å‡ºèŒƒå›´
        )
        
        self.freeze_sam_components()
        
        print("DSCå¢å¼ºSAMæ¨¡å‹æ”¹è£…å®Œæ¯•ï¼")
    
    def freeze_sam_components(self):
        # ç¨å¾®å‡å°‘å†»ç»“å±‚æ•°ï¼Œä¿ç•™æ›´å¤šå¯è®­ç»ƒå‚æ•°
        layers = list(self.sam.image_encoder.children())
        for i, layer in enumerate(layers[:-4]):  # å‡å°‘å†»ç»“å±‚
            for param in layer.parameters():
                param.requires_grad = False
        
        print("SAMå‚æ•°éƒ¨åˆ†å†»ç»“å®Œæ¯•ï¼")
    
    def forward(self, images):
        batch_size = images.shape[0]
        
        image_embeddings = self.sam.image_encoder(images)
        
        attention_map = self.attention(image_embeddings)
        enhanced_features = image_embeddings * attention_map
        
        segmentation_logits = self.segmentation_head(enhanced_features)
        
        # è¾¹ç•Œç»†åŒ–
        boundary_refinement = self.boundary_refine(segmentation_logits)
        refined_logits = segmentation_logits + 0.1 * boundary_refinement
        
        refined_logits = F.interpolate(
            refined_logits,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        
        return refined_logits

# ===== ğŸ¯ åŠ è½½æ¨¡å‹ =====
def load_model(model_path, sam_checkpoint):
    sam = sam_model_registry[config.SAM_MODEL_TYPE](checkpoint=sam_checkpoint)
    model = EnhancedSAMModel(sam, config.NUM_CLASSES)
    checkpoint = torch.load(model_path, map_location=config.DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(config.DEVICE)
    model.eval()
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    return model

# ===== ğŸ› ï¸ å›¾åƒé¢„å¤„ç†ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰ =====
def preprocess_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    original_size = image.shape[:2]
    
    # è°ƒæ•´å¤§å°
    image_resized = cv2.resize(image, (config.IMAGE_SIZE, config.IMAGE_SIZE))
    
    # è½¬æ¢ä¸ºtensor
    image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0
    
    # æ ‡å‡†åŒ–
    mean = torch.tensor(config.PIXEL_MEAN).view(3, 1, 1) / 255.0
    std = torch.tensor(config.PIXEL_STD).view(3, 1, 1) / 255.0
    image_tensor = (image_tensor - mean) / std
    
    # æ·»åŠ batchç»´åº¦
    image_tensor = image_tensor.unsqueeze(0).to(config.DEVICE)
    
    return image_tensor, image, original_size

# ===== ğŸ” é¢„æµ‹å¹¶è·å–mask =====
def predict_mask(model, image_tensor, original_size):
    with torch.no_grad():
        logits = model(image_tensor)
        probs = torch.softmax(logits, dim=1)
        pred_mask = torch.argmax(probs, dim=1).squeeze(0).cpu().numpy()
    
    # è°ƒæ•´å›åŸå¤§å°
    pred_mask = cv2.resize(pred_mask.astype(np.uint8), (original_size[1], original_size[0]), interpolation=cv2.INTER_NEAREST)
    
    return pred_mask

# ===== ğŸ“Š å¯è§†åŒ–ç»“æœ =====
def visualize_results(original_image, pred_mask):
    # 1. åŸå›¾åƒ
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    axes[0].imshow(original_image)
    axes[0].set_title("1. original")
    axes[0].axis('off')
    
    # 2. Overlay maskï¼ˆçº¢è‰²åŠé€æ˜ï¼‰
    overlay = original_image.copy()
    lesion_mask = (pred_mask == 1)  # ç—…ç¶ç±»ä¸º1
    overlay[lesion_mask] = (overlay[lesion_mask] * 0.5 + np.array([255, 0, 0]) * 0.5).astype(np.uint8)
    axes[1].imshow(overlay)
    axes[1].set_title(f"2. Mask Overlay")
    axes[1].axis('off')
    
    # 3. ç—…ç¶è¾¹ç¼˜å‹¾ç”»ï¼ˆç»¿è‰²è½®å»“ï¼‰
    edges = cv2.Canny(pred_mask * 255, 100, 200)  # ä½¿ç”¨Cannyæ£€æµ‹è¾¹ç¼˜
    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    edges_colored[edges > 0] = [0, 255, 0]  # ç»¿è‰²
    contour_image = cv2.addWeighted(original_image, 1.0, edges_colored, 0.8, 0)
    axes[2].imshow(contour_image)
    axes[2].set_title(f"3. Lesion Edge")
    axes[2].axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
    save_path = "autodl-tmp/SAM/results/models/run_rtzl/visualization_result.png"
    plt.savefig(save_path)
    print(f"å¯è§†åŒ–ç»“æœä¿å­˜ä¸º: {save_path}")

def main():
    image_path = "/root/autodl-tmp/SAM/12classes_lesion/test/images/å£°å¸¦ç™½æ–‘ä¸­é‡_é™ˆå»ºè¾‰D50217B85_20210824_240300572.jpg"  
    mask_path = "/root/autodl-tmp/SAM/12classes_lesion/test/masks/å£°å¸¦ç™½æ–‘ä¸­é‡_é™ˆå»ºè¾‰D50217B85_20210824_240300572.png"  
    model_path = "/root/autodl-tmp/SAM/results/models/dsc_enhanced_sdbb/models/best_model_lesion_dice.pth"  
    sam_checkpoint = "/root/autodl-tmp/SAM/pre_models/sam_vit_b_01ec64.pth"  
    
    # æ–°å¢ï¼šæ£€æŸ¥maskæ˜¯å¦åŒ…å«æŒ‡å®šLESION_ID
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    if mask is None or config.LESION_ID not in np.unique(mask):
        print(f"è­¦å‘Šï¼šå›¾åƒçš„maskä¸åŒ…å«ID={config.LESION_ID} ({config.LESION_NAME})ï¼é¢„æµ‹ç»“æœå¯èƒ½æ— æ•ˆã€‚")
    else:
        print(f"ç¡®è®¤ï¼šå›¾åƒçš„maskåŒ…å«ID={config.LESION_ID} ({config.LESION_NAME})ã€‚")
    
    model = load_model(model_path, sam_checkpoint)
    
    image_tensor, original_image, original_size = preprocess_image(image_path)
    
    pred_mask = predict_mask(model, image_tensor, original_size)
    
    visualize_results(original_image, pred_mask)

if __name__ == "__main__":
    main()