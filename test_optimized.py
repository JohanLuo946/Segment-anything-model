#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ SAM ViT-Bå£°å¸¦åˆ†å‰²æµ‹è¯•è„šæœ¬ - è€å“¥å®šåˆ¶ç‰ˆ ğŸ”¥
åŒ…å«è¯¦ç»†çš„ç»“æœè§£é‡Šå’Œæ›´å¥½çš„å¯è§†åŒ–
æ”¯æŒï¼šèƒŒæ™¯ã€å·¦å£°å¸¦ã€å³å£°å¸¦ã€å£°å¸¦å°ç»“ã€å£°å¸¦ç™½æ–‘ã€å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
ä¸train_sam_optimized.pyå®Œå…¨å…¼å®¹ï¼
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# è®¾ç½®matplotlibä½¿ç”¨è‹±æ–‡ï¼Œé¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# SAMæ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼‰
class EnhancedSAMModel(nn.Module):
    """è€å“¥çš„å¼ºåŒ–SAMæ¨¡å‹ - ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼"""
    
    def __init__(self, sam_model, num_classes):
        super().__init__()
        self.sam = sam_model
        self.num_classes = num_classes
        
        # ğŸ”¥ å¤šå°ºåº¦ç‰¹å¾èåˆåˆ†å‰²å¤´ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        self.segmentation_head = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šç‰¹å¾æå–
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # ç¬¬äºŒå±‚ï¼šç‰¹å¾ç»†åŒ–
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # ç¬¬ä¸‰å±‚ï¼šåˆ†ç±»è¾“å‡º
            nn.Conv2d(64, num_classes, kernel_size=1)
        )
        
        # ğŸ¯ æ³¨æ„åŠ›æ¨¡å— - è®©æ¨¡å‹ä¸»åŠ¨å…³æ³¨ç—…ç¶ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰
        self.attention = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # å†»ç»“SAMçš„éƒ¨åˆ†å‚æ•°ï¼Œåªå¾®è°ƒå…³é”®éƒ¨åˆ†
        self.freeze_sam_components()
        
        print("ğŸ¯ EnhancedSAMModel loaded (compatible with trained model)")
    
    def freeze_sam_components(self):
        """å†»ç»“SAMçš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œåªè®­ç»ƒå¿…è¦çš„éƒ¨åˆ†"""
        # å†»ç»“image_encoderçš„å‰é¢å‡ å±‚
        layers = list(self.sam.image_encoder.children())
        for i, layer in enumerate(layers[:-3]):  # åªè§£å†»æœ€å3å±‚
            for param in layer.parameters():
                param.requires_grad = False
    
    def forward(self, images):
        batch_size = images.shape[0]
        
        # SAMå›¾åƒç¼–ç 
        image_embeddings = self.sam.image_encoder(images)
        
        # ğŸ¯ æ³¨æ„åŠ›å¢å¼º
        attention_map = self.attention(image_embeddings)
        enhanced_features = image_embeddings * attention_map
        
        # å¤šå°ºåº¦åˆ†å‰²
        segmentation_logits = self.segmentation_head(enhanced_features)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        segmentation_logits = F.interpolate(
            segmentation_logits,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        
        # è™šæ‹ŸIoUé¢„æµ‹ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        iou_predictions = torch.ones(batch_size, 1).to(images.device) * 0.8
        
        return segmentation_logits, iou_predictions

def test_single_image_sam(model_path, image_path, save_dir="/root/autodl-tmp/SAM/results/predictions"):
    """ğŸ”¥ SAM ViT-Bä¼˜åŒ–æ¨¡å‹6ç±»åˆ†å‰²å•å¼ å›¾ç‰‡æµ‹è¯• - è€å“¥å®šåˆ¶ç‰ˆ"""
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Using device: {device}")
    
    # åŠ è½½SAMæ¨¡å‹
    print(f"ğŸ“‚ Loading SAM model: {model_path}")
    try:
        from segment_anything import sam_model_registry
        
        # åˆ›å»ºSAMåŸºç¡€æ¨¡å‹
        sam = sam_model_registry["vit_b"](checkpoint=None)  # å…ˆåˆ›å»ºæ¶æ„
        
        # åˆ›å»ºæˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹
        model = EnhancedSAMModel(sam, 6).to(device)  # 6ç±»è¾“å‡º
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½ä¿¡æ¯
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            print(f"âœ… Model performance: mIoU={metrics.get('mIoU', 0):.4f}")
        
    except ImportError:
        print("âŒ segment_anything not found, installing...")
        os.system("pip install segment-anything")
        from segment_anything import sam_model_registry
        sam = sam_model_registry["vit_b"](checkpoint=None)  # ä½¿ç”¨ViT-bæ¨¡å‹
        model = EnhancedSAMModel(sam, 6).to(device)
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
    
    # è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
    print(f"ğŸ” Processing image: {image_path}")
    image = cv2.imread(str(image_path), cv2.IMREAD_COLOR)
    if image is None:
        raise ValueError(f"Cannot read image: {image_path}")
    
    original_size = image.shape[:2]
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # SAMæ ‡å‡†é¢„å¤„ç†ï¼ˆ1024x1024ï¼‰
    target_size = 1024
    image_resized = cv2.resize(image_rgb, (target_size, target_size))
    
    # æ ‡å‡†åŒ–ï¼ˆSAMä½¿ç”¨çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    pixel_mean = np.array([123.675, 116.28, 103.53])
    pixel_std = np.array([58.395, 57.12, 57.375])
    image_normalized = (image_resized - pixel_mean) / pixel_std
    
    # è½¬æ¢ä¸ºtensor
    image_tensor = torch.from_numpy(image_normalized).float().permute(2, 0, 1).unsqueeze(0).to(device)
    
    # é¢„æµ‹
    print("ğŸ”® Running inference...")
    with torch.no_grad():
        outputs, iou_preds = model(image_tensor)
        
        # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
        outputs = F.interpolate(outputs, size=original_size, mode='bilinear', align_corners=False)
        
        # è·å–é¢„æµ‹ç»“æœ
        pred_probs = F.softmax(outputs, dim=1).squeeze(0).cpu().numpy()
        pred_mask = np.argmax(pred_probs, axis=0).astype(np.uint8)
    
    # 6ç±»åˆ«å®šä¹‰ï¼ˆä¸è®­ç»ƒè„šæœ¬IDæ˜ å°„ä¸€è‡´ï¼‰
    class_names = [
        'Background',      # ID: 0
        'Left', # ID: 170 -> 1
        'Right',# ID: 184 -> 2
        'sdxj',    # ID: 105 -> 3
        'sdbb',# ID: 23 -> 4
        'rtzl'  # ID: 146 -> 5
    ]
    
    class_colors = [
        [0, 0, 0],        # èƒŒæ™¯-é»‘è‰²
        [255, 0, 0],      # å·¦å£°å¸¦-çº¢è‰²
        [0, 255, 0],      # å³å£°å¸¦-ç»¿è‰²  
        [255, 255, 0],    # å£°å¸¦å°ç»“-é»„è‰²
        [255, 0, 255],    # å£°å¸¦ç™½æ–‘-æ´‹çº¢è‰²
        [0, 255, 255],    # å£°å¸¦ä¹³å¤´çŠ¶ç˜¤-é’è‰²
    ]
    
    # åˆ›å»ºå½©è‰²åˆ†å‰²å›¾
    colored_mask = np.zeros((*pred_mask.shape, 3), dtype=np.uint8)
    for i, color in enumerate(class_colors):
        colored_mask[pred_mask == i] = color
    
    # åˆ›å»ºå åŠ å›¾
    overlay = cv2.addWeighted(image_rgb, 0.7, colored_mask, 0.3, 0)
    
    # ç»Ÿè®¡å„ç±»åˆ«åƒç´ æ•°
    unique, counts = np.unique(pred_mask, return_counts=True)
    total_pixels = pred_mask.size
    
    print("\nğŸ“Š Segmentation Statistics:")
    for cls, count in zip(unique, counts):
        percentage = count / total_pixels * 100
        print(f"  {class_names[cls]}: {count:,} pixels ({percentage:.2f}%)")
    
    # åˆ›å»ºå¯è§†åŒ–å›¾
    fig = plt.figure(figsize=(20, 16))
    filename = Path(image_path).name
    fig.suptitle(f'ğŸ”¥ SAM ViT-B Vocal Fold Segmentation Analysis - è€å“¥ä¼˜åŒ–ç‰ˆ: {filename}', fontsize=18, fontweight='bold')
    
    # åˆ›å»ºç½‘æ ¼å¸ƒå±€ - 3è¡Œ4åˆ—
    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], hspace=0.3, wspace=0.2)
    
    # ç¬¬ä¸€è¡Œï¼šåŸºæœ¬ç»“æœ
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(image_rgb)
    ax1.set_title('Original Image', fontsize=14, fontweight='bold')
    ax1.axis('off')
    
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.imshow(colored_mask)
    ax2.set_title('Segmentation Result', fontsize=14, fontweight='bold')
    ax2.axis('off')
    
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.imshow(overlay)
    ax3.set_title('Overlay (70% Original + 30% Mask)', fontsize=14, fontweight='bold')
    ax3.axis('off')
    
    # æ·»åŠ å›¾ä¾‹
    ax4 = fig.add_subplot(gs[0, 3])
    ax4.axis('off')
    legend_elements = []
    for i, (name, color) in enumerate(zip(class_names, class_colors)):
        legend_elements.append(patches.Patch(color=np.array(color)/255, label=name))
    ax4.legend(handles=legend_elements, loc='center', fontsize=12, title='Class Legend', title_fontsize=14)
    ax4.set_title('Color Legend', fontsize=14, fontweight='bold')
    
    # ç¬¬äºŒè¡Œï¼šå‰3ç±»ç½®ä¿¡åº¦åˆ†æ
    confidence_titles = [
        'Background Confidence',
        'Left',
        'Right'
    ]
    
    for i in range(3):
        ax = fig.add_subplot(gs[1, i])
        im = ax.imshow(pred_probs[i], cmap='hot', vmin=0, vmax=1)
        ax.set_title(confidence_titles[i], fontsize=12, fontweight='bold')
        ax.axis('off')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.tick_params(labelsize=10)
        cbar.set_label('Confidence', fontsize=10)
    
    # ç¬¬äºŒè¡Œå³ä¾§ï¼šç»Ÿè®¡ä¿¡æ¯
    ax = fig.add_subplot(gs[1, 3])
    ax.axis('off')
    
    # è®¡ç®—æœ€å¤§ç½®ä¿¡åº¦ç”¨äºç»Ÿè®¡
    max_confidence = np.max(pred_probs, axis=0)
    
    stats_text = "Detailed Statistics:\n\n"
    for cls, count in zip(unique, counts):
        percentage = count / total_pixels * 100
        avg_conf = pred_probs[cls][pred_mask == cls].mean() if count > 0 else 0
        stats_text += f"{class_names[cls]}:\n"
        stats_text += f"  Pixels: {count:,} ({percentage:.2f}%)\n"
        stats_text += f"  Avg Confidence: {avg_conf:.3f}\n\n"
    
    # æ·»åŠ æ•´ä½“ç½®ä¿¡åº¦ç»Ÿè®¡
    overall_conf = max_confidence.mean()
    stats_text += f"Overall Avg Confidence: {overall_conf:.3f}\n"
    stats_text += f"Image Size: {original_size[1]}Ã—{original_size[0]}"
    
    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax.set_title('Statistics', fontsize=14, fontweight='bold')
    
    # ç¬¬ä¸‰è¡Œï¼šç—…ç¶ç±»åˆ«ç½®ä¿¡åº¦åˆ†æ
    lesion_titles = [
        'sdxj',
        'sdbb', 
        'rtzl'
    ]
    
    for i in range(3):
        ax = fig.add_subplot(gs[2, i])
        lesion_idx = i + 3  # ç—…ç¶ç±»åˆ«ä»ç´¢å¼•3å¼€å§‹
        im = ax.imshow(pred_probs[lesion_idx], cmap='hot', vmin=0, vmax=1)
        ax.set_title(lesion_titles[i], fontsize=12, fontweight='bold')
        ax.axis('off')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
        cbar.ax.tick_params(labelsize=10)
        cbar.set_label('Confidence', fontsize=10)
    
    # ç¬¬ä¸‰è¡Œå³ä¾§ï¼šç—…ç¶ç»Ÿè®¡
    ax = fig.add_subplot(gs[2, 3])
    ax.axis('off')
    
    lesion_stats_text = "Lesion Analysis:\n\n"
    lesion_detected = False
    
    for cls in [3, 4, 5]:  # ç—…ç¶ç±»åˆ«
        if cls in unique:
            count = counts[unique == cls][0]
            percentage = count / total_pixels * 100
            avg_conf = pred_probs[cls][pred_mask == cls].mean()
            
            lesion_stats_text += f"{class_names[cls]}:\n"
            lesion_stats_text += f"  Area: {percentage:.3f}%\n"
            lesion_stats_text += f"  Confidence: {avg_conf:.3f}\n\n"
            
            if percentage > 0.01:  # å¦‚æœé¢ç§¯å¤§äº0.01%
                lesion_detected = True
    
    if not lesion_detected:
        lesion_stats_text += "No significant lesions detected.\n"
    
    ax.text(0.05, 0.95, lesion_stats_text, transform=ax.transAxes, fontsize=12,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    ax.set_title('Lesion Detection', fontsize=14, fontweight='bold')
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜ç»“æœ
    result_path = save_dir / f"{Path(image_path).stem}_sam_analysis.png"
    plt.savefig(result_path, dpi=150, bbox_inches='tight', facecolor='white')
    print(f"ğŸ’¾ Analysis saved: {result_path}")
    
    plt.show()
    
    return pred_mask, pred_probs, colored_mask, overlay

if __name__ == "__main__":
    # æµ‹è¯•å‚æ•° - ä½¿ç”¨æ–°è®­ç»ƒçš„SAMæ¨¡å‹è·¯å¾„
    model_path = "autodl-tmp/SAM/results/models/run_3/models/best_model.pth"
    
    # è·å–å›¾åƒè·¯å¾„
    import sys
    if len(sys.argv) > 1:
        image_path = sys.argv[1]
    else:
         # é»˜è®¤æµ‹è¯•å›¾ç‰‡
        image_path = "autodl-tmp/SAM/data/test/images/å£°å¸¦ç™½æ–‘ä¸­é‡_å†¯æ¶¦è™133004001875398_20160301_011014090.jpg"
    
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        print("Please complete SAM training first or specify correct model path")
        print("Expected model path: /root/autodl-tmp/SAM/results/models/best_model.pth")
    elif not os.path.exists(image_path):
        print(f"âŒ Image file not found: {image_path}")
        print("Please specify correct image path")
    else:
        try:
            print("="*80)
            print("ğŸ”¥ SAM ViT-B VOCAL FOLD SEGMENTATION ANALYSIS - è€å“¥ä¼˜åŒ–ç‰ˆ")
            print("="*80)
            print(f"ğŸ“‚ Model: {model_path}")
            print(f"ğŸ–¼ï¸  Image: {image_path}")
            print("\nğŸ“‹ Understanding the Results:")
            print("â€¢ Row 1: Original image, segmentation result, overlay, and color legend")
            print("â€¢ Row 2: Background and vocal fold confidence maps + statistics")
            print("â€¢ Row 3: Lesion confidence maps + lesion detection summary")
            print("â€¢ Color coding: Red=Left fold, Green=Right fold, Yellow=sdxj, Magenta=sdbb, Cyan=rtzl")
            print("â€¢ Confidence interpretation: Bright=high confidence, Dark=low confidence")
            print("="*80)
            
            test_single_image_sam(model_path, image_path)
            print("âœ… SAM segmentation analysis completed successfully!")
        except Exception as e:
            print(f"âŒ Error during testing: {e}")
            import traceback
            traceback.print_exc() 