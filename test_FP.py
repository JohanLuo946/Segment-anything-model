#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ SAM 2ç±»ç—…ç¶åˆ†å‰² - çº¯åŒ»å­¦åˆ†å‰²ä¸“ç”¨æŒ‡æ ‡æµ‹è¯•è„šæœ¬

ä¸“æ³¨äºçœŸæ­£æœ‰æ„ä¹‰çš„åŒ»å­¦åˆ†å‰²æŒ‡æ ‡ï¼š
ğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡: Diceç³»æ•°ã€IoUã€F1åˆ†æ•°
ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§æŒ‡æ ‡: Hausdorffè·ç¦»ã€å¹³å‡è¡¨é¢è·ç¦»(ASD)ã€HD95

âš¡ å·²åˆ é™¤æ— æ„ä¹‰çš„ä¼ ç»Ÿåˆ†ç±»æŒ‡æ ‡ï¼š
âŒ ç²¾ç¡®åº¦(Precision) - å¿½ç•¥ç©ºé—´è¿ç»­æ€§ï¼Œå¯¹åˆ†å‰²æ— æ„ä¹‰
âŒ å¬å›ç‡(Recall) - åŸºäºåƒç´ åˆ†ç±»ï¼Œæ— æ³•è¯„ä¼°è¾¹ç•Œè´¨é‡
âŒ ç‰¹å¼‚æ€§(Specificity) - å¯¹åŒ»å­¦åˆ†å‰²ä»»åŠ¡æ— å®é™…ä»·å€¼

âœ… ä¸ºä»€ä¹ˆè¾¹ç•Œè·ç¦»æŒ‡æ ‡æ›´é‡è¦ï¼š
â€¢ Hausdorffè·ç¦»: ç›´æ¥æµ‹é‡è¾¹ç•Œæœ€å¤§åç§»ï¼Œè¯†åˆ«è¿‡åº¦åˆ†å‰²
â€¢ å¹³å‡è¡¨é¢è·ç¦»: è¯„ä¼°æ•´ä½“è¾¹ç•Œç²¾ç¡®æ€§ï¼Œå¯¹æ‰€æœ‰FPæ•æ„Ÿ
â€¢ HD95: é²æ£’çš„è¾¹ç•Œè´¨é‡è¯„ä¼°ï¼Œå¿½ç•¥å¼‚å¸¸ç‚¹

ğŸ¯ è¯„ä¼°æ ‡å‡†: è¾¹ç•Œè·ç¦» <5px(ä¼˜ç§€) | 5-10px(è‰¯å¥½) | >10px(éœ€æ”¹è¿›)
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import cv2
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
import logging
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')
from scipy.spatial.distance import directed_hausdorff
from scipy import ndimage

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class TestConfig:
    # æµ‹è¯•æ•°æ®é…ç½®
    TEST_IMAGES_DIR = "autodl-tmp/SAM/12classes_lesion/test/images"
    TEST_MASKS_DIR = "autodl-tmp/SAM/12classes_lesion/test/masks"
    
    # æ¨¡å‹é…ç½®
    MODEL_PATH = "autodl-tmp/SAM/results/models/dsc_enhanced_sdbb_4/models/best_model_lesion_dice.pth"
    SAM_MODEL_PATH = "autodl-tmp/SAM/pre_models/sam_vit_b_01ec64.pth"  # SAMé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    LESION_ID = 29
    LESION_NAME = "å£°å¸¦ç™½æ–‘"
    LESION_CODE = "sdbb"
    
    # æµ‹è¯•é…ç½®
    IMAGE_SIZE = 1024
    BATCH_SIZE = 1
    NUM_WORKERS = 4
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # åˆ†æé…ç½®
    CONFIDENCE_THRESHOLDS = [0.5, 0.6, 0.7, 0.8]  # ç®€åŒ–é˜ˆå€¼
    SAVE_VISUALIZATIONS = True
    MAX_VISUALIZATIONS = 10  # å‡å°‘å¯è§†åŒ–æ•°é‡
    
    # è¾“å‡ºç›®å½•
    RESULTS_DIR = f"test_results_{LESION_CODE}"
    
    # SAMé…ç½®
    PIXEL_MEAN = [123.675, 116.28, 103.53]
    PIXEL_STD = [58.395, 57.12, 57.375]
    
    ID_MAPPING = {0: 0, LESION_ID: 1}
    CLASS_NAMES = ["èƒŒæ™¯", LESION_NAME]

config = TestConfig()

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹ç±»
from train_2classes_v3 import DSCEnhancedSAMModel, DSCEnhancedDataset

class LesionOnlyDataset(DSCEnhancedDataset):
    """åªåŒ…å«ç›®æ ‡ç—…ç¶çš„æµ‹è¯•æ•°æ®é›†"""
    
    def __init__(self, images_dir, masks_dir):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        all_files = []
        for file in os.listdir(images_dir):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
                mask_path = os.path.join(masks_dir, mask_file)
                if os.path.exists(mask_path):
                    all_files.append(file)
        
        # è¿‡æ»¤ï¼šåªä¿ç•™åŒ…å«ç›®æ ‡ç—…ç¶çš„å›¾åƒ
        self.image_files = []
        self.lesion_areas = []
        
        logger.info(f"å¼€å§‹è¿‡æ»¤æµ‹è¯•é›†ï¼Œåªä¿ç•™å«æœ‰{config.LESION_NAME}(ID={config.LESION_ID})çš„å›¾åƒ...")
        
        for file in tqdm(all_files, desc="è¿‡æ»¤æ•°æ®é›†"):
            mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
            mask_path = os.path.join(masks_dir, mask_file)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            
            if mask is not None and config.LESION_ID in np.unique(mask):
                self.image_files.append(file)
                lesion_area = np.sum(mask == config.LESION_ID)
                self.lesion_areas.append(lesion_area)
        
        logger.info(f"è¿‡æ»¤å®Œæˆï¼šåŸå§‹{len(all_files)}ä¸ª â†’ æœ‰æ•ˆ{len(self.image_files)}ä¸ª")
        if self.lesion_areas:
            logger.info(f"ç—…ç¶é¢ç§¯ç»Ÿè®¡: æœ€å°={min(self.lesion_areas)}, æœ€å¤§={max(self.lesion_areas)}, å¹³å‡={np.mean(self.lesion_areas):.0f}")
    
    def __getitem__(self, idx):
        image_file = self.image_files[idx]
        image_path = os.path.join(self.images_dir, image_file)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        mask_file = image_file.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_path = os.path.join(self.masks_dir, mask_file)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        # ä¿å­˜åŸå§‹å°ºå¯¸å’Œæ©ç 
        original_height, original_width = image.shape[:2]
        original_mask = mask.copy()
        
        # åº”ç”¨IDæ˜ å°„
        mask = self.apply_id_mapping(mask)
        
        # è°ƒæ•´å°ºå¯¸
        image = cv2.resize(image, (config.IMAGE_SIZE, config.IMAGE_SIZE))
        mask = cv2.resize(mask, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)
        
        # å½’ä¸€åŒ–
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        mask = torch.from_numpy(mask).long()
        
        # SAMæ ‡å‡†åŒ–
        mean = torch.tensor(config.PIXEL_MEAN).view(3, 1, 1) / 255.0
        std = torch.tensor(config.PIXEL_STD).view(3, 1, 1) / 255.0
        image = (image - mean) / std
        
        return image, mask, image_file, (original_height, original_width), original_mask

class MetricsCalculator:
    """åŒ»å­¦åˆ†å‰²ä¸“ç”¨æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.sample_metrics = []
        self.threshold_metrics = {th: {'tp': 0, 'fp': 0, 'fn': 0, 'tn': 0} for th in config.CONFIDENCE_THRESHOLDS}
    
    def calculate_hausdorff_distance(self, pred_mask, target_mask):
        """è®¡ç®—Hausdorffè·ç¦» - è¾¹ç•Œè´¨é‡çš„å…³é”®æŒ‡æ ‡"""
        try:
            # è·å–è¾¹ç•Œç‚¹
            pred_boundary = self.get_boundary_points(pred_mask)
            target_boundary = self.get_boundary_points(target_mask)
            
            if len(pred_boundary) == 0 or len(target_boundary) == 0:
                return float('inf'), float('inf'), float('inf')
            
            # è®¡ç®—åŒå‘Hausdorffè·ç¦»
            hd1 = directed_hausdorff(pred_boundary, target_boundary)[0]
            hd2 = directed_hausdorff(target_boundary, pred_boundary)[0]
            hd = max(hd1, hd2)
            
            # è®¡ç®—å¹³å‡è¡¨é¢è·ç¦»(ASD)
            asd = self.calculate_average_surface_distance(pred_boundary, target_boundary)
            
            # è®¡ç®—HD95 (95åˆ†ä½æ•°)
            hd95 = self.calculate_hd95(pred_boundary, target_boundary)
            
            return float(hd), float(asd), float(hd95)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—HDè·ç¦»æ—¶å‡ºé”™: {e}")
            return float('inf'), float('inf'), float('inf')
    
    def get_boundary_points(self, mask):
        """æå–è¾¹ç•Œç‚¹åæ ‡"""
        # ä½¿ç”¨å½¢æ€å­¦æ“ä½œè·å–è¾¹ç•Œ
        boundary = mask - ndimage.binary_erosion(mask)
        y_coords, x_coords = np.where(boundary > 0)
        if len(y_coords) > 0:
            return np.column_stack([y_coords, x_coords])
        return np.array([])
    
    def calculate_average_surface_distance(self, pred_boundary, target_boundary):
        """è®¡ç®—å¹³å‡è¡¨é¢è·ç¦»"""
        if len(pred_boundary) == 0 or len(target_boundary) == 0:
            return float('inf')
        
        # ä»é¢„æµ‹è¾¹ç•Œåˆ°çœŸå®è¾¹ç•Œçš„å¹³å‡è·ç¦»
        dist1 = np.mean([np.min(np.linalg.norm(pred_boundary - target_pt, axis=1)) 
                        for target_pt in target_boundary])
        
        # ä»çœŸå®è¾¹ç•Œåˆ°é¢„æµ‹è¾¹ç•Œçš„å¹³å‡è·ç¦»  
        dist2 = np.mean([np.min(np.linalg.norm(target_boundary - pred_pt, axis=1)) 
                        for pred_pt in pred_boundary])
        
        return (dist1 + dist2) / 2
    
    def calculate_hd95(self, pred_boundary, target_boundary):
        """è®¡ç®—95åˆ†ä½æ•°Hausdorffè·ç¦» - æ›´é²æ£’çš„è¾¹ç•ŒæŒ‡æ ‡"""
        if len(pred_boundary) == 0 or len(target_boundary) == 0:
            return float('inf')
        
        # è®¡ç®—æ‰€æœ‰ç‚¹å¯¹è·ç¦»çš„95åˆ†ä½æ•°
        distances1 = [np.min(np.linalg.norm(pred_boundary - target_pt, axis=1)) 
                     for target_pt in target_boundary]
        distances2 = [np.min(np.linalg.norm(target_boundary - pred_pt, axis=1)) 
                     for pred_pt in pred_boundary]
        
        all_distances = distances1 + distances2
        if len(all_distances) > 0:
            return np.percentile(all_distances, 95)
        return float('inf')
    
    def calculate_sample_metrics(self, pred_prob, target_mask, filename):
        """è®¡ç®—åˆ†å‰²ä¸“ç”¨æŒ‡æ ‡ - ä¸“æ³¨äºåŒ»å­¦æ„ä¹‰"""
        sample_result = {'filename': str(filename), 'lesion_area': int(np.sum(target_mask == 1))}
        
        for threshold in config.CONFIDENCE_THRESHOLDS:
            pred_mask = (pred_prob > threshold).astype(np.uint8)
            target_binary = target_mask.astype(np.uint8)
            
            # è®¡ç®—æ··æ·†çŸ©é˜µ - ä»…ç”¨äºè®¡ç®—åˆ†å‰²è´¨é‡æŒ‡æ ‡
            tp = np.sum((pred_mask == 1) & (target_binary == 1))
            fp = np.sum((pred_mask == 1) & (target_binary == 0))
            fn = np.sum((pred_mask == 0) & (target_binary == 1))
            tn = np.sum((pred_mask == 0) & (target_binary == 0))
            
            # ğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡ - æœ€é‡è¦çš„æŒ‡æ ‡
            epsilon = 1e-8
            dice = 2 * tp / (2 * tp + fp + fn + epsilon)
            iou = tp / (tp + fp + fn + epsilon)
            f1 = dice  # å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼ŒF1ä¸Diceç­‰ä»·
            
            # FPç‡ - ä»…ä½œä¸ºé”™è¯¯åˆ†æå‚è€ƒ
            fp_rate = fp / (fp + tn + epsilon)
            
            # ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§æŒ‡æ ‡ - å¯¹è¿‡åº¦åˆ†å‰²æœ€æ•æ„Ÿ
            hd, asd, hd95 = float('inf'), float('inf'), float('inf')
            if np.sum(target_binary) > 0 and np.sum(pred_mask) > 0:  # åªæœ‰å½“ä¸¤è€…éƒ½æœ‰å‰æ™¯æ—¶æ‰è®¡ç®—
                hd, asd, hd95 = self.calculate_hausdorff_distance(pred_mask, target_binary)
            elif np.sum(target_binary) > 0 and np.sum(pred_mask) == 0:  # æœ‰çœŸå€¼ä½†æ— é¢„æµ‹
                hd, asd, hd95 = float('inf'), float('inf'), float('inf')
            elif np.sum(target_binary) == 0 and np.sum(pred_mask) > 0:  # æ— çœŸå€¼ä½†æœ‰é¢„æµ‹(çº¯FP)
                hd, asd, hd95 = float('inf'), float('inf'), float('inf')
            else:  # éƒ½æ²¡æœ‰
                hd, asd, hd95 = 0.0, 0.0, 0.0
            
            # ä¿å­˜åˆ†å‰²ä¸“ç”¨æŒ‡æ ‡
            sample_result[f'dice_{threshold}'] = float(dice)
            sample_result[f'iou_{threshold}'] = float(iou)
            sample_result[f'f1_{threshold}'] = float(f1)
            sample_result[f'fp_count_{threshold}'] = int(fp)
            sample_result[f'fn_count_{threshold}'] = int(fn)
            sample_result[f'fp_rate_{threshold}'] = float(fp_rate)
            
            # ğŸ”¥ è¾¹ç•Œè·ç¦»æŒ‡æ ‡
            sample_result[f'hausdorff_{threshold}'] = float(hd) if hd != float('inf') else 999.0
            sample_result[f'asd_{threshold}'] = float(asd) if asd != float('inf') else 999.0
            sample_result[f'hd95_{threshold}'] = float(hd95) if hd95 != float('inf') else 999.0
            
            # ç´¯ç§¯å…¨å±€æŒ‡æ ‡
            self.threshold_metrics[threshold]['tp'] += tp
            self.threshold_metrics[threshold]['fp'] += fp
            self.threshold_metrics[threshold]['fn'] += fn
            self.threshold_metrics[threshold]['tn'] += tn
            
            # ç´¯ç§¯è¾¹ç•Œè·ç¦»æŒ‡æ ‡
            if 'hd_values' not in self.threshold_metrics[threshold]:
                self.threshold_metrics[threshold]['hd_values'] = []
                self.threshold_metrics[threshold]['asd_values'] = []
                self.threshold_metrics[threshold]['hd95_values'] = []
            
            if hd != float('inf'):
                self.threshold_metrics[threshold]['hd_values'].append(hd)
            if asd != float('inf'):
                self.threshold_metrics[threshold]['asd_values'].append(asd)
            if hd95 != float('inf'):
                self.threshold_metrics[threshold]['hd95_values'].append(hd95)
        
        self.sample_metrics.append(sample_result)
        return sample_result
    
    def get_summary_metrics(self):
        """è·å–åˆ†å‰²ä¸“ç”¨æ±‡æ€»æŒ‡æ ‡"""
        summary = {}
        
        for threshold in config.CONFIDENCE_THRESHOLDS:
            metrics = self.threshold_metrics[threshold]
            tp, fp, fn, tn = metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']
            
            epsilon = 1e-8
            # ğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡
            dice = 2 * tp / (2 * tp + fp + fn + epsilon)
            iou = tp / (tp + fp + fn + epsilon)
            f1 = dice  # å¯¹äºåˆ†å‰²ä»»åŠ¡ï¼ŒF1ä¸Diceç­‰ä»·
            
            # FPç‡ - ä»…ä½œä¸ºé”™è¯¯åˆ†æå‚è€ƒ
            fp_rate = fp / (fp + tn + epsilon)
            
            # ğŸ”¥ è¾¹ç•Œè·ç¦»æŒ‡æ ‡ç»Ÿè®¡
            mean_hd = np.mean(metrics['hd_values']) if metrics['hd_values'] else 999.0
            mean_asd = np.mean(metrics['asd_values']) if metrics['asd_values'] else 999.0
            mean_hd95 = np.mean(metrics['hd95_values']) if metrics['hd95_values'] else 999.0
            
            std_hd = np.std(metrics['hd_values']) if metrics['hd_values'] else 0.0
            std_asd = np.std(metrics['asd_values']) if metrics['asd_values'] else 0.0
            std_hd95 = np.std(metrics['hd95_values']) if metrics['hd95_values'] else 0.0
            
            max_hd = np.max(metrics['hd_values']) if metrics['hd_values'] else 999.0
            max_asd = np.max(metrics['asd_values']) if metrics['asd_values'] else 999.0
            max_hd95 = np.max(metrics['hd95_values']) if metrics['hd95_values'] else 999.0
            
            summary[f'threshold_{threshold}'] = {
                # ğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡
                'dice': float(dice),
                'iou': float(iou),
                'f1': float(f1),
                
                # é”™è¯¯åˆ†ææŒ‡æ ‡
                'fp_rate': float(fp_rate),
                'total_fp': int(fp),
                'total_fn': int(fn),
                'total_tp': int(tp),
                
                # ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§æŒ‡æ ‡ - æœ€é‡è¦çš„æŒ‡æ ‡
                'mean_hausdorff': float(mean_hd),
                'std_hausdorff': float(std_hd),
                'max_hausdorff': float(max_hd),
                'mean_asd': float(mean_asd),
                'std_asd': float(std_asd),
                'max_asd': float(max_asd),
                'mean_hd95': float(mean_hd95),
                'std_hd95': float(std_hd95),
                'max_hd95': float(max_hd95),
                'valid_samples': len(metrics['hd_values'])  # æœ‰æ•ˆè®¡ç®—è¾¹ç•Œè·ç¦»çš„æ ·æœ¬æ•°
            }
        
        return summary

class SimpleTester:
    """ç®€åŒ–çš„æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.device = torch.device(config.DEVICE)
        self.setup_directories()
        self.load_model()
        self.setup_data()
        self.metrics_calculator = MetricsCalculator()
    
    def setup_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„å¹¶åˆ›å»ºç›®å½•
        self.results_dir = os.path.abspath(config.RESULTS_DIR)
        self.viz_dir = os.path.join(self.results_dir, "visualizations")
        self.reports_dir = os.path.join(self.results_dir, "reports")
        
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(self.viz_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
        
        logger.info(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.results_dir}")
        logger.info(f"å¯è§†åŒ–ç›®å½•: {self.viz_dir}")
        logger.info(f"æŠ¥å‘Šç›®å½•: {self.reports_dir}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"åŠ è½½æ¨¡å‹: {config.MODEL_PATH}")
        
        if not os.path.exists(config.MODEL_PATH):
            logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config.MODEL_PATH}")
            sys.exit(1)
        
        checkpoint = torch.load(config.MODEL_PATH, map_location=self.device)
        
        try:
            from segment_anything import sam_model_registry
            sam = sam_model_registry["vit_b"](checkpoint=config.SAM_MODEL_PATH)
            sam.to(self.device)
            
            self.model = DSCEnhancedSAMModel(sam, num_classes=2)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            logger.info(f"è®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'Unknown')}")
            logger.info(f"æœ€ä½³ç—…ç¶Dice: {checkpoint.get('best_lesion_dice', 'Unknown')}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.error(f"è¯·æ£€æŸ¥SAMæ¨¡å‹è·¯å¾„: {config.SAM_MODEL_PATH}")
            sys.exit(1)
    
    def custom_collate_fn(self, batch):
        """è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†éå¼ é‡æ•°æ®"""
        images = torch.stack([item[0] for item in batch])
        masks = torch.stack([item[1] for item in batch])
        filenames = [item[2] for item in batch]
        original_sizes = [item[3] for item in batch]
        original_masks = [item[4] for item in batch]
        
        return images, masks, filenames, original_sizes, original_masks
    
    def setup_data(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.test_dataset = LesionOnlyDataset(config.TEST_IMAGES_DIR, config.TEST_MASKS_DIR)
        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=config.BATCH_SIZE,
            shuffle=False,
            num_workers=config.NUM_WORKERS,
            pin_memory=True,
            collate_fn=self.custom_collate_fn
        )
        
        logger.info(f"æœ‰æ•ˆæµ‹è¯•æ ·æœ¬: {len(self.test_dataset)} ä¸ª")
    
    def run_inference(self):
        """è¿è¡Œæ¨ç†"""
        logger.info("å¼€å§‹æ¨ç†...")
        
        all_results = []
        visualization_count = 0
        
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(tqdm(self.test_loader, desc="æ¨ç†ä¸­")):
                images, masks, filenames, original_sizes, original_masks = batch_data
                images = images.to(self.device)
                masks = masks.to(self.device)
                
                # æ¨¡å‹æ¨ç†
                predictions, _ = self.model(images)
                pred_probs = torch.softmax(predictions, dim=1)
                
                # å¤„ç†æ¯ä¸ªæ ·æœ¬
                for i in range(images.size(0)):
                    filename = filenames[i]
                    pred_prob = pred_probs[i, 1].cpu().numpy()  # ç—…ç¶ç±»åˆ«æ¦‚ç‡
                    target_mask = masks[i].cpu().numpy()
                    original_size = original_sizes[i]  # (height, width)
                    
                    # è·å–åŸå§‹å°ºå¯¸
                    height, width = original_size[0], original_size[1]
                    
                    # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
                    pred_prob_resized = cv2.resize(pred_prob, (width, height))
                    target_mask_resized = cv2.resize(target_mask, (width, height), interpolation=cv2.INTER_NEAREST)
                    
                    # è®¡ç®—æŒ‡æ ‡
                    sample_metrics = self.metrics_calculator.calculate_sample_metrics(
                        pred_prob_resized, target_mask_resized, filename
                    )
                    
                    # ä¿å­˜ç»“æœ
                    result = {
                        'filename': filename,
                        'pred_prob': pred_prob_resized,
                        'target_mask': target_mask_resized,
                        'metrics': sample_metrics
                    }
                    all_results.append(result)
                    
                    # å¯è§†åŒ–éƒ¨åˆ†ç»“æœ
                    if config.SAVE_VISUALIZATIONS and visualization_count < config.MAX_VISUALIZATIONS:
                        self.visualize_result(result, visualization_count)
                        visualization_count += 1
        
        self.all_results = all_results
        logger.info(f"æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {len(all_results)} ä¸ªæ ·æœ¬")
    
    def visualize_result(self, result, idx):
        """ç®€åŒ–çš„å¯è§†åŒ–"""
        filename = result['filename']
        pred_prob = result['pred_prob']
        target_mask = result['target_mask']
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle(f'{filename}', fontsize=14)
        
        # Ground Truth
        axes[0, 0].imshow(target_mask, cmap='gray')
        axes[0, 0].set_title('Ground Truth')
        axes[0, 0].axis('off')
        
        # é¢„æµ‹æ¦‚ç‡
        im = axes[0, 1].imshow(pred_prob, cmap='hot', vmin=0, vmax=1)
        axes[0, 1].set_title('Prediction Probability')
        axes[0, 1].axis('off')
        plt.colorbar(im, ax=axes[0, 1])
        
        # é¢„æµ‹ç»“æœ (é˜ˆå€¼=0.5)
        pred_mask_05 = (pred_prob > 0.5).astype(np.uint8)
        axes[0, 2].imshow(pred_mask_05, cmap='gray')
        axes[0, 2].set_title('Prediction (th=0.5)')
        axes[0, 2].axis('off')
        
        # é”™è¯¯åˆ†æ
        error_map = np.zeros((*pred_mask_05.shape, 3))
        error_map[target_mask == 1] = [0, 1, 0]  # TP - ç»¿è‰²
        error_map[(pred_mask_05 == 1) & (target_mask == 0)] = [1, 0, 0]  # FP - çº¢è‰²
        error_map[(pred_mask_05 == 0) & (target_mask == 1)] = [0, 0, 1]  # FN - è“è‰²
        
        axes[1, 0].imshow(error_map)
        axes[1, 0].set_title('Error Analysis\nRed:FP, Blue:FN, Green:TP')
        axes[1, 0].axis('off')
        
        # æŒ‡æ ‡æ–‡æœ¬ - çªå‡ºåˆ†å‰²ä¸“ç”¨æŒ‡æ ‡
        metrics = result['metrics']
        hd = metrics['hausdorff_0.5']
        asd = metrics['asd_0.5']
        hd95 = metrics['hd95_0.5']
        
        # æ ¼å¼åŒ–è·ç¦»æŒ‡æ ‡
        hd_str = f"{hd:.1f}" if hd < 900 else "âˆ"
        asd_str = f"{asd:.1f}" if asd < 900 else "âˆ"
        hd95_str = f"{hd95:.1f}" if hd95 < 900 else "âˆ"
        
        metrics_text = f"""ğŸ¯ åˆ†å‰²è´¨é‡ (é˜ˆå€¼ 0.5):
Dice: {metrics['dice_0.5']:.4f}
IoU: {metrics['iou_0.5']:.4f}
F1: {metrics['f1_0.5']:.4f}

ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§ (å…³é”®æŒ‡æ ‡):
Hausdorff: {hd_str} px
ASD: {asd_str} px  
HD95: {hd95_str} px

ğŸ“Š é”™è¯¯ç»Ÿè®¡:
FPåƒç´ : {metrics['fp_count_0.5']}
FNåƒç´ : {metrics['fn_count_0.5']}
FPç‡: {metrics['fp_rate_0.5']:.4f}"""
        
        axes[1, 1].text(0.1, 0.9, metrics_text, transform=axes[1, 1].transAxes, 
                       verticalalignment='top', fontsize=10, fontfamily='monospace')
        axes[1, 1].axis('off')
        
        # æ¦‚ç‡åˆ†å¸ƒ
        axes[1, 2].hist(pred_prob.flatten(), bins=30, alpha=0.7, color='blue')
        axes[1, 2].axvline(0.5, color='red', linestyle='--', label='Threshold 0.5')
        axes[1, 2].set_title('Probability Distribution')
        axes[1, 2].set_xlabel('Probability')
        axes[1, 2].legend()
        
        plt.tight_layout()
        
        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
        safe_filename = filename.replace('.jpg', '').replace('.jpeg', '').replace('[', '_').replace(']', '_')
        save_path = os.path.join(self.viz_dir, f"sample_{idx+1:03d}_{safe_filename}.png")
        
        # å†æ¬¡ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    def convert_to_json_serializable(self, obj):
        """é€’å½’è½¬æ¢æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {key: self.convert_to_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    def generate_report(self):
        """ç”Ÿæˆç®€åŒ–æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        # è·å–æ±‡æ€»æŒ‡æ ‡
        summary_metrics = self.metrics_calculator.get_summary_metrics()
        
        # æ ·æœ¬çº§æ•°æ®
        sample_df = pd.DataFrame(self.metrics_calculator.sample_metrics)
        
        # æ‰¾å‡ºæœ€ä½³é˜ˆå€¼
        best_threshold = 0.5
        best_f1 = 0
        for th in config.CONFIDENCE_THRESHOLDS:
            f1 = summary_metrics[f'threshold_{th}']['f1']
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = th
        
        # ç”ŸæˆæŠ¥å‘Š - ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
        report = {
            'test_config': {
                'total_samples': int(len(self.test_dataset)),
                'lesion_type': str(config.LESION_NAME),
                'model_path': str(config.MODEL_PATH)
            },
            'summary_metrics': self.convert_to_json_serializable(summary_metrics),
            'best_threshold': float(best_threshold),
            'best_f1': float(best_f1),
            'sample_statistics': {
                'mean_lesion_area': float(np.mean(self.test_dataset.lesion_areas)),
                'std_lesion_area': float(np.std(self.test_dataset.lesion_areas)),
                'min_lesion_area': int(min(self.test_dataset.lesion_areas)),
                'max_lesion_area': int(max(self.test_dataset.lesion_areas))
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.reports_dir, "test_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ ·æœ¬æ•°æ®
        sample_csv_path = os.path.join(self.reports_dir, "sample_metrics.csv")
        sample_df.to_csv(sample_csv_path, index=False, encoding='utf-8')
        
        # åˆ›å»ºæ€§èƒ½å›¾è¡¨
        self.create_performance_chart(summary_metrics, sample_df)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report
    
    def create_performance_chart(self, summary_metrics, sample_df):
        """åˆ›å»ºæ€§èƒ½å›¾è¡¨"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        thresholds = config.CONFIDENCE_THRESHOLDS
        
        # 1. åˆ†å‰²è´¨é‡æŒ‡æ ‡éšé˜ˆå€¼å˜åŒ– (é‡ç‚¹å…³æ³¨)
        ax1 = axes[0, 0]
        metrics_to_plot = ['dice', 'iou', 'f1']  # ä¸“æ³¨äºæœ€é‡è¦çš„æŒ‡æ ‡
        colors = ['red', 'blue', 'green']
        for i, metric in enumerate(metrics_to_plot):
            values = [summary_metrics[f'threshold_{th}'][metric] for th in thresholds]
            ax1.plot(thresholds, values, marker='o', label=metric.upper(), 
                    linewidth=3, color=colors[i])
        ax1.set_xlabel('Confidence Threshold')
        ax1.set_ylabel('åˆ†å‰²è´¨é‡')
        ax1.set_title('ğŸ”¥ æ ¸å¿ƒåˆ†å‰²è´¨é‡æŒ‡æ ‡')
        ax1.legend(fontsize=12)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1)
        
        # 2. ğŸ”¥ è¾¹ç•Œè·ç¦»æŒ‡æ ‡ - å¯¹FPé—®é¢˜æœ€æ•æ„Ÿ
        ax2 = axes[0, 1]
        hd_values = []
        asd_values = []
        hd95_values = []
        
        for th in thresholds:
            hd = summary_metrics[f'threshold_{th}']['mean_hausdorff']
            asd = summary_metrics[f'threshold_{th}']['mean_asd']  
            hd95 = summary_metrics[f'threshold_{th}']['mean_hd95']
            
            # è¿‡æ»¤å¼‚å¸¸å€¼
            hd_values.append(hd if hd < 900 else np.nan)
            asd_values.append(asd if asd < 900 else np.nan)
            hd95_values.append(hd95 if hd95 < 900 else np.nan)
        
        ax2.plot(thresholds, hd_values, 'r-o', label='Hausdorff', linewidth=2, markersize=6)
        ax2.plot(thresholds, asd_values, 'b-s', label='ASD', linewidth=2, markersize=6)
        ax2.plot(thresholds, hd95_values, 'g-^', label='HD95', linewidth=2, markersize=6)
        
        ax2.set_xlabel('Confidence Threshold')
        ax2.set_ylabel('è·ç¦» (åƒç´ )')
        ax2.set_title('ğŸ¯ è¾¹ç•Œç²¾ç¡®æ€§æŒ‡æ ‡ (è¶Šå°è¶Šå¥½)')
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ å‚è€ƒçº¿
        if not all(np.isnan(hd_values + asd_values + hd95_values)):
            ax2.axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='è‰¯å¥½é˜ˆå€¼')
            ax2.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='å¯æ¥å—é˜ˆå€¼')
        
        # 3. Diceåˆ†æ•°åˆ†å¸ƒ
        ax3 = axes[0, 2]
        dice_scores = sample_df['dice_0.5']
        ax3.hist(dice_scores, bins=15, alpha=0.7, color='skyblue', edgecolor='black')
        ax3.axvline(dice_scores.mean(), color='red', linestyle='--', linewidth=2, 
                   label=f'å‡å€¼: {dice_scores.mean():.3f}')
        ax3.set_xlabel('Dice Score')
        ax3.set_ylabel('Sample Count')
        ax3.set_title('Diceåˆ†æ•°åˆ†å¸ƒ (é˜ˆå€¼=0.5)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç—…ç¶å¤§å° vs æ€§èƒ½
        ax4 = axes[1, 0]
        lesion_areas = sample_df['lesion_area']
        dice_scores = sample_df['dice_0.5']
        scatter = ax4.scatter(lesion_areas, dice_scores, alpha=0.6, c=dice_scores, cmap='viridis')
        ax4.set_xlabel('Lesion Area (pixels)')
        ax4.set_ylabel('Dice Score')
        ax4.set_title('ç—…ç¶å¤§å° vs Diceè¡¨ç°')
        plt.colorbar(scatter, ax=ax4)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(lesion_areas) > 1:
            z = np.polyfit(lesion_areas, dice_scores, 1)
            p = np.poly1d(z)
            ax4.plot(lesion_areas, p(lesion_areas), "r--", alpha=0.8)
        
        # 5. FPåˆ†å¸ƒ
        ax5 = axes[1, 1]
        fp_counts = sample_df['fp_count_0.5']
        ax5.hist(fp_counts, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax5.axvline(fp_counts.mean(), color='red', linestyle='--', linewidth=2,
                   label=f'å‡å€¼: {fp_counts.mean():.1f}')
        ax5.set_xlabel('FP Count')
        ax5.set_ylabel('Sample Count')
        ax5.set_title('FPæ•°é‡åˆ†å¸ƒ (é˜ˆå€¼=0.5)')
        ax5.legend()
        ax5.grid(True, alpha=0.3)
        
        # 6. æ€§èƒ½æ€»ç»“
        ax6 = axes[1, 2]
        ax6.axis('off')
        
        # æ‰¾æœ€ä½³é˜ˆå€¼
        best_th = thresholds[0]
        best_f1 = 0
        for th in thresholds:
            f1 = summary_metrics[f'threshold_{th}']['f1']
            if f1 > best_f1:
                best_f1 = f1
                best_th = th
        
        # è·å–è¾¹ç•Œè·ç¦»æŒ‡æ ‡
        best_hd = summary_metrics[f'threshold_{best_th}']['mean_hausdorff']
        best_asd = summary_metrics[f'threshold_{best_th}']['mean_asd']
        best_hd95 = summary_metrics[f'threshold_{best_th}']['mean_hd95']
        
        hd_str = f"{best_hd:.1f}px" if best_hd < 900 else "è®¡ç®—å¤±è´¥"
        asd_str = f"{best_asd:.1f}px" if best_asd < 900 else "è®¡ç®—å¤±è´¥"
        hd95_str = f"{best_hd95:.1f}px" if best_hd95 < 900 else "è®¡ç®—å¤±è´¥"
        
        summary_text = f"""ğŸ”¥ åŒ»å­¦åˆ†å‰²ä¸“ç”¨è¯„ä¼°:

ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:
â€¢ æœ‰æ•ˆæ ·æœ¬: {len(sample_df)}ä¸ª
â€¢ ç—…ç¶é¢ç§¯: {lesion_areas.mean():.0f}Â±{lesion_areas.std():.0f} px

ğŸ¯ åˆ†å‰²è´¨é‡ (é˜ˆå€¼={best_th}):
â€¢ Diceç³»æ•°: {summary_metrics[f'threshold_{best_th}']['dice']:.4f}
â€¢ IoU: {summary_metrics[f'threshold_{best_th}']['iou']:.4f}  
â€¢ F1åˆ†æ•°: {summary_metrics[f'threshold_{best_th}']['f1']:.4f}

ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§ (æœ€é‡è¦!):
â€¢ Hausdorffè·ç¦»: {hd_str}
â€¢ å¹³å‡è¡¨é¢è·ç¦»: {asd_str}
â€¢ HD95: {hd95_str}

ğŸ“ˆ è´¨é‡åˆ†å¸ƒ:
â€¢ é«˜è´¨é‡(Dice>0.8): {np.sum(dice_scores > 0.8)}/{len(dice_scores)} ({100*np.sum(dice_scores > 0.8)/len(dice_scores):.1f}%)
â€¢ ä¼˜ç§€(Dice>0.9): {np.sum(dice_scores > 0.9)}/{len(dice_scores)} ({100*np.sum(dice_scores > 0.9)/len(dice_scores):.1f}%)

ğŸ“Š é”™è¯¯åˆ†æ:
â€¢ FP/FNæ¯”: {summary_metrics[f'threshold_{best_th}']['total_fp']/(summary_metrics[f'threshold_{best_th}']['total_fn']+1e-8):.1f}

ğŸ¯ è¾¹ç•Œè´¨é‡è¯„ä¼°æ ‡å‡†:
â€¢ <5px: ä¼˜ç§€ | 5-10px: è‰¯å¥½ | >10px: éœ€æ”¹è¿›
        """
        
        ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, 
                fontsize=11, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        chart_path = os.path.join(self.reports_dir, "performance_analysis.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_path}")
    
    def print_summary(self, report):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ” æµ‹è¯•ç»“æœæ‘˜è¦")
        logger.info("="*60)
        
        config_info = report['test_config']
        logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®: {config_info['total_samples']}ä¸ª{config_info['lesion_type']}æ ·æœ¬")
        
        best_th = report['best_threshold']
        best_metrics = report['summary_metrics'][f'threshold_{best_th}']
        
        logger.info(f"\nğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡ (é˜ˆå€¼={best_th}):")
        logger.info(f"   â€¢ Diceç³»æ•°: {best_metrics['dice']:.4f}")
        logger.info(f"   â€¢ IoU: {best_metrics['iou']:.4f}")
        logger.info(f"   â€¢ F1åˆ†æ•°: {best_metrics['f1']:.4f}")
        
        logger.info(f"\nğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§æŒ‡æ ‡ (å…³é”®è¯„ä¼°!):")
        hd = best_metrics['mean_hausdorff']
        asd = best_metrics['mean_asd']
        hd95 = best_metrics['mean_hd95']
        
        if hd < 999:
            logger.info(f"   â€¢ å¹³å‡Hausdorffè·ç¦»: {hd:.2f} åƒç´ ")
            logger.info(f"   â€¢ å¹³å‡è¡¨é¢è·ç¦»(ASD): {asd:.2f} åƒç´ ")
            logger.info(f"   â€¢ HD95: {hd95:.2f} åƒç´ ")
            logger.info(f"   â€¢ æœ€å¤§è¾¹ç•Œåç§»: {best_metrics['max_hausdorff']:.2f} åƒç´ ")
            
            # æ ¹æ®è¾¹ç•Œè·ç¦»è¯„ä¼°è´¨é‡
            if hd < 5:
                logger.info(f"   âœ… è¾¹ç•Œè´¨é‡: ä¼˜ç§€ (Hausdorff < 5px)")
            elif hd < 10:
                logger.info(f"   ğŸŸ¡ è¾¹ç•Œè´¨é‡: è‰¯å¥½ (Hausdorff 5-10px)")
            else:
                logger.info(f"   ğŸ”´ è¾¹ç•Œè´¨é‡: éœ€æ”¹è¿› (Hausdorff > 10px)")
        else:
            logger.info(f"   âš ï¸  è¾¹ç•Œè·ç¦»è®¡ç®—å¤±è´¥ (å¯èƒ½å­˜åœ¨ä¸¥é‡åˆ†å‰²é”™è¯¯)")
        
        logger.info(f"\nğŸ“Š é”™è¯¯åˆ†æ:")
        logger.info(f"   â€¢ FPç‡: {best_metrics['fp_rate']:.4f} ({best_metrics['fp_rate']*100:.2f}%)")
        logger.info(f"   â€¢ æ€»FPåƒç´ : {best_metrics['total_fp']:,}")
        logger.info(f"   â€¢ æ€»FNåƒç´ : {best_metrics['total_fn']:,}")
        
        # FP vs FN åˆ†æ
        fp_fn_ratio = best_metrics['total_fp'] / (best_metrics['total_fn'] + 1e-8)
        if fp_fn_ratio > 2:
            logger.info(f"   ğŸ”´ é—®é¢˜: è¿‡åº¦åˆ†å‰²ä¸¥é‡ (FP/FNæ¯”={fp_fn_ratio:.1f})")
        elif fp_fn_ratio > 1.2:
            logger.info(f"   ğŸŸ¡ é—®é¢˜: è½»å¾®è¿‡åº¦åˆ†å‰² (FP/FNæ¯”={fp_fn_ratio:.1f})")
        elif fp_fn_ratio < 0.5:
            logger.info(f"   ğŸ”µ é—®é¢˜: åˆ†å‰²ä¸è¶³ (FP/FNæ¯”={fp_fn_ratio:.1f})")
        else:
            logger.info(f"   âœ… FP/FNå¹³è¡¡è‰¯å¥½ (æ¯”å€¼={fp_fn_ratio:.1f})")
        
        # æ ·æœ¬ç»Ÿè®¡
        sample_df = pd.DataFrame(self.metrics_calculator.sample_metrics)
        dice_scores = sample_df['dice_0.5']
        high_quality = np.sum(dice_scores > 0.8)
        excellent = np.sum(dice_scores > 0.9)
        
        logger.info(f"\nğŸ“ˆ æ ·æœ¬è´¨é‡åˆ†å¸ƒ:")
        logger.info(f"   â€¢ é«˜è´¨é‡æ ·æœ¬ (Dice>0.8): {high_quality}/{len(dice_scores)} ({100*high_quality/len(dice_scores):.1f}%)")
        logger.info(f"   â€¢ ä¼˜ç§€æ ·æœ¬ (Dice>0.9): {excellent}/{len(dice_scores)} ({100*excellent/len(dice_scores):.1f}%)")
        
        logger.info("="*60 + "\n")
    
    def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        
        # 1. æ¨ç†
        self.run_inference()
        
        # 2. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()
        
        # 3. æ‰“å°æ‘˜è¦
        self.print_summary(report)
        
        logger.info(f"æµ‹è¯•å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.results_dir}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ” å¯åŠ¨ç—…ç¶åˆ†å‰²æ¨¡å‹æµ‹è¯•...")
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(config.TEST_IMAGES_DIR):
        logger.error(f"æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {config.TEST_IMAGES_DIR}")
        return
    
    if not os.path.exists(config.TEST_MASKS_DIR):
        logger.error(f"æµ‹è¯•æ©ç ç›®å½•ä¸å­˜åœ¨: {config.TEST_MASKS_DIR}")
        return
    
    if not os.path.exists(config.MODEL_PATH):
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config.MODEL_PATH}")
        logger.info("è¯·ä¿®æ”¹ MODEL_PATH ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„")
        return
    
    # è¿è¡Œæµ‹è¯•
    tester = SimpleTester()
    report = tester.run_test()
    
    logger.info("ğŸ‰ æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 