#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ SAMå£°å¸¦ç—…ç¶åˆ†å‰² - å®Œæ•´æµ‹è¯•è¯„ä¼°è„šæœ¬
ç§‘å­¦è¯„ä»·ä½“ç³»ï¼šåªè®¡ç®—å­˜åœ¨çš„ç±»åˆ«ï¼Œé¿å…ç»Ÿè®¡é™·é˜±
"""

import os
import sys
import json
import numpy as np
import torch
import torch.nn.functional as F
import cv2
from tqdm import tqdm
import logging
from collections import defaultdict, Counter
import pandas as pd
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class TestConfig:
    """æµ‹è¯•é…ç½®"""
    # è·¯å¾„é…ç½®
    TEST_IMAGES_DIR = "autodl-tmp/SAM/6classdata/images"
    TEST_MASKS_DIR = "autodl-tmp/SAM/6classdata/masks"
    MODEL_PATH = "autodl-tmp/SAM/results/models/run_2/models/best_model.pth"
    RESULTS_DIR = "autodl-tmp/SAM/results/fold_test"
    
    # åŸºç¡€é…ç½®
    NUM_CLASSES = 6
    IMAGE_SIZE = 1024
    BATCH_SIZE = 4
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç±»åˆ«æ˜ å°„
    ID_MAPPING = {
        0: 0,    # èƒŒæ™¯
        170: 1,  # å·¦å£°å¸¦
        184: 2,  # å³å£°å¸¦
        105: 3,  # å£°å¸¦å°ç»“
        23: 4,   # å£°å¸¦ç™½æ–‘
        146: 5,  # å£°å¸¦ä¹³å¤´çŠ¶ç˜¤
    }
    
    CLASS_NAMES = [
        "èƒŒæ™¯", "å·¦å£°å¸¦", "å³å£°å¸¦", "å£°å¸¦å°ç»“", "å£°å¸¦ç™½æ–‘", "å£°å¸¦ä¹³å¤´çŠ¶ç˜¤"
    ]
    
    # ç—…ç¶ç±»åˆ«ç´¢å¼•
    LESION_CLASSES = [3, 4, 5]
    LESION_NAMES = ["å£°å¸¦å°ç»“", "å£°å¸¦ç™½æ–‘", "å£°å¸¦ä¹³å¤´çŠ¶ç˜¤"]
    
    # SAMé…ç½®
    PIXEL_MEAN = [123.675, 116.28, 103.53]
    PIXEL_STD = [58.395, 57.12, 57.375]

config = TestConfig()

class ComprehensiveMetrics:
    """å…¨é¢çš„è¯„ä»·æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        # åŸºç¡€ç»Ÿè®¡
        self.total_images = 0
        self.total_pixels = 0
        self.total_correct = 0
        
        # ç±»åˆ«çº§ç»Ÿè®¡
        self.class_stats = {
            'intersections': np.zeros(config.NUM_CLASSES),
            'unions': np.zeros(config.NUM_CLASSES),
            'pred_counts': np.zeros(config.NUM_CLASSES),
            'true_counts': np.zeros(config.NUM_CLASSES),
            'image_counts': np.zeros(config.NUM_CLASSES)  # æ¯ä¸ªç±»åˆ«å‡ºç°åœ¨å¤šå°‘å¼ å›¾åƒä¸­
        }
        
        # å›¾åƒçº§ç»Ÿè®¡
        self.image_metrics = []
        
        # ç—…ç¶ä¸“é¡¹ç»Ÿè®¡
        self.lesion_stats = {
            'detection_counts': np.zeros(len(config.LESION_CLASSES)),  # æ£€æµ‹åˆ°çš„æ¬¡æ•°
            'ground_truth_counts': np.zeros(len(config.LESION_CLASSES)),  # çœŸå®å­˜åœ¨çš„æ¬¡æ•°
            'size_analysis': {
                'small': {'count': 0, 'ious': []},    # <1000åƒç´ 
                'medium': {'count': 0, 'ious': []},   # 1000-5000åƒç´ 
                'large': {'count': 0, 'ious': []}     # >5000åƒç´ 
            }
        }
        
        # å¤šç—…ç¶åˆ†æ
        self.multi_lesion_stats = {
            'single_lesion_images': {'count': 0, 'ious': []},
            'multi_lesion_images': {'count': 0, 'ious': []},
            'no_lesion_images': {'count': 0, 'ious': []}
        }
    
    def update(self, predictions, targets, image_name):
        """æ›´æ–°æŒ‡æ ‡"""
        predictions = torch.argmax(predictions, dim=1)
        
        self.total_images += 1
        self.total_pixels += targets.numel()
        self.total_correct += (predictions == targets).sum().item()
        
        # åˆ†æå½“å‰å›¾åƒ
        image_analysis = self._analyze_single_image(predictions, targets, image_name)
        self.image_metrics.append(image_analysis)
        
        # æ›´æ–°ç±»åˆ«ç»Ÿè®¡
        self._update_class_stats(predictions, targets)
        
        # æ›´æ–°ç—…ç¶ç»Ÿè®¡
        self._update_lesion_stats(predictions, targets)
        
        # æ›´æ–°å¤šç—…ç¶ç»Ÿè®¡
        self._update_multi_lesion_stats(predictions, targets, image_analysis)
    
    def _analyze_single_image(self, predictions, targets, image_name):
        """åˆ†æå•å¼ å›¾åƒ"""
        pred = predictions[0].cpu().numpy()
        target = targets[0].cpu().numpy()
        
        # è®¡ç®—è¯¥å›¾åƒçš„ç±»åˆ«IoU
        image_class_ious = {}
        present_classes = []
        
        for class_idx in range(config.NUM_CLASSES):
            pred_mask = (pred == class_idx)
            target_mask = (target == class_idx)
            
            intersection = np.sum(pred_mask & target_mask)
            union = np.sum(pred_mask | target_mask)
            
            if union > 0:
                iou = intersection / union
                image_class_ious[config.CLASS_NAMES[class_idx]] = iou
                present_classes.append(class_idx)
        
        # è®¡ç®—è¯¥å›¾åƒçš„mIoUï¼ˆåªè®¡ç®—å­˜åœ¨çš„ç±»åˆ«ï¼‰
        image_miou = np.mean(list(image_class_ious.values())) if image_class_ious else 0
        
        # ç—…ç¶åˆ†æ
        lesion_analysis = self._analyze_lesions_in_image(pred, target)
        
        image_result = {
            'image_name': image_name,
            'present_classes': [config.CLASS_NAMES[i] for i in present_classes],
            'class_ious': image_class_ious,
            'image_miou': image_miou,
            'lesion_analysis': lesion_analysis,
            'pixel_accuracy': np.sum(pred == target) / target.size
        }
        
        return image_result
    
    def _analyze_lesions_in_image(self, pred, target):
        """åˆ†æå›¾åƒä¸­çš„ç—…ç¶"""
        lesion_analysis = {
            'lesions_present': [],
            'lesions_detected': [],
            'lesion_ious': {},
            'lesion_sizes': {}
        }
        
        for i, lesion_class in enumerate(config.LESION_CLASSES):
            target_mask = (target == lesion_class)
            pred_mask = (pred == lesion_class)
            
            if np.sum(target_mask) > 0:  # çœŸå®å­˜åœ¨è¯¥ç—…ç¶
                lesion_name = config.LESION_NAMES[i]
                lesion_analysis['lesions_present'].append(lesion_name)
                
                # è®¡ç®—ç—…ç¶å¤§å°
                lesion_size = np.sum(target_mask)
                lesion_analysis['lesion_sizes'][lesion_name] = lesion_size
                
                # è®¡ç®—IoU
                intersection = np.sum(pred_mask & target_mask)
                union = np.sum(pred_mask | target_mask)
                
                if union > 0:
                    iou = intersection / union
                    lesion_analysis['lesion_ious'][lesion_name] = iou
                    
                    if intersection > 0:  # æ£€æµ‹åˆ°äº†
                        lesion_analysis['lesions_detected'].append(lesion_name)
        
        return lesion_analysis
    
    def _update_class_stats(self, predictions, targets):
        """æ›´æ–°ç±»åˆ«ç»Ÿè®¡"""
        pred = predictions[0].cpu().numpy()
        target = targets[0].cpu().numpy()
        
        for class_idx in range(config.NUM_CLASSES):
            pred_mask = (pred == class_idx)
            target_mask = (target == class_idx)
            
            intersection = np.sum(pred_mask & target_mask)
            union = np.sum(pred_mask | target_mask)
            
            if union > 0:
                self.class_stats['intersections'][class_idx] += intersection
                self.class_stats['unions'][class_idx] += union
                self.class_stats['image_counts'][class_idx] += 1
            
            self.class_stats['pred_counts'][class_idx] += np.sum(pred_mask)
            self.class_stats['true_counts'][class_idx] += np.sum(target_mask)
    
    def _update_lesion_stats(self, predictions, targets):
        """æ›´æ–°ç—…ç¶ç»Ÿè®¡"""
        pred = predictions[0].cpu().numpy()
        target = targets[0].cpu().numpy()
        
        for i, lesion_class in enumerate(config.LESION_CLASSES):
            target_mask = (target == lesion_class)
            pred_mask = (pred == lesion_class)
            
            if np.sum(target_mask) > 0:  # çœŸå®å­˜åœ¨
                self.lesion_stats['ground_truth_counts'][i] += 1
                
                # å¤§å°åˆ†æ
                lesion_size = np.sum(target_mask)
                intersection = np.sum(pred_mask & target_mask)
                union = np.sum(pred_mask | target_mask)
                
                if union > 0:
                    iou = intersection / union
                    
                    if lesion_size < 1000:
                        self.lesion_stats['size_analysis']['small']['count'] += 1
                        self.lesion_stats['size_analysis']['small']['ious'].append(iou)
                    elif lesion_size < 5000:
                        self.lesion_stats['size_analysis']['medium']['count'] += 1
                        self.lesion_stats['size_analysis']['medium']['ious'].append(iou)
                    else:
                        self.lesion_stats['size_analysis']['large']['count'] += 1
                        self.lesion_stats['size_analysis']['large']['ious'].append(iou)
                
                if intersection > 0:  # æ£€æµ‹åˆ°äº†
                    self.lesion_stats['detection_counts'][i] += 1
    
    def _update_multi_lesion_stats(self, predictions, targets, image_analysis):
        """æ›´æ–°å¤šç—…ç¶ç»Ÿè®¡"""
        lesions_count = len(image_analysis['lesion_analysis']['lesions_present'])
        image_miou = image_analysis['image_miou']
        
        if lesions_count == 0:
            self.multi_lesion_stats['no_lesion_images']['count'] += 1
            self.multi_lesion_stats['no_lesion_images']['ious'].append(image_miou)
        elif lesions_count == 1:
            self.multi_lesion_stats['single_lesion_images']['count'] += 1
            self.multi_lesion_stats['single_lesion_images']['ious'].append(image_miou)
        else:
            self.multi_lesion_stats['multi_lesion_images']['count'] += 1
            self.multi_lesion_stats['multi_lesion_images']['ious'].append(image_miou)
    
    def compute_final_metrics(self):
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        results = {}
        
        # 1. åŸºç¡€æŒ‡æ ‡
        results['basic_metrics'] = {
            'total_images': self.total_images,
            'pixel_accuracy': self.total_correct / self.total_pixels if self.total_pixels > 0 else 0
        }
        
        # 2. ç±»åˆ«çº§æŒ‡æ ‡
        class_metrics = self._compute_class_metrics()
        results['class_metrics'] = class_metrics
        
        # 3. æ•´ä½“mIoUï¼ˆä¼˜åŒ–ç‰ˆå’Œä¼ ç»Ÿç‰ˆï¼‰
        results['overall_metrics'] = self._compute_overall_metrics(class_metrics)
        
        # 4. ç—…ç¶ä¸“é¡¹æŒ‡æ ‡
        results['lesion_metrics'] = self._compute_lesion_metrics(class_metrics)
        
        # 5. å¤šç—…ç¶åˆ†æ
        results['multi_lesion_analysis'] = self._compute_multi_lesion_metrics()
        
        # 6. å›¾åƒçº§åˆ†æ
        results['image_level_analysis'] = self._compute_image_level_metrics()
        
        return results
    
    def _compute_class_metrics(self):
        """è®¡ç®—ç±»åˆ«æŒ‡æ ‡"""
        class_metrics = {}
        
        for class_idx, class_name in enumerate(config.CLASS_NAMES):
            if self.class_stats['unions'][class_idx] > 0:
                iou = self.class_stats['intersections'][class_idx] / self.class_stats['unions'][class_idx]
                
                # Precision, Recall, F1
                precision = (self.class_stats['intersections'][class_idx] / 
                           self.class_stats['pred_counts'][class_idx] if self.class_stats['pred_counts'][class_idx] > 0 else 0)
                recall = (self.class_stats['intersections'][class_idx] / 
                         self.class_stats['true_counts'][class_idx] if self.class_stats['true_counts'][class_idx] > 0 else 0)
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                class_metrics[class_name] = {
                    'IoU': iou,
                    'Precision': precision,
                    'Recall': recall,
                    'F1': f1,
                    'image_frequency': self.class_stats['image_counts'][class_idx] / self.total_images,
                    'present_in_images': int(self.class_stats['image_counts'][class_idx])
                }
            else:
                class_metrics[class_name] = {
                    'IoU': 0.0,
                    'Precision': 0.0,
                    'Recall': 0.0,
                    'F1': 0.0,
                    'image_frequency': 0.0,
                    'present_in_images': 0
                }
        
        return class_metrics
    
    def _compute_overall_metrics(self, class_metrics):
        """è®¡ç®—æ•´ä½“æŒ‡æ ‡"""
        # ä¼ ç»Ÿæ–¹å¼ï¼ˆåŒ…å«æ‰€æœ‰ç±»åˆ«ï¼‰
        all_ious = [metrics['IoU'] for metrics in class_metrics.values()]
        all_f1s = [metrics['F1'] for metrics in class_metrics.values()]
        
        traditional_miou = np.mean(all_ious)
        traditional_mf1 = np.mean(all_f1s)
        
        # ä¼˜åŒ–æ–¹å¼ï¼ˆåªè®¡ç®—å­˜åœ¨çš„ç±»åˆ«ï¼‰
        existing_ious = [metrics['IoU'] for metrics in class_metrics.values() if metrics['present_in_images'] > 0]
        existing_f1s = [metrics['F1'] for metrics in class_metrics.values() if metrics['present_in_images'] > 0]
        existing_classes = [name for name, metrics in class_metrics.items() if metrics['present_in_images'] > 0]
        
        optimized_miou = np.mean(existing_ious) if existing_ious else 0
        optimized_mf1 = np.mean(existing_f1s) if existing_f1s else 0
        
        return {
            'traditional_mIoU': traditional_miou,
            'traditional_mF1': traditional_mf1,
            'optimized_mIoU': optimized_miou,
            'optimized_mF1': optimized_mf1,
            'existing_classes': existing_classes,
            'improvement': {
                'mIoU_improvement': optimized_miou - traditional_miou,
                'mF1_improvement': optimized_mf1 - traditional_mf1
            }
        }
    
    def _compute_lesion_metrics(self, class_metrics):
        """è®¡ç®—ç—…ç¶ä¸“é¡¹æŒ‡æ ‡"""
        lesion_metrics = {}
        
        # å„ç—…ç¶çš„è¯¦ç»†æŒ‡æ ‡
        for i, lesion_name in enumerate(config.LESION_NAMES):
            # æ£€æµ‹ç‡
            detection_rate = (self.lesion_stats['detection_counts'][i] / 
                            self.lesion_stats['ground_truth_counts'][i] 
                            if self.lesion_stats['ground_truth_counts'][i] > 0 else 0)
            
            lesion_metrics[lesion_name] = {
                'IoU': class_metrics[lesion_name]['IoU'],
                'F1': class_metrics[lesion_name]['F1'],
                'detection_rate': detection_rate,
                'ground_truth_count': int(self.lesion_stats['ground_truth_counts'][i]),
                'detected_count': int(self.lesion_stats['detection_counts'][i])
            }
        
        # ç—…ç¶æ•´ä½“æŒ‡æ ‡
        lesion_ious = [lesion_metrics[name]['IoU'] for name in config.LESION_NAMES 
                      if lesion_metrics[name]['ground_truth_count'] > 0]
        lesion_f1s = [lesion_metrics[name]['F1'] for name in config.LESION_NAMES 
                     if lesion_metrics[name]['ground_truth_count'] > 0]
        
        overall_lesion_metrics = {
            'lesion_mIoU_optimized': np.mean(lesion_ious) if lesion_ious else 0,
            'lesion_mF1_optimized': np.mean(lesion_f1s) if lesion_f1s else 0,
            'lesion_mIoU_traditional': np.mean([lesion_metrics[name]['IoU'] for name in config.LESION_NAMES]),
            'overall_detection_rate': np.mean([lesion_metrics[name]['detection_rate'] for name in config.LESION_NAMES])
        }
        
        # ç—…ç¶å¤§å°æ•æ„Ÿæ€§åˆ†æ
        size_analysis = {}
        for size_category, data in self.lesion_stats['size_analysis'].items():
            if data['count'] > 0:
                size_analysis[size_category] = {
                    'count': data['count'],
                    'avg_iou': np.mean(data['ious']),
                    'std_iou': np.std(data['ious'])
                }
            else:
                size_analysis[size_category] = {'count': 0, 'avg_iou': 0, 'std_iou': 0}
        
        return {
            'individual_lesions': lesion_metrics,
            'overall_lesion_metrics': overall_lesion_metrics,
            'size_sensitivity_analysis': size_analysis
        }
    
    def _compute_multi_lesion_metrics(self):
        """è®¡ç®—å¤šç—…ç¶åˆ†ææŒ‡æ ‡"""
        multi_lesion_analysis = {}
        
        for category, data in self.multi_lesion_stats.items():
            if data['count'] > 0:
                multi_lesion_analysis[category] = {
                    'count': data['count'],
                    'percentage': data['count'] / self.total_images * 100,
                    'avg_miou': np.mean(data['ious']),
                    'std_miou': np.std(data['ious'])
                }
            else:
                multi_lesion_analysis[category] = {
                    'count': 0, 'percentage': 0, 'avg_miou': 0, 'std_miou': 0
                }
        
        return multi_lesion_analysis
    
    def _compute_image_level_metrics(self):
        """è®¡ç®—å›¾åƒçº§åˆ†æ"""
        image_mious = [img['image_miou'] for img in self.image_metrics]
        
        return {
            'avg_image_miou': np.mean(image_mious),
            'std_image_miou': np.std(image_mious),
            'min_image_miou': np.min(image_mious),
            'max_image_miou': np.max(image_mious),
            'median_image_miou': np.median(image_mious)
        }

class TestDataset:
    """æµ‹è¯•æ•°æ®é›†"""
    
    def __init__(self, images_dir, masks_dir):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        
        # è·å–å›¾åƒæ–‡ä»¶
        self.image_files = []
        for file in os.listdir(images_dir):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
                mask_path = os.path.join(masks_dir, mask_file)
                if os.path.exists(mask_path):
                    self.image_files.append(file)
        
        logger.info(f"æ‰¾åˆ° {len(self.image_files)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    def apply_id_mapping(self, mask):
        """åº”ç”¨IDæ˜ å°„è½¬æ¢"""
        mapped_mask = np.zeros_like(mask)
        for original_id, new_id in config.ID_MAPPING.items():
            mapped_mask[mask == original_id] = new_id
        
        # æœªçŸ¥IDæ˜ å°„ä¸ºèƒŒæ™¯
        unknown_mask = np.ones_like(mask, dtype=bool)
        for original_id in config.ID_MAPPING.keys():
            unknown_mask &= (mask != original_id)
        mapped_mask[unknown_mask] = 0
        
        return mapped_mask
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_file = self.image_files[idx]
        image_path = os.path.join(self.images_dir, image_file)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # åŠ è½½æ©ç 
        mask_file = image_file.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_path = os.path.join(self.masks_dir, mask_file)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        # åº”ç”¨IDæ˜ å°„
        mask = self.apply_id_mapping(mask)
        
        # è°ƒæ•´å°ºå¯¸
        image = cv2.resize(image, (config.IMAGE_SIZE, config.IMAGE_SIZE))
        mask = cv2.resize(mask, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)
        
        # è½¬æ¢ä¸ºtensor
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        mask = torch.from_numpy(mask).long()
        
        # æ ‡å‡†åŒ–
        mean = torch.tensor(config.PIXEL_MEAN).view(3, 1, 1) / 255.0
        std = torch.tensor(config.PIXEL_STD).view(3, 1, 1) / 255.0
        image = (image - mean) / std
        
        return image, mask, image_file

def load_model(model_path):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    try:
        # å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹ç±»
        sys.path.append('.')
        from train_sobel_optimized import EnhancedSAMModel
        from segment_anything import sam_model_registry
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(model_path, map_location=config.DEVICE)
        
        # åˆ›å»ºSAMæ¨¡å‹
        sam = sam_model_registry["vit_b"](checkpoint="models/sam_vit_b_01ec64.pth")
        sam.to(config.DEVICE)
        
        # åˆ›å»ºå¢å¼ºæ¨¡å‹
        model = EnhancedSAMModel(sam, config.NUM_CLASSES)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(config.DEVICE)
        model.eval()
        
        logger.info("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def run_inference(model, dataset, metrics):
    """è¿è¡Œæ¨ç†"""
    logger.info("å¼€å§‹æ¨ç†...")
    
    with torch.no_grad():
        for idx in tqdm(range(len(dataset)), desc="æ¨ç†è¿›åº¦"):
            image, mask, image_name = dataset[idx]
            
            # æ·»åŠ batchç»´åº¦
            image = image.unsqueeze(0).to(config.DEVICE)
            mask = mask.unsqueeze(0).to(config.DEVICE)
            
            # æ¨¡å‹æ¨ç†
            try:
                predictions, _ = model(image)
                metrics.update(predictions, mask, image_name)
            except Exception as e:
                logger.error(f"æ¨ç†å¤±è´¥ {image_name}: {e}")
                continue
    
    logger.info("æ¨ç†å®Œæˆï¼")

def generate_comprehensive_report(metrics_results, output_dir):
    """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
    logger.info("ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ç”ŸæˆJSONæŠ¥å‘Š
    json_report_path = os.path.join(output_dir, "comprehensive_test_report.json")
    with open(json_report_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_results, f, indent=2, ensure_ascii=False, default=str)
    
    # 2. ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
    readable_report_path = os.path.join(output_dir, "test_report_readable.txt")
    generate_readable_report(metrics_results, readable_report_path)
    
    # 3. ç”ŸæˆCSVè¯¦ç»†æ•°æ®
    csv_report_path = os.path.join(output_dir, "detailed_class_metrics.csv")
    generate_csv_report(metrics_results, csv_report_path)
    
    # 4. ç”Ÿæˆç—…ç¶è¯¦ç»†CSV
    lesion_csv_path = os.path.join(output_dir, "lesion_detailed_metrics.csv")
    generate_lesion_csv_report(metrics_results, lesion_csv_path)
    
    # 5. ç”Ÿæˆå›¾åƒçº§è¯¦ç»†CSV
    image_csv_path = os.path.join(output_dir, "image_level_metrics.csv")
    generate_image_csv_report(metrics_results, image_csv_path)
    
    logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {output_dir}")

def generate_readable_report(results, output_path):
    """ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("ğŸ”¥ SAMå£°å¸¦ç—…ç¶åˆ†å‰² - å…¨é¢æµ‹è¯•è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # åŸºç¡€ä¿¡æ¯
        f.write("ğŸ“Š æµ‹è¯•åŸºç¡€ä¿¡æ¯\n")
        f.write("-" * 40 + "\n")
        f.write(f"æµ‹è¯•å›¾åƒæ€»æ•°: {results['basic_metrics']['total_images']}\n")
        f.write(f"æ•´ä½“åƒç´ å‡†ç¡®ç‡: {results['basic_metrics']['pixel_accuracy']:.4f} ({results['basic_metrics']['pixel_accuracy']*100:.2f}%)\n\n")
        
        # æ•´ä½“mIoUå¯¹æ¯”
        f.write("ğŸ¯ æ•´ä½“mIoUå¯¹æ¯”åˆ†æ\n")
        f.write("-" * 40 + "\n")
        overall = results['overall_metrics']
        f.write(f"ä¼ ç»ŸmIoU (åŒ…å«æ‰€æœ‰ç±»åˆ«): {overall['traditional_mIoU']:.4f} ({overall['traditional_mIoU']*100:.2f}%)\n")
        f.write(f"ä¼˜åŒ–mIoU (åªè®¡ç®—å­˜åœ¨ç±»åˆ«): {overall['optimized_mIoU']:.4f} ({overall['optimized_mIoU']*100:.2f}%)\n")
        f.write(f"mIoUæå‡: {overall['improvement']['mIoU_improvement']:+.4f} ({overall['improvement']['mIoU_improvement']*100:+.2f}%)\n")
        f.write(f"å­˜åœ¨çš„ç±»åˆ«: {', '.join(overall['existing_classes'])}\n\n")
        
        # å„ç±»åˆ«è¯¦ç»†è¡¨ç°
        f.write("ğŸ” å„ç±»åˆ«è¯¦ç»†è¡¨ç°\n")
        f.write("-" * 40 + "\n")
        for class_name, metrics in results['class_metrics'].items():
            status = ""
            if class_name in config.LESION_NAMES:
                if metrics['IoU'] < 0.1:
                    status = "ğŸ˜° éœ€è¦å…³æ³¨"
                elif metrics['IoU'] < 0.3:
                    status = "ğŸ˜ æœ‰å¾…æé«˜"
                elif metrics['IoU'] < 0.5:
                    status = "ğŸ˜Š ä¸é”™"
                else:
                    status = "ğŸ”¥ ä¼˜ç§€"
            else:
                if metrics['IoU'] < 0.5:
                    status = "ğŸ˜ ä¸€èˆ¬"
                elif metrics['IoU'] < 0.7:
                    status = "ğŸ˜Š ä¸é”™"
                else:
                    status = "ğŸ”¥ ä¼˜ç§€"
            
            f.write(f"{class_name}: IoU={metrics['IoU']:.4f} | F1={metrics['F1']:.4f} | å‡ºç°åœ¨{metrics['present_in_images']}å¼ å›¾åƒä¸­ {status}\n")
        f.write("\n")
        
        # ç—…ç¶ä¸“é¡¹åˆ†æ
        f.write("ğŸ¥ ç—…ç¶ä¸“é¡¹åˆ†æ\n")
        f.write("-" * 40 + "\n")
        lesion_metrics = results['lesion_metrics']
        f.write(f"ç—…ç¶æ•´ä½“mIoU (ä¼˜åŒ–): {lesion_metrics['overall_lesion_metrics']['lesion_mIoU_optimized']:.4f} ({lesion_metrics['overall_lesion_metrics']['lesion_mIoU_optimized']*100:.2f}%)\n")
        f.write(f"ç—…ç¶æ•´ä½“æ£€æµ‹ç‡: {lesion_metrics['overall_lesion_metrics']['overall_detection_rate']:.4f} ({lesion_metrics['overall_lesion_metrics']['overall_detection_rate']*100:.2f}%)\n\n")
        
        f.write("å„ç—…ç¶è¯¦ç»†è¡¨ç°:\n")
        for lesion_name, metrics in lesion_metrics['individual_lesions'].items():
            f.write(f"  {lesion_name}:\n")
            f.write(f"    IoU: {metrics['IoU']:.4f} ({metrics['IoU']*100:.2f}%)\n")
            f.write(f"    æ£€æµ‹ç‡: {metrics['detection_rate']:.4f} ({metrics['detection_rate']*100:.2f}%)\n")
            f.write(f"    çœŸå®å­˜åœ¨: {metrics['ground_truth_count']}æ¬¡ | æˆåŠŸæ£€æµ‹: {metrics['detected_count']}æ¬¡\n")
        f.write("\n")
        
        # ç—…ç¶å¤§å°æ•æ„Ÿæ€§åˆ†æ
        f.write("ğŸ“ ç—…ç¶å¤§å°æ•æ„Ÿæ€§åˆ†æ\n")
        f.write("-" * 40 + "\n")
        size_analysis = lesion_metrics['size_sensitivity_analysis']
        f.write(f"å°ç—…ç¶ (<1000åƒç´ ): {size_analysis['small']['count']}ä¸ª, å¹³å‡IoU={size_analysis['small']['avg_iou']:.4f}\n")
        f.write(f"ä¸­ç­‰ç—…ç¶ (1000-5000åƒç´ ): {size_analysis['medium']['count']}ä¸ª, å¹³å‡IoU={size_analysis['medium']['avg_iou']:.4f}\n")
        f.write(f"å¤§ç—…ç¶ (>5000åƒç´ ): {size_analysis['large']['count']}ä¸ª, å¹³å‡IoU={size_analysis['large']['avg_iou']:.4f}\n\n")
        
        # å¤šç—…ç¶å›¾åƒåˆ†æ
        f.write("ğŸ”¬ å¤šç—…ç¶å›¾åƒåˆ†æ\n")
        f.write("-" * 40 + "\n")
        multi_lesion = results['multi_lesion_analysis']
        f.write(f"æ— ç—…ç¶å›¾åƒ: {multi_lesion['no_lesion_images']['count']}å¼  ({multi_lesion['no_lesion_images']['percentage']:.1f}%), å¹³å‡mIoU={multi_lesion['no_lesion_images']['avg_miou']:.4f}\n")
        f.write(f"å•ç—…ç¶å›¾åƒ: {multi_lesion['single_lesion_images']['count']}å¼  ({multi_lesion['single_lesion_images']['percentage']:.1f}%), å¹³å‡mIoU={multi_lesion['single_lesion_images']['avg_miou']:.4f}\n")
        f.write(f"å¤šç—…ç¶å›¾åƒ: {multi_lesion['multi_lesion_images']['count']}å¼  ({multi_lesion['multi_lesion_images']['percentage']:.1f}%), å¹³å‡mIoU={multi_lesion['multi_lesion_images']['avg_miou']:.4f}\n\n")
        
        # å›¾åƒçº§ç»Ÿè®¡
        f.write("ğŸ“ˆ å›¾åƒçº§ç»Ÿè®¡åˆ†æ\n")
        f.write("-" * 40 + "\n")
        image_stats = results['image_level_analysis']
        f.write(f"å¹³å‡å›¾åƒmIoU: {image_stats['avg_image_miou']:.4f} Â± {image_stats['std_image_miou']:.4f}\n")
        f.write(f"æœ€ä½³å›¾åƒmIoU: {image_stats['max_image_miou']:.4f}\n")
        f.write(f"æœ€å·®å›¾åƒmIoU: {image_stats['min_image_miou']:.4f}\n")
        f.write(f"ä¸­ä½æ•°mIoU: {image_stats['median_image_miou']:.4f}\n\n")
        
        # æ€»ç»“å»ºè®®
        f.write("ğŸ’¡ æ€»ç»“ä¸å»ºè®®\n")
        f.write("-" * 40 + "\n")
        
        # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
        optimized_miou = overall['optimized_mIoU']
        lesion_miou = lesion_metrics['overall_lesion_metrics']['lesion_mIoU_optimized']
        
        if optimized_miou > 0.8:
            f.write("âœ… æ¨¡å‹æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼\n")
        elif optimized_miou > 0.6:
            f.write("ğŸ˜Š æ¨¡å‹æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œæœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´ã€‚\n")
        else:
            f.write("ğŸ˜ æ¨¡å‹æ•´ä½“è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´ç­–ç•¥ã€‚\n")
        
        if lesion_miou > 0.5:
            f.write("âœ… ç—…ç¶åˆ†å‰²èƒ½åŠ›å¼ºï¼\n")
        elif lesion_miou > 0.3:
            f.write("ğŸ˜Š ç—…ç¶åˆ†å‰²èƒ½åŠ›ä¸­ç­‰ï¼Œå¯è€ƒè™‘å¢åŠ ç—…ç¶æ ·æœ¬æƒé‡ã€‚\n")
        else:
            f.write("ğŸ˜° ç—…ç¶åˆ†å‰²èƒ½åŠ›è¾ƒå¼±ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–ç—…ç¶æ£€æµ‹ç­–ç•¥ã€‚\n")

def generate_csv_report(results, output_path):
    """ç”ŸæˆCSVè¯¦ç»†æŠ¥å‘Š"""
    # å‡†å¤‡æ•°æ®
    data = []
    for class_name, metrics in results['class_metrics'].items():
        row = {
            'ç±»åˆ«': class_name,
            'IoU': f"{metrics['IoU']:.4f}",
            'Precision': f"{metrics['Precision']:.4f}",
            'Recall': f"{metrics['Recall']:.4f}",
            'F1': f"{metrics['F1']:.4f}",
            'å‡ºç°å›¾åƒæ•°': metrics['present_in_images'],
            'å‡ºç°é¢‘ç‡': f"{metrics['image_frequency']:.4f}",
            'æ˜¯å¦ç—…ç¶': 'æ˜¯' if class_name in config.LESION_NAMES else 'å¦'
        }
        data.append(row)
    
    # ä¿å­˜CSV
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

def generate_lesion_csv_report(results, output_path):
    """ç”Ÿæˆç—…ç¶è¯¦ç»†CSVæŠ¥å‘Š"""
    lesion_data = []
    
    # ç—…ç¶æ•´ä½“æŒ‡æ ‡
    overall_metrics = results['lesion_metrics']['overall_lesion_metrics']
    lesion_data.append({
        'æŒ‡æ ‡ç±»å‹': 'ç—…ç¶æ•´ä½“',
        'ç—…ç¶åç§°': 'æ‰€æœ‰ç—…ç¶',
        'IoU': f"{overall_metrics['lesion_mIoU_optimized']:.4f}",
        'F1': f"{overall_metrics['lesion_mF1_optimized']:.4f}",
        'æ£€æµ‹ç‡': f"{overall_metrics['overall_detection_rate']:.4f}",
        'çœŸå®å­˜åœ¨æ¬¡æ•°': '-',
        'æˆåŠŸæ£€æµ‹æ¬¡æ•°': '-',
        'å¤‡æ³¨': 'ä¼˜åŒ–ç‰ˆæœ¬(åªè®¡ç®—å­˜åœ¨çš„ç—…ç¶)'
    })
    
    # å„ç—…ç¶è¯¦ç»†æŒ‡æ ‡
    for lesion_name, metrics in results['lesion_metrics']['individual_lesions'].items():
        lesion_data.append({
            'æŒ‡æ ‡ç±»å‹': 'å•ä¸ªç—…ç¶',
            'ç—…ç¶åç§°': lesion_name,
            'IoU': f"{metrics['IoU']:.4f}",
            'F1': f"{metrics['F1']:.4f}",
            'æ£€æµ‹ç‡': f"{metrics['detection_rate']:.4f}",
            'çœŸå®å­˜åœ¨æ¬¡æ•°': metrics['ground_truth_count'],
            'æˆåŠŸæ£€æµ‹æ¬¡æ•°': metrics['detected_count'],
            'å¤‡æ³¨': f"æ£€æµ‹æˆåŠŸç‡: {metrics['detected_count']}/{metrics['ground_truth_count']}"
        })
    
    # ç—…ç¶å¤§å°æ•æ„Ÿæ€§åˆ†æ
    size_analysis = results['lesion_metrics']['size_sensitivity_analysis']
    for size_name, data in [('å°ç—…ç¶(<1000åƒç´ )', 'small'), ('ä¸­ç­‰ç—…ç¶(1000-5000åƒç´ )', 'medium'), ('å¤§ç—…ç¶(>5000åƒç´ )', 'large')]:
        lesion_data.append({
            'æŒ‡æ ‡ç±»å‹': 'å¤§å°æ•æ„Ÿæ€§',
            'ç—…ç¶åç§°': size_name,
            'IoU': f"{size_analysis[data]['avg_iou']:.4f}",
            'F1': '-',
            'æ£€æµ‹ç‡': '-',
            'çœŸå®å­˜åœ¨æ¬¡æ•°': size_analysis[data]['count'],
            'æˆåŠŸæ£€æµ‹æ¬¡æ•°': '-',
            'å¤‡æ³¨': f"æ ‡å‡†å·®: {size_analysis[data]['std_iou']:.4f}"
        })
    
    # ä¿å­˜CSV
    df = pd.DataFrame(lesion_data)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

def generate_image_csv_report(results, output_path):
    """ç”Ÿæˆå›¾åƒçº§è¯¦ç»†CSVæŠ¥å‘Š"""
    image_data = []
    
    # å›¾åƒçº§ç»Ÿè®¡
    image_stats = results['image_level_analysis']
    image_data.append({
        'ç»Ÿè®¡ç±»å‹': 'å›¾åƒçº§æ•´ä½“',
        'æŒ‡æ ‡åç§°': 'å¹³å‡mIoU',
        'æ•°å€¼': f"{image_stats['avg_image_miou']:.4f}",
        'æ ‡å‡†å·®': f"{image_stats['std_image_miou']:.4f}",
        'å¤‡æ³¨': 'æ‰€æœ‰å›¾åƒmIoUçš„å¹³å‡å€¼'
    })
    
    image_data.append({
        'ç»Ÿè®¡ç±»å‹': 'å›¾åƒçº§æ•´ä½“',
        'æŒ‡æ ‡åç§°': 'æœ€ä½³mIoU',
        'æ•°å€¼': f"{image_stats['max_image_miou']:.4f}",
        'æ ‡å‡†å·®': '-',
        'å¤‡æ³¨': 'å•å¼ å›¾åƒçš„æœ€é«˜mIoU'
    })
    
    image_data.append({
        'ç»Ÿè®¡ç±»å‹': 'å›¾åƒçº§æ•´ä½“',
        'æŒ‡æ ‡åç§°': 'æœ€å·®mIoU',
        'æ•°å€¼': f"{image_stats['min_image_miou']:.4f}",
        'æ ‡å‡†å·®': '-',
        'å¤‡æ³¨': 'å•å¼ å›¾åƒçš„æœ€ä½mIoU'
    })
    
    image_data.append({
        'ç»Ÿè®¡ç±»å‹': 'å›¾åƒçº§æ•´ä½“',
        'æŒ‡æ ‡åç§°': 'ä¸­ä½æ•°mIoU',
        'æ•°å€¼': f"{image_stats['median_image_miou']:.4f}",
        'æ ‡å‡†å·®': '-',
        'å¤‡æ³¨': 'æ‰€æœ‰å›¾åƒmIoUçš„ä¸­ä½æ•°'
    })
    
    # å¤šç—…ç¶åˆ†æ
    multi_lesion = results['multi_lesion_analysis']
    for category_en, category_cn in [('no_lesion_images', 'æ— ç—…ç¶å›¾åƒ'), ('single_lesion_images', 'å•ç—…ç¶å›¾åƒ'), ('multi_lesion_images', 'å¤šç—…ç¶å›¾åƒ')]:
        data = multi_lesion[category_en]
        image_data.append({
            'ç»Ÿè®¡ç±»å‹': 'å¤šç—…ç¶åˆ†æ',
            'æŒ‡æ ‡åç§°': category_cn,
            'æ•°å€¼': f"{data['avg_miou']:.4f}",
            'æ ‡å‡†å·®': f"{data['std_miou']:.4f}",
            'å¤‡æ³¨': f"å›¾åƒæ•°é‡: {data['count']}å¼  ({data['percentage']:.1f}%)"
        })
    
    # ä¿å­˜CSV
    df = pd.DataFrame(image_data)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”¥ å¼€å§‹å…¨é¢æµ‹è¯•è¯„ä¼°")
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(config.TEST_IMAGES_DIR):
        logger.error(f"æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨: {config.TEST_IMAGES_DIR}")
        return
    
    if not os.path.exists(config.TEST_MASKS_DIR):
        logger.error(f"æµ‹è¯•æ©ç ç›®å½•ä¸å­˜åœ¨: {config.TEST_MASKS_DIR}")
        return
    
    if not os.path.exists(config.MODEL_PATH):
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {config.MODEL_PATH}")
        return
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(config.RESULTS_DIR, exist_ok=True)
    
    # åŠ è½½æ•°æ®é›†
    dataset = TestDataset(config.TEST_IMAGES_DIR, config.TEST_MASKS_DIR)
    
    # åŠ è½½æ¨¡å‹
    model = load_model(config.MODEL_PATH)
    if model is None:
        return
    
    # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
    metrics = ComprehensiveMetrics()
    
    # è¿è¡Œæ¨ç†
    run_inference(model, dataset, metrics)
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    results = metrics.compute_final_metrics()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_comprehensive_report(results, config.RESULTS_DIR)
    
    logger.info("ğŸ‰ æµ‹è¯•è¯„ä¼°å®Œæˆï¼")
    
    # æ‰“å°ç®€è¦ç»“æœ
    logger.info("=" * 60)
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœç®€è¦:")
    logger.info(f"  æµ‹è¯•å›¾åƒ: {results['basic_metrics']['total_images']}å¼ ")
    logger.info(f"  ä¼˜åŒ–mIoU: {results['overall_metrics']['optimized_mIoU']:.4f} ({results['overall_metrics']['optimized_mIoU']*100:.1f}%)")
    logger.info(f"  ç—…ç¶mIoU: {results['lesion_metrics']['overall_lesion_metrics']['lesion_mIoU_optimized']:.4f} ({results['lesion_metrics']['overall_lesion_metrics']['lesion_mIoU_optimized']*100:.1f}%)")
    logger.info(f"  ç—…ç¶æ£€æµ‹ç‡: {results['lesion_metrics']['overall_lesion_metrics']['overall_detection_rate']:.4f} ({results['lesion_metrics']['overall_lesion_metrics']['overall_detection_rate']*100:.1f}%)")
    logger.info(f"  è¯¦ç»†æŠ¥å‘Š: {config.RESULTS_DIR}")
    logger.info("=" * 60)

if __name__ == "__main__":
    main() 