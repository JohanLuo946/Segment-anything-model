#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import os
import sys
import time
import json
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_fscore_support
import logging
from tqdm import tqdm
import gc
from collections import defaultdict, Counter

# å†…å­˜ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.cuda.empty_cache()
gc.collect()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class DSCEnhancedConfig:
    
    LESION_ID = 29  
    LESION_NAME = "å£°å¸¦ç™½æ–‘"  
    LESION_CODE = "sdbb"  
    
    TRAIN_IMAGES_DIR = "/root/autodl-tmp/SAM/sdbb/train/images"
    TRAIN_MASKS_DIR = "/root/autodl-tmp/SAM/sdbb/train/masks"
    VAL_IMAGES_DIR = "/root/autodl-tmp/SAM/sdbb/val/images"
    VAL_MASKS_DIR = "/root/autodl-tmp/SAM/sdbb/val/masks"
    SAM_MODEL_PATH = "/root/autodl-tmp/SAM/pre_models/sam_vit_b_01ec64.pth"  
    RESULTS_DIR = f"/root/autodl-tmp/SAM/results/models/dsc_enhanced_{LESION_CODE}_4"  
    
    NUM_CLASSES = 2
    IMAGE_SIZE = 1024
    BATCH_SIZE = 2      
    NUM_WORKERS = 8
    
    ID_MAPPING = {
        0: 0,          
        LESION_ID: 1,   
    }
    
    CLASS_NAMES = [
        "èƒŒæ™¯",              # 0
        LESION_NAME,        # 1
    ]
    
    INITIAL_CLASS_WEIGHTS = [
        0.6,   # èƒŒæ™¯æƒé‡å¤§å¹…å¢åŠ  
        2.0,   # ç—…ç¶æƒé‡æ¿€è¿›é™ä½ 
    ]
    DYNAMIC_WEIGHT_UPDATE = True
    
    NUM_EPOCHS = 120         
    LEARNING_RATE = 1e-3    
    WEIGHT_DECAY = 1e-4
    GRADIENT_ACCUMULATION_STEPS = 3  
    
    # SAMé…ç½®
    SAM_MODEL_TYPE = "vit_b"
    PIXEL_MEAN = [123.675, 116.28, 103.53]
    PIXEL_STD = [58.395, 57.12, 57.375]
    
    # è®¾å¤‡é…ç½®
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MIXED_PRECISION = True
    
    USE_ENHANCED_DICE_LOSS = False      
    USE_FOCAL_LOSS = True               
    USE_LABEL_SMOOTHING = False         
    USE_EDGE_LOSS = False               
    USE_BOUNDARY_LOSS = True            
    USE_HAUSDORFF_LOSS = True           
    USE_MULTI_SCALE_DICE = False        
    USE_WEIGHTED_SAMPLING = True        
    USE_BOUNDARY_AWARE_DICE = True      
    USE_TVERSKY_LOSS = True             
    
    CE_WEIGHT = 0.4                     # äº¤å‰ç†µæŸå¤±
    FOCAL_WEIGHT = 0.1                  # FocalæŸå¤± 
    DICE_WEIGHT = 0.15                  # æ ‡å‡†DiceæŸå¤±  
    BOUNDARY_AWARE_DICE_WEIGHT = 0.25   # è¾¹ç•Œæ„ŸçŸ¥DiceæŸå¤± 
    TVERSKY_WEIGHT = 0.4                # TverskyæŸå¤±  
    BOUNDARY_LOSS_WEIGHT = 0.15         # è¾¹ç•Œè·ç¦»æŸå¤± 
    HAUSDORFF_LOSS_WEIGHT = 0.1         # Hausdorffè·ç¦»æŸå¤± 
    
    # Tversky Loss å‚æ•°  
    TVERSKY_ALPHA = 0.4                 # FPæƒé‡
    TVERSKY_BETA = 0.6                  # FNæƒé‡
    
    # Focal Loss å‚æ•° 
    FOCAL_ALPHA = [
        0.3,    # èƒŒæ™¯ 
        1.8,    # ç—…ç¶ 
    ]
    FOCAL_GAMMA = 1.5                   # è¿›ä¸€æ­¥é™ä½gamma å‡å°‘éš¾æ ·æœ¬èšç„¦
    
    # æ¿€è¿›å¹³è¡¡é‡‡æ ·ç­–ç•¥
    LESION_OVERSAMPLE_FACTOR = 1.5      # ç—…ç¶æœ€å°è¿‡é‡‡æ ·
    BACKGROUND_UNDERSAMPLE_FACTOR = 0.5  # èƒŒæ™¯é‡‡æ ·å¤§å¹…å¢åŠ 
    
    # å¤šå°ºåº¦è®­ç»ƒå‚æ•° (å·²å…³é—­ï¼Œä½†ä¿ç•™é…ç½®)
    MULTI_SCALE_SIZES = [768, 896, 1024]
    SCALE_CHANGE_FREQUENCY = 10
    
    # æ˜¾å­˜ä¼˜åŒ–
    CLEAR_CACHE_EVERY = 2
    PIN_MEMORY = True
    
    # åå¤„ç†ä¼˜åŒ–å‚æ•° - é™ä½FP
    USE_MORPHOLOGICAL_POSTPROCESS = True    # å¯ç”¨å½¢æ€å­¦åå¤„ç†
    MIN_LESION_SIZE = 50                    # æœ€å°ç—…ç¶åŒºåŸŸå¤§å°(åƒç´ )
    BOUNDARY_SMOOTHING = True               # è¾¹ç•Œå¹³æ»‘
    
    # ä¿å­˜å’Œè¯„ä¼°
    SAVE_EVERY = 25
    EVAL_EVERY = 1
    EARLY_STOPPING_PATIENCE = 15

config = DSCEnhancedConfig()

# ===== ğŸ¯ DSCä¼˜åŒ–æ•°æ®é›†ç±» =====
class DSCEnhancedDataset(Dataset):
    """DSCå¢å¼ºæ•°æ®é›† - ä¿æŒåŸæœ‰2ç±»ç»“æ„"""
    
    def __init__(self, images_dir, masks_dir, is_train=True, current_epoch=0):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.is_train = is_train
        self.current_epoch = current_epoch
        
        # è·å–å›¾åƒæ–‡ä»¶
        self.image_files = []
        for file in os.listdir(images_dir):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
                mask_path = os.path.join(masks_dir, mask_file)
                if os.path.exists(mask_path):
                    self.image_files.append(file)
        
        logger.info(f"æ‰¾åˆ° {len(self.image_files)} ä¸ªå›¾åƒ-æ©ç å¯¹")
        
        # è¿‡æ»¤æ•°æ®é›†ï¼Œåªä¿ç•™åŒ…å«ç›®æ ‡ç—…ç¶çš„å›¾åƒ
        self.filter_lesion_only()
        
        if is_train and config.USE_WEIGHTED_SAMPLING:
            self.analyze_sample_distribution()
    
    def filter_lesion_only(self):
        """è¿‡æ»¤æ•°æ®é›†ï¼Œåªä¿ç•™åŒ…å«æŒ‡å®šLESION_IDçš„å›¾åƒï¼Œå¹¶ç»Ÿè®¡æ•°é‡"""
        filtered_files = []
        lesion_areas = []
        
        for file in tqdm(self.image_files, desc=f"è¿‡æ»¤æ•°æ®é›†ï¼ˆåªä¿ç•™å«ID={config.LESION_ID}çš„å›¾åƒï¼‰"):
            mask_file = file.replace('.jpg', '.png').replace('.jpeg', '.png')
            mask_path = os.path.join(self.masks_dir, mask_file)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
            if mask is not None and config.LESION_ID in np.unique(mask):
                filtered_files.append(file)
                # ç»Ÿè®¡ç—…ç¶é¢ç§¯
                lesion_area = np.sum(mask == config.LESION_ID)
                lesion_areas.append(lesion_area)
        
        original_count = len(self.image_files)
        self.image_files = filtered_files
        self.lesion_areas = lesion_areas
        filtered_count = len(self.image_files)
        
        logger.info(f"è¿‡æ»¤å‰æ•°æ®é›†æ•°é‡: {original_count}")
        logger.info(f"è¿‡æ»¤åæ•°æ®é›†æ•°é‡ï¼ˆå«ID={config.LESION_ID}ï¼‰: {filtered_count}")
        
        if lesion_areas:
            logger.info(f"ç—…ç¶é¢ç§¯ç»Ÿè®¡: æœ€å°={min(lesion_areas)}, æœ€å¤§={max(lesion_areas)}, å¹³å‡={np.mean(lesion_areas):.0f}")
    
    def analyze_sample_distribution(self):
        """åŸºäºç—…ç¶é¢ç§¯åˆ†ææ ·æœ¬åˆ†å¸ƒï¼Œä¸ºDSCä¼˜åŒ–è°ƒæ•´é‡‡æ ·æƒé‡"""
        logger.info("åˆ†ææ ·æœ¬åˆ†å¸ƒï¼ˆDSCä¼˜åŒ–ï¼‰...")
        
        self.sample_weights = []
        
        # æ ¹æ®ç—…ç¶é¢ç§¯åˆ†é…æƒé‡
        lesion_areas = np.array(self.lesion_areas)
        
        # è®¡ç®—é¢ç§¯åˆ†ä½æ•°
        q25 = np.percentile(lesion_areas, 25)
        q75 = np.percentile(lesion_areas, 75)
        
        small_lesion_count = 0
        medium_lesion_count = 0
        large_lesion_count = 0
        
        for area in lesion_areas:
            if area <= q25:
                # å°ç—…ç¶æƒé‡æœ€é«˜ï¼ˆDSCæ›´éš¾ï¼‰
                weight = config.LESION_OVERSAMPLE_FACTOR * 2.0
                small_lesion_count += 1
            elif area <= q75:
                # ä¸­ç­‰ç—…ç¶æ ‡å‡†æƒé‡
                weight = config.LESION_OVERSAMPLE_FACTOR
                medium_lesion_count += 1
            else:
                # å¤§ç—…ç¶æƒé‡ç¨ä½
                weight = config.LESION_OVERSAMPLE_FACTOR * 0.7
                large_lesion_count += 1
            
            self.sample_weights.append(weight)
        
        logger.info(f"å°ç—…ç¶æ ·æœ¬: {small_lesion_count} ä¸ª (æƒé‡x2.0)")
        logger.info(f"ä¸­ç­‰ç—…ç¶æ ·æœ¬: {medium_lesion_count} ä¸ª (æ ‡å‡†æƒé‡)")
        logger.info(f"å¤§ç—…ç¶æ ·æœ¬: {large_lesion_count} ä¸ª (æƒé‡x0.7)")
    
    def get_weighted_sampler(self):
        if hasattr(self, 'sample_weights'):
            return WeightedRandomSampler(
                weights=self.sample_weights,
                num_samples=len(self.sample_weights),
                replacement=True
            )
        return None
    
    def apply_id_mapping(self, mask):
        mapped_mask = np.zeros_like(mask)
        for original_id, new_id in config.ID_MAPPING.items():
            mapped_mask[mask == original_id] = new_id
        
        # å…¶ä»–IDæ˜ å°„ä¸ºèƒŒæ™¯
        unknown_mask = np.ones_like(mask, dtype=bool)
        for original_id in config.ID_MAPPING.keys():
            unknown_mask &= (mask != original_id)
        mapped_mask[unknown_mask] = 0
        
        return mapped_mask
    
    def smart_augmentation(self, image, mask):
        """è¾¹ç•Œç²¾ç¡®çš„æ•°æ®å¢å¼º - é™ä½FP"""
        if not self.is_train:
            return image, mask
        
        unique_ids = np.unique(mask)
        has_lesion = 1 in unique_ids
        
        if has_lesion:
            # æä¿å®ˆå¢å¼ºç­–ç•¥ - åŸºäºHD=52.26pxä¸¥é‡è¾¹ç•Œé—®é¢˜
            if random.random() < 0.2:  # è¿›ä¸€æ­¥é™ä½å¢å¼ºæ¦‚ç‡ (0.3â†’0.2)
                angle = random.uniform(-3, 3)  # æå°æ—‹è½¬è§’åº¦ (5â†’3)
                h, w = image.shape[:2]
                center = (w//2, h//2)
                M = cv2.getRotationMatrix2D(center, angle, 1.0)
                image = cv2.warpAffine(image, M, (w, h))
                mask = cv2.warpAffine(mask, M, (w, h), flags=cv2.INTER_NEAREST)
            
            if random.random() < 0.1:  # è¿›ä¸€æ­¥é™ä½äº®åº¦è°ƒæ•´æ¦‚ç‡ (0.2â†’0.1)
                # æè½»å¾®çš„äº®åº¦è°ƒæ•´
                factor = random.uniform(0.99, 1.01)  # è¿›ä¸€æ­¥ç¼©å°èŒƒå›´
                image = np.clip(image * factor, 0, 255).astype(np.uint8)
            
            # å¼ºåŒ–è¾¹ç•Œæ”¶ç¼©å¢å¼º - ä¸“æ²»è¾¹ç•Œä¸ç²¾ç¡®é—®é¢˜
            if random.random() < 0.25:  # å¢åŠ è¾¹ç•Œæ”¶ç¼©æ¦‚ç‡ (0.15â†’0.25)
                # è½»å¾®è…èš€æ“ä½œï¼Œå¼ºåŒ–è¾¹ç•Œç²¾ç¡®æ€§å­¦ä¹ 
                kernel = np.ones((3,3), np.uint8)
                mask_eroded = cv2.erode(mask.astype(np.uint8), kernel, iterations=1)
                mask = mask_eroded.astype(mask.dtype)
        
        return image, mask
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        current_size = config.IMAGE_SIZE
        
        image_file = self.image_files[idx]
        image_path = os.path.join(self.images_dir, image_file)
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        mask_file = image_file.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_path = os.path.join(self.masks_dir, mask_file)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        mask = self.apply_id_mapping(mask)
        
        image, mask = self.smart_augmentation(image, mask)
        
        image = cv2.resize(image, (current_size, current_size))
        mask = cv2.resize(mask, (current_size, current_size), interpolation=cv2.INTER_NEAREST)
        
        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
        mask = torch.from_numpy(mask).long()
        
        mean = torch.tensor(config.PIXEL_MEAN).view(3, 1, 1) / 255.0
        std = torch.tensor(config.PIXEL_STD).view(3, 1, 1) / 255.0
        image = (image - mean) / std
        
        return image, mask, image_file

# ===== ğŸ”¥ DSCä¸“ç”¨å¢å¼ºæŸå¤±å‡½æ•° =====
class DSCEnhancedLoss(nn.Module):
    """DSCä¸“ç”¨å¢å¼ºæŸå¤±å‡½æ•° - åŸºäºåŸæœ‰ç»“æ„ä¼˜åŒ–"""
    
    def __init__(self, class_weights=None, focal_alpha=None, focal_gamma=2.0):
        super().__init__()
        self.class_weights = class_weights
        self.focal_alpha = focal_alpha
        self.focal_gamma = focal_gamma
        
        if class_weights is not None:
            self.ce_loss = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).float())
        else:
            self.ce_loss = nn.CrossEntropyLoss()
        
        # è¾¹ç¼˜æ£€æµ‹å·ç§¯æ ¸ (ä¿ç•™ï¼Œå› ä¸ºè¾¹ç•Œæ„ŸçŸ¥Diceéœ€è¦)
        self.register_buffer('sobel_x', torch.tensor([
            [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]
        ], dtype=torch.float32).unsqueeze(0))
        
        self.register_buffer('sobel_y', torch.tensor([
            [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]
        ], dtype=torch.float32).unsqueeze(0))
        
        logger.info("DSCå¢å¼ºæŸå¤±å‡½æ•°è£…é…å®Œæ¯•ï¼ğŸ”¥")
    
    def standard_dice_loss(self, predictions, targets):
        """æ ‡å‡†DiceæŸå¤±"""
        smooth = 1e-6
        predictions = torch.softmax(predictions, dim=1)
        
        dice_loss = 0
        for class_idx in range(config.NUM_CLASSES):
            pred_class = predictions[:, class_idx]
            target_class = (targets == class_idx).float()
            
            intersection = (pred_class * target_class).sum()
            union = pred_class.sum() + target_class.sum()
            
            dice = (2 * intersection + smooth) / (union + smooth)
            dice_loss += 1 - dice
        
        return dice_loss / config.NUM_CLASSES
    
    def tversky_loss(self, predictions, targets):
        """Tversky Loss - æƒ©ç½šFPæ›´å¤š"""
        smooth = 1e-6
        predictions = torch.softmax(predictions, dim=1)
        
        tversky_loss = 0
        for class_idx in range(config.NUM_CLASSES):
            pred_class = predictions[:, class_idx]
            target_class = (targets == class_idx).float()
            
            # é€æ ·æœ¬è®¡ç®—Tversky
            batch_size = pred_class.size(0)
            sample_tversky_losses = []
            
            for b in range(batch_size):
                pred_sample = pred_class[b]
                target_sample = target_class[b]
                
                TP = (pred_sample * target_sample).sum()
                FP = ((1 - target_sample) * pred_sample).sum()
                FN = (target_sample * (1 - pred_sample)).sum()
                
                tversky = (TP + smooth) / (TP + config.TVERSKY_ALPHA * FP + config.TVERSKY_BETA * FN + smooth)
                sample_loss = 1 - tversky
                
                sample_tversky_losses.append(sample_loss)
            
            if sample_tversky_losses:
                class_tversky_loss = torch.stack(sample_tversky_losses).mean()
                tversky_loss += class_tversky_loss
        
        return tversky_loss / config.NUM_CLASSES
    
    def boundary_aware_dice_loss(self, predictions, targets):
        """è¾¹ç•Œæ„ŸçŸ¥DiceæŸå¤±"""
        predictions_soft = torch.softmax(predictions, dim=1)
        boundary_dice_loss = 0
        
        for class_idx in [1]:  # åªå¯¹ç—…ç¶ç±»åˆ«è®¡ç®—
            pred_class = predictions_soft[:, class_idx]
            target_class = (targets == class_idx).float()
            
            if target_class.sum() > 10:  # åªå¯¹è¶³å¤Ÿå¤§çš„ç›®æ ‡è®¡ç®—
                # æ£€æµ‹è¾¹ç•ŒåŒºåŸŸ
                target_edges = self.sobel_edge_detection(target_class)
                
                # è†¨èƒ€è¾¹ç•ŒåŒºåŸŸè·å¾—è¾¹ç•Œå¸¦
                kernel = torch.ones(5, 5, device=target_edges.device).unsqueeze(0).unsqueeze(0)
                boundary_region = F.conv2d(target_edges.unsqueeze(1), kernel, padding=2) > 0
                boundary_region = boundary_region.squeeze(1).float()
                
                if boundary_region.sum() > 0:
                    # è¾¹ç•ŒåŒºåŸŸçš„DiceæŸå¤±
                    boundary_pred = pred_class * boundary_region
                    boundary_target = target_class * boundary_region
                    
                    intersection = (boundary_pred * boundary_target).sum()
                    union = boundary_pred.sum() + boundary_target.sum()
                    
                    if union > 0:
                        boundary_dice = (2 * intersection + 1e-6) / (union + 1e-6)
                        boundary_dice_loss += 1 - boundary_dice
        
        return boundary_dice_loss
    
    def boundary_distance_loss(self, predictions, targets):
        """è¾¹ç•Œè·ç¦»æŸå¤± - ç›´æ¥ä¼˜åŒ–è¾¹ç•Œç²¾ç¡®æ€§"""
        predictions_soft = torch.softmax(predictions, dim=1)
        boundary_loss = 0
        
        for class_idx in [1]:  # åªå¯¹ç—…ç¶ç±»åˆ«è®¡ç®—
            pred_class = predictions_soft[:, class_idx]
            target_class = (targets == class_idx).float()
            
            if target_class.sum() > 10:  # åªå¯¹è¶³å¤Ÿå¤§çš„ç›®æ ‡è®¡ç®—
                # è·å–é¢„æµ‹å’ŒçœŸå®è¾¹ç•Œ
                pred_edges = self.sobel_edge_detection(pred_class)
                target_edges = self.sobel_edge_detection(target_class)
                
                # è®¡ç®—è¾¹ç•ŒåŒºåŸŸçš„L2è·ç¦»
                if pred_edges.sum() > 0 and target_edges.sum() > 0:
                    # è·ç¦»å˜æ¢
                    boundary_diff = torch.abs(pred_edges - target_edges)
                    boundary_loss += boundary_diff.mean()
        
        return boundary_loss
    
    def hausdorff_loss_approximation(self, predictions, targets):
        """Hausdorffè·ç¦»æŸå¤±çš„è¿‘ä¼¼å®ç°"""
        predictions_soft = torch.softmax(predictions, dim=1)
        hausdorff_loss = 0
        
        for class_idx in [1]:  # åªå¯¹ç—…ç¶ç±»åˆ«è®¡ç®—
            pred_class = predictions_soft[:, class_idx]
            target_class = (targets == class_idx).float()
            
            if target_class.sum() > 10:
                # è·å–è¾¹ç•Œç‚¹
                pred_edges = self.sobel_edge_detection(pred_class)
                target_edges = self.sobel_edge_detection(target_class)
                
                if pred_edges.sum() > 0 and target_edges.sum() > 0:
                    # ä½¿ç”¨å·ç§¯æ“ä½œè¿‘ä¼¼è®¡ç®—Hausdorffè·ç¦»
                    # åˆ›å»ºè·ç¦»æ ¸
                    kernel_size = 9
                    kernel = torch.ones(1, 1, kernel_size, kernel_size, device=pred_edges.device)
                    kernel = kernel / kernel.sum()
                    
                    # è®¡ç®—è¾¹ç•ŒåŒºåŸŸçš„æœ€å¤§è·ç¦»
                    pred_dilated = F.conv2d(pred_edges.unsqueeze(1), kernel, padding=kernel_size//2)
                    target_dilated = F.conv2d(target_edges.unsqueeze(1), kernel, padding=kernel_size//2)
                    
                    # è¿‘ä¼¼Hausdorffè·ç¦» 
                    max_dist = torch.max(torch.abs(pred_dilated - target_dilated))
                    hausdorff_loss += max_dist
        
        return hausdorff_loss
    
    def sobel_edge_detection(self, mask):
        """Sobelè¾¹ç¼˜æ£€æµ‹"""
        if len(mask.shape) == 3:
            mask = mask.unsqueeze(1).float()
        elif len(mask.shape) == 2:
            mask = mask.unsqueeze(0).unsqueeze(0).float()
        
        edge_x = F.conv2d(mask, self.sobel_x, padding=1)
        edge_y = F.conv2d(mask, self.sobel_y, padding=1)
        
        edge_magnitude = torch.sqrt(edge_x**2 + edge_y**2)
        edge_binary = (edge_magnitude > 0.1).float()
        
        return edge_binary.squeeze(1)
    
    def focal_loss(self, predictions, targets):
        """Focal Loss"""
        ce_loss = F.cross_entropy(predictions, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        if self.focal_alpha is not None:
            alpha_t = torch.tensor(self.focal_alpha).to(predictions.device)[targets]
        else:
            alpha_t = 1.0
        
        focal_loss = alpha_t * (1 - pt) ** self.focal_gamma * ce_loss
        return focal_loss.mean()
    
    def forward(self, predictions, targets):
        loss_dict = {}
        total_loss = 0
        
        # 1. äº¤å‰ç†µæŸå¤±
        if config.USE_LABEL_SMOOTHING:
            ce_loss = F.cross_entropy(predictions, targets, label_smoothing=0.1)
        else:
            ce_loss = self.ce_loss(predictions, targets)
        loss_dict['ce_loss'] = ce_loss.item()
        total_loss += config.CE_WEIGHT * ce_loss
        
        # 2. FocalæŸå¤±
        if config.USE_FOCAL_LOSS:
            focal_loss = self.focal_loss(predictions, targets)
            loss_dict['focal_loss'] = focal_loss.item()
            total_loss += config.FOCAL_WEIGHT * focal_loss
        
        # 3. æ ‡å‡†DiceæŸå¤±
        dice_loss = self.standard_dice_loss(predictions, targets)
        loss_dict['dice_loss'] = dice_loss.item()
        total_loss += config.DICE_WEIGHT * dice_loss
        
        # 6. è¾¹ç•Œæ„ŸçŸ¥DiceæŸå¤±
        if config.USE_BOUNDARY_AWARE_DICE:
            boundary_aware_dice = self.boundary_aware_dice_loss(predictions, targets)
            loss_dict['boundary_aware_dice'] = boundary_aware_dice.item()
            total_loss += config.BOUNDARY_AWARE_DICE_WEIGHT * boundary_aware_dice
        
        # 7. TverskyæŸå¤±
        if config.USE_TVERSKY_LOSS:
            tversky = self.tversky_loss(predictions, targets)
            loss_dict['tversky_loss'] = tversky.item()
            total_loss += config.TVERSKY_WEIGHT * tversky
        
        # 8. è¾¹ç•Œè·ç¦»æŸå¤± 
        if config.USE_BOUNDARY_LOSS:
            boundary_loss = self.boundary_distance_loss(predictions, targets)
            loss_dict['boundary_loss'] = boundary_loss.item()
            total_loss += config.BOUNDARY_LOSS_WEIGHT * boundary_loss
        
        # 9. Hausdorffè·ç¦»æŸå¤± 
        if config.USE_HAUSDORFF_LOSS:
            hausdorff_loss = self.hausdorff_loss_approximation(predictions, targets)
            loss_dict['hausdorff_loss'] = hausdorff_loss.item()
            total_loss += config.HAUSDORFF_LOSS_WEIGHT * hausdorff_loss
        
        # å…³é—­çš„æŸå¤±è®¾ç½®ä¸º0
        loss_dict['enhanced_dice'] = 0.0
        loss_dict['multi_scale_dice'] = 0.0
        loss_dict['edge_loss'] = 0.0
        
        return total_loss, loss_dict

# ===== ğŸš€ DSCå¢å¼ºSAMæ¨¡å‹ =====
class DSCEnhancedSAMModel(nn.Module):
    """DSCå¢å¼ºSAMæ¨¡å‹ - åŸºäºåŸæœ‰æ¶æ„ä¼˜åŒ–"""
    
    def __init__(self, sam_model, num_classes):
        super().__init__()
        self.sam = sam_model
        self.num_classes = num_classes
        
        # å¢å¼ºçš„åˆ†å‰²å¤´ - ä¸ºDSCä¼˜åŒ–
        self.segmentation_head = nn.Sequential(
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1),  # æ·»åŠ dropouté˜²è¿‡æ‹Ÿåˆ
            
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.05),
            
            # æ·»åŠ æ®‹å·®è¿æ¥
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(32, num_classes, kernel_size=1)
        )
        
        # å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 16, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # å¢å¼ºè¾¹ç•Œç»†åŒ–æ¨¡å— - ä¸“é—¨å¯¹æŠ—è¿‡åº¦åˆ†å‰²
        self.boundary_refine = nn.Sequential(
            nn.Conv2d(num_classes, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, num_classes, kernel_size=1),
            nn.Tanh()  # ä½¿ç”¨tanhé™åˆ¶è¾“å‡ºèŒƒå›´
        )
        
        # å¢å¼ºè¾¹ç•Œæ”¶ç¼©æ¨¡å— - ä¸“é—¨å¯¹æŠ—HD=52.26pxä¸¥é‡åç§»
        self.boundary_contract = nn.Sequential(
            nn.Conv2d(num_classes, 16, kernel_size=5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 8, kernel_size=3, padding=1),
            nn.Sigmoid(),
            nn.Conv2d(8, num_classes, kernel_size=1)
        )
        
        self.freeze_sam_components()
        
        logger.info("DSCå¢å¼ºSAMæ¨¡å‹æ”¹è£…å®Œæ¯•ï¼")
    
    def freeze_sam_components(self):
        # ç¨å¾®å‡å°‘å†»ç»“å±‚æ•°ï¼Œä¿ç•™æ›´å¤šå¯è®­ç»ƒå‚æ•°
        layers = list(self.sam.image_encoder.children())
        for i, layer in enumerate(layers[:-4]):  # å‡å°‘å†»ç»“å±‚
            for param in layer.parameters():
                param.requires_grad = False
        
        logger.info("SAMå‚æ•°éƒ¨åˆ†å†»ç»“å®Œæ¯•ï¼")
    
    def forward(self, images):
        batch_size = images.shape[0]
        
        image_embeddings = self.sam.image_encoder(images)
        
        attention_map = self.attention(image_embeddings)
        enhanced_features = image_embeddings * attention_map
        
        segmentation_logits = self.segmentation_head(enhanced_features)
        
        # è¾¹ç•Œç»†åŒ–å’Œæ”¶ç¼© - æ¿€è¿›ä¼˜åŒ–è¾¹ç•Œç²¾ç¡®æ€§
        boundary_refinement = self.boundary_refine(segmentation_logits)
        refined_logits = segmentation_logits + 0.06 * boundary_refinement  # è¿›ä¸€æ­¥é™ä½ç»†åŒ–æƒé‡
        
        # å¼ºåŒ–è¾¹ç•Œæ”¶ç¼© - å¯¹æŠ—HD=52.26pxä¸¥é‡åç§» 
        boundary_contraction = self.boundary_contract(refined_logits)
        refined_logits = refined_logits - 0.12 * boundary_contraction  # å¤§å¹…å¢åŠ æ”¶ç¼©åŠ›åº¦ (0.05â†’0.12)
        
        refined_logits = F.interpolate(
            refined_logits,
            size=(images.shape[2], images.shape[3]),
            mode="bilinear",
            align_corners=False,
        )
        
        iou_predictions = torch.ones(batch_size, 1).to(images.device) * 0.8
        
        return refined_logits, iou_predictions

# ===== ğŸ§  DSCä¸“ç”¨æŒ‡æ ‡è®¡ç®—å™¨ =====
class DSCEnhancedMetrics:
    """DSCä¸“ç”¨æŒ‡æ ‡è®¡ç®—å™¨ - é‡ç‚¹å…³æ³¨Diceåˆ†æ•°"""
    
    def __init__(self, num_classes, class_names):
        self.num_classes = num_classes
        self.class_names = class_names
        self.reset()
    
    def reset(self):
        self.class_ious = np.zeros(self.num_classes)
        self.class_dices = np.zeros(self.num_classes)
        self.class_f1s = np.zeros(self.num_classes)
        self.class_counts = np.zeros(self.num_classes)
        self.total_correct = 0
        self.total_pixels = 0
        
        # DSCä¸“ç”¨ç»Ÿè®¡
        self.dice_scores_per_sample = []
        self.lesion_sizes = []
        self.dice_by_size = {'small': [], 'medium': [], 'large': []}
        
        self.lesion_progress = {
            config.LESION_CODE: [],
        }
    
    def calculate_sample_dice(self, pred_mask, target_mask):
        """è®¡ç®—å•ä¸ªæ ·æœ¬çš„Diceåˆ†æ•°"""
        smooth = 1e-6
        intersection = (pred_mask * target_mask).sum().item()
        union = pred_mask.sum().item() + target_mask.sum().item()
        
        if union > 0:
            dice = (2 * intersection + smooth) / (union + smooth)
        else:
            dice = 1.0 if intersection == 0 else 0.0
        
        return dice
    
    def update(self, predictions, targets):
        predictions = torch.argmax(predictions, dim=1)
        
        self.total_correct += (predictions == targets).sum().item()
        self.total_pixels += targets.numel()
        
        batch_size = targets.size(0)
        
        # é€æ ·æœ¬è®¡ç®—Dice
        for b in range(batch_size):
            batch_pred = predictions[b]
            batch_target = targets[b]
            
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„Dice
            for class_idx in range(self.num_classes):
                pred_mask = (batch_pred == class_idx).float()
                target_mask = (batch_target == class_idx).float()
                
                intersection = (pred_mask * target_mask).sum().item()
                pred_sum = pred_mask.sum().item()
                target_sum = target_mask.sum().item()
                union = pred_sum + target_sum - intersection  # ä¿®å¤ï¼šä½¿ç”¨æ•°å­¦å…¬å¼è®¡ç®—å¹¶é›†
                
                if union > 0:
                    # IoU
                    iou = intersection / union
                    self.class_ious[class_idx] += iou
                    
                    # Dice
                    dice = self.calculate_sample_dice(pred_mask, target_mask)
                    self.class_dices[class_idx] += dice
                    
                    self.class_counts[class_idx] += 1
                    
                    # ç—…ç¶ä¸“é¡¹ç»Ÿè®¡
                    if class_idx == 1:  # ç—…ç¶ç±»åˆ«
                        self.dice_scores_per_sample.append(dice)
                        lesion_size = target_sum
                        self.lesion_sizes.append(lesion_size)
                        
                        # æŒ‰å°ºå¯¸åˆ†ç±»
                        if lesion_size < 500:
                            self.dice_by_size['small'].append(dice)
                        elif lesion_size < 2000:
                            self.dice_by_size['medium'].append(dice)
                        else:
                            self.dice_by_size['large'].append(dice)
                    
                    # F1è®¡ç®—
                    precision = intersection / (pred_sum + 1e-8)
                    recall = intersection / (target_sum + 1e-8)
                    f1 = 2 * precision * recall / (precision + recall + 1e-8)
                    self.class_f1s[class_idx] += f1
        
        # æ›´æ–°ç—…ç¶è¿›åº¦
        if self.class_counts[1] > 0:
            current_dice = self.class_dices[1] / self.class_counts[1]
            self.lesion_progress[config.LESION_CODE].append(current_dice)
    
    def compute(self):
        accuracy = self.total_correct / self.total_pixels if self.total_pixels > 0 else 0
        
        class_ious = {}
        class_dices = {}
        class_f1s = {}
        
        for i, name in enumerate(self.class_names):
            if self.class_counts[i] > 0:
                class_ious[name] = self.class_ious[i] / self.class_counts[i]
                class_dices[name] = self.class_dices[i] / self.class_counts[i]
                class_f1s[name] = self.class_f1s[i] / self.class_counts[i]
            else:
                class_ious[name] = 0.0
                class_dices[name] = 0.0
                class_f1s[name] = 0.0
        
        # è®¡ç®—å¹³å‡å€¼
        mean_iou = np.mean([class_ious[name] for name in self.class_names])
        mean_dice = np.mean([class_dices[name] for name in self.class_names])
        mean_f1 = np.mean([class_f1s[name] for name in self.class_names])
        
        # ç—…ç¶ä¸“é¡¹æŒ‡æ ‡
        lesion_name = config.LESION_NAME
        lesion_dice = class_dices.get(lesion_name, 0)
        lesion_iou = class_ious.get(lesion_name, 0)
        lesion_f1 = class_f1s.get(lesion_name, 0)
        
        # DSCè¯¦ç»†åˆ†æ
        dsc_analysis = {}
        if self.dice_scores_per_sample:
            scores = np.array(self.dice_scores_per_sample)
            dsc_analysis.update({
                'mean': np.mean(scores),
                'std': np.std(scores),
                'min': np.min(scores),
                'max': np.max(scores),
                'median': np.median(scores),
                'q25': np.percentile(scores, 25),
                'q75': np.percentile(scores, 75),
                'samples_above_0.8': np.sum(scores > 0.8),
                'samples_above_0.9': np.sum(scores > 0.9),
                'total_samples': len(scores)
            })
            
            # æŒ‰å°ºå¯¸åˆ†æDSC
            for size_cat, dice_scores in self.dice_by_size.items():
                if dice_scores:
                    dsc_analysis[f'{size_cat}_count'] = len(dice_scores)
                    dsc_analysis[f'{size_cat}_mean_dice'] = np.mean(dice_scores)
                    dsc_analysis[f'{size_cat}_std_dice'] = np.std(dice_scores)
        
        # ç—…ç¶è¿›åº¦æŠ¥å‘Š
        lesion_report = {}
        for lesion_code, progress in self.lesion_progress.items():
            if len(progress) >= 1:
                current_dice = progress[-1]
                best_dice = max(progress)
                
                # åˆ¤æ–­è¶‹åŠ¿
                if len(progress) >= 5:
                    recent_avg = np.mean(progress[-5:])
                    earlier_avg = np.mean(progress[-10:-5]) if len(progress) >= 10 else np.mean(progress[:-5])
                    trend = 'improving' if recent_avg > earlier_avg else 'stable'
                else:
                    trend = 'stable'
                
                lesion_report[lesion_code] = {
                    'current_dice': current_dice,
                    'best_dice': best_dice,
                    'trend': trend,
                    'improvement': current_dice - (progress[0] if progress else 0)
                }
        
        return {
            'accuracy': accuracy,
            'mIoU': mean_iou,
            'mDice': mean_dice,
            'mF1': mean_f1,
            
            'class_ious': class_ious,
            'class_dices': class_dices,
            'class_f1s': class_f1s,
            
            'lesion_dice': lesion_dice,
            'lesion_iou': lesion_iou,
            'lesion_f1': lesion_f1,
            
            'dsc_analysis': dsc_analysis,
            'lesion_report': lesion_report
        }

# ===== ğŸ® DSCå¢å¼ºè®­ç»ƒå™¨ =====
class DSCEnhancedTrainer:
    """DSCå¢å¼ºè®­ç»ƒå™¨ - åŸºäºåŸæœ‰æ¶æ„ä¼˜åŒ–"""
    
    def __init__(self):
        self.device = torch.device(config.DEVICE)
        self.scaler = torch.cuda.amp.GradScaler() if config.MIXED_PRECISION else None
        
        os.makedirs(config.RESULTS_DIR, exist_ok=True)
        os.makedirs(os.path.join(config.RESULTS_DIR, "models"), exist_ok=True)
        os.makedirs(os.path.join(config.RESULTS_DIR, "logs"), exist_ok=True)
        
        logger.info("DSCå¢å¼ºè®­ç»ƒå™¨å¯åŠ¨ï¼")
        
        self.best_dice = 0.0
        self.best_lesion_dice = 0.0
        self.patience_counter = 0
        self.current_weights = config.INITIAL_CLASS_WEIGHTS.copy()
        
        self.setup_model()
        self.setup_data()
        self.setup_training()
        
        self.history = {
            'train_loss': [], 'val_loss': [],
            'train_dice': [], 'val_dice': [],
            'lesion_dice': [], 'dsc_progress': []
        }
    
    def setup_model(self):
        logger.info("è£…é…DSCå¢å¼ºSAMæ¨¡å‹...")
        
        try:
            from segment_anything import sam_model_registry
            sam = sam_model_registry[config.SAM_MODEL_TYPE](checkpoint=config.SAM_MODEL_PATH)
            sam.to(self.device)
            
            self.model = DSCEnhancedSAMModel(sam, config.NUM_CLASSES)
            self.model.to(self.device)
            
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            logger.info(f"æ€»å‚æ•°: {total_params:,}")
            logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            
        except ImportError:
            os.system("pip install segment-anything")
            from segment_anything import sam_model_registry
            sam = sam_model_registry[config.SAM_MODEL_TYPE](checkpoint=config.SAM_MODEL_PATH)
            sam.to(self.device)
            self.model = DSCEnhancedSAMModel(sam, config.NUM_CLASSES)
            self.model.to(self.device)
    
    def setup_data(self):
        logger.info("å‡†å¤‡DSCå¢å¼ºæ•°æ®...")
        
        self.train_dataset = DSCEnhancedDataset(
            config.TRAIN_IMAGES_DIR,
            config.TRAIN_MASKS_DIR,
            is_train=True
        )
        
        self.val_dataset = DSCEnhancedDataset(
            config.VAL_IMAGES_DIR,
            config.VAL_MASKS_DIR,
            is_train=False
        )
        
        train_sampler = self.train_dataset.get_weighted_sampler()
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=config.BATCH_SIZE,
            sampler=train_sampler,
            num_workers=config.NUM_WORKERS,
            pin_memory=config.PIN_MEMORY,
            drop_last=True
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=config.BATCH_SIZE,
            shuffle=False,
            num_workers=config.NUM_WORKERS,
            pin_memory=config.PIN_MEMORY
        )
        
        logger.info(f"è®­ç»ƒæ ·æœ¬: {len(self.train_dataset)}")
        logger.info(f"éªŒè¯æ ·æœ¬: {len(self.val_dataset)}")
    
    def setup_training(self):
        logger.info("é…ç½®DSCå¢å¼ºè®­ç»ƒå‚æ•°...")
        
        # åˆ†å±‚å­¦ä¹ ç‡ - ä¼˜åŒ–DSC
        param_groups = [
            {'params': [p for n, p in self.model.sam.named_parameters() if p.requires_grad], 
             'lr': config.LEARNING_RATE * 0.1},
            {'params': self.model.segmentation_head.parameters(), 
             'lr': config.LEARNING_RATE},
            {'params': self.model.attention.parameters(), 
             'lr': config.LEARNING_RATE * 0.8},
            {'params': self.model.boundary_refine.parameters(), 
             'lr': config.LEARNING_RATE * 1.2}
        ]
        
        self.optimizer = optim.AdamW(
            param_groups,
            weight_decay=config.WEIGHT_DECAY
        )
        
        # ä½¿ç”¨CosineAnnealingWarmRestartsä¼˜åŒ–DSCæ”¶æ•›
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=20,
            T_mult=1,
            eta_min=1e-6
        )
        
        self.criterion = DSCEnhancedLoss(
            class_weights=self.current_weights,
            focal_alpha=config.FOCAL_ALPHA,
            focal_gamma=config.FOCAL_GAMMA
        )
        self.criterion.to(self.device)
        
        self.metrics = DSCEnhancedMetrics(config.NUM_CLASSES, config.CLASS_NAMES)
        
        logger.info("DSCå¢å¼ºè®­ç»ƒé…ç½®å®Œæ¯•ï¼")
    
    def dynamic_weight_adjustment(self, val_metrics):
        """åŸºäºDSCè¡¨ç°å’ŒFPæƒ…å†µåŠ¨æ€è°ƒæ•´æƒé‡"""
        if not config.DYNAMIC_WEIGHT_UPDATE:
            return
        
        lesion_dice = val_metrics.get('lesion_dice', 0)
        overall_dice = val_metrics.get('mDice', 0)
        
        # è·å–ç²¾ç¡®åº¦ä½œä¸ºFPæŒ‡æ ‡
        lesion_precision = 0.0
        if 'class_dices' in val_metrics and config.LESION_NAME in val_metrics['class_dices']:
            # å¦‚æœæœ‰è¯¦ç»†æŒ‡æ ‡ï¼Œä½¿ç”¨ç²¾ç¡®åº¦
            lesion_precision = val_metrics.get('lesion_precision', 0.8)  # é»˜è®¤å€¼
        
        # æ¿€è¿›æƒé‡è°ƒæ•´ç­–ç•¥ - åŸºäºHD=52.26pxä¸¥é‡è¾¹ç•Œé—®é¢˜
        if lesion_dice < 0.5:
            # DSCæä½æ—¶ï¼Œé€‚åº¦å¢åŠ ç—…ç¶æƒé‡
            self.current_weights[1] = min(self.current_weights[1] * 1.05, 2.5)  # è¿›ä¸€æ­¥é™ä½ä¸Šé™
            logger.info(f"ç—…ç¶DSCæä½({lesion_dice:.3f})ï¼Œå°å¹…å¢åŠ ç—…ç¶æƒé‡è‡³ {self.current_weights[1]:.2f}")
        elif lesion_dice > 0.7:  # é™ä½é˜ˆå€¼ï¼Œæ›´æ—©ä»‹å…¥
            # DSCä¸é”™æ—¶ï¼Œæ¿€è¿›é™ä½ç—…ç¶æƒé‡ä»¥æ”¹å–„è¾¹ç•Œ
            self.current_weights[1] = max(self.current_weights[1] * 0.92, 1.2)  # æ›´æ¿€è¿›é™ä½
            self.current_weights[0] = min(self.current_weights[0] * 1.08, 0.8)  # æ›´å¤§å¹…å¢åŠ èƒŒæ™¯æƒé‡
            logger.info(f"DSCè¾ƒå¥½({lesion_dice:.3f})ä½†è¾¹ç•Œé—®é¢˜ä¸¥é‡ï¼Œæ¿€è¿›å¹³è¡¡æƒé‡: èƒŒæ™¯={self.current_weights[0]:.2f}, ç—…ç¶={self.current_weights[1]:.2f}")
        elif lesion_dice > 0.8 and lesion_precision > 0.9:
            # è¡¨ç°ä¼˜ç§€æ—¶ï¼Œä¿æŒç¨³å®š
            logger.info(f"æ¨¡å‹è¡¨ç°ä¼˜ç§€(Dice:{lesion_dice:.3f}, ç²¾ç¡®åº¦:{lesion_precision:.3f})ï¼Œä¿æŒå½“å‰æƒé‡")
        
        # æ›´æ–°æŸå¤±å‡½æ•°æƒé‡
        self.criterion.class_weights = torch.tensor(self.current_weights).float().to(self.device)
        if hasattr(self.criterion, 'ce_loss'):
            self.criterion.ce_loss = nn.CrossEntropyLoss(
                weight=torch.tensor(self.current_weights).float().to(self.device)
            )
    
    def train_one_epoch(self, epoch):
        self.model.train()
        self.metrics.reset()
        
        running_loss = 0.0
        running_loss_dict = defaultdict(float)
        num_batches = 0
        
        self.train_dataset.current_epoch = epoch
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS} [DSCè®­ç»ƒ]")
        
        for batch_idx, (images, masks, filenames) in enumerate(pbar):
            images = images.to(self.device)
            masks = masks.to(self.device)
            
            if config.MIXED_PRECISION:
                with torch.cuda.amp.autocast():
                    predictions, _ = self.model(images)
                    loss, loss_dict = self.criterion(predictions, masks)
                    loss = loss / config.GRADIENT_ACCUMULATION_STEPS
                
                self.scaler.scale(loss).backward()
                
                if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                predictions, _ = self.model(images)
                loss, loss_dict = self.criterion(predictions, masks)
                loss = loss / config.GRADIENT_ACCUMULATION_STEPS
                
                loss.backward()
                
                if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
            
            self.metrics.update(predictions.detach(), masks)
            
            running_loss += loss.item()
            for key, value in loss_dict.items():
                running_loss_dict[key] += value
            num_batches += 1
            
            if num_batches > 0:
                avg_loss = running_loss / num_batches * config.GRADIENT_ACCUMULATION_STEPS
                pbar.set_postfix({
                    'loss': f'{avg_loss:.4f}',
                    'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                })
            
            if batch_idx % config.CLEAR_CACHE_EVERY == 0:
                torch.cuda.empty_cache()
                gc.collect()
        
        self.scheduler.step()
        
        epoch_metrics = self.metrics.compute()
        avg_loss = running_loss / num_batches * config.GRADIENT_ACCUMULATION_STEPS
        
        loss_breakdown = {}
        for key, value in running_loss_dict.items():
            loss_breakdown[key] = value / num_batches
        
        return avg_loss, epoch_metrics, loss_breakdown
    
    def validate_one_epoch(self, epoch):
        self.model.eval()
        self.metrics.reset()
        
        running_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.val_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS} [DSCéªŒè¯]")
        
        with torch.no_grad():
            for batch_idx, (images, masks, filenames) in enumerate(pbar):
                images = images.to(self.device)
                masks = masks.to(self.device)
                
                if config.MIXED_PRECISION:
                    with torch.cuda.amp.autocast():
                        predictions, _ = self.model(images)
                        loss, _ = self.criterion(predictions, masks)
                else:
                    predictions, _ = self.model(images)
                    loss, _ = self.criterion(predictions, masks)
                
                self.metrics.update(predictions, masks)
                running_loss += loss.item()
                num_batches += 1
                
                if num_batches > 0:
                    avg_loss = running_loss / num_batches
                    pbar.set_postfix({'val_loss': f'{avg_loss:.4f}'})
        
        epoch_metrics = self.metrics.compute()
        avg_loss = running_loss / num_batches if num_batches > 0 else 0
        
        return avg_loss, epoch_metrics
    
    def print_epoch_summary(self, epoch, train_loss, train_metrics, val_loss, val_metrics, loss_breakdown):
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ”¥ ç¬¬ {epoch+1} ä¸ªEpochæ€»ç»“ - DSCå¢å¼ºç‰ˆ:")
        logger.info(f"{'='*80}")
        
        logger.info(f"ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡:")
        logger.info(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")
        logger.info(f"  è®­ç»ƒmDice: {train_metrics['mDice']:.4f} | éªŒè¯mDice: {val_metrics['mDice']:.4f}")
        logger.info(f"  ç—…ç¶Dice: {val_metrics['lesion_dice']:.4f} | ç—…ç¶IoU: {val_metrics['lesion_iou']:.4f}")
        
        # DSCè¯¦ç»†åˆ†æ
        if val_metrics.get('dsc_analysis'):
            dsc = val_metrics['dsc_analysis']
            logger.info(f"ğŸ¯ DSCè¯¦ç»†åˆ†æ:")
            if 'mean' in dsc:
                logger.info(f"  å¹³å‡DSC: {dsc['mean']:.4f} Â± {dsc.get('std', 0):.4f}")
                logger.info(f"  DSCèŒƒå›´: [{dsc.get('min', 0):.4f}, {dsc.get('max', 0):.4f}]")
                logger.info(f"  ä¸­ä½æ•°: {dsc.get('median', 0):.4f} | Q25-Q75: [{dsc.get('q25', 0):.4f}, {dsc.get('q75', 0):.4f}]")
                
                total = dsc.get('total_samples', 0)
                above_08 = dsc.get('samples_above_0.8', 0)
                above_09 = dsc.get('samples_above_0.9', 0)
                if total > 0:
                    logger.info(f"  é«˜è´¨é‡æ ·æœ¬: DSC>0.8: {above_08}/{total} ({100*above_08/total:.1f}%)")
                    logger.info(f"  ä¼˜ç§€æ ·æœ¬: DSC>0.9: {above_09}/{total} ({100*above_09/total:.1f}%)")
            
            # æŒ‰ç—…ç¶å¤§å°åˆ†æ
            for size in ['small', 'medium', 'large']:
                if f'{size}_mean_dice' in dsc:
                    mean_dice = dsc[f'{size}_mean_dice']
                    count = dsc.get(f'{size}_count', 0)
                    std_dice = dsc.get(f'{size}_std_dice', 0)
                    logger.info(f"  {size}ç—…ç¶: DSC={mean_dice:.4f}Â±{std_dice:.4f} (n={count})")
        
        logger.info(f"ğŸ” æŸå¤±åˆ†è§£:")
        loss_names = ['ce_loss', 'focal_loss', 'dice_loss', 'boundary_aware_dice', 'tversky_loss', 'boundary_loss', 'hausdorff_loss']
        for loss_name in loss_names:
            if loss_name in loss_breakdown:
                logger.info(f"  {loss_name}: {loss_breakdown[loss_name]:.4f}")
        
        logger.info(f"ğŸ¥ å„ç±»åˆ«è¡¨ç°:")
        for class_name, dice in val_metrics['class_dices'].items():
            iou = val_metrics['class_ious'].get(class_name, 0)
            f1 = val_metrics['class_f1s'].get(class_name, 0)
            logger.info(f"  {class_name}: Dice={dice:.4f} | IoU={iou:.4f} | F1={f1:.4f}")
        
        if val_metrics.get('lesion_report'):
            logger.info(f"ğŸ“ˆ ç—…ç¶å­¦ä¹ è¿›åº¦:")
            for lesion_code, report in val_metrics['lesion_report'].items():
                trend_emoji = "ğŸ“ˆ" if report['trend'] == 'improving' else "ğŸ“Š"
                improvement = report.get('improvement', 0)
                logger.info(f"  {config.LESION_NAME}: å½“å‰={report['current_dice']:.4f} | æœ€ä½³={report['best_dice']:.4f} | æå‡=+{improvement:.4f} {trend_emoji}")
        
        current_lr = self.optimizer.param_groups[0]['lr']
        logger.info(f"ğŸ“š å­¦ä¹ ç‡: {current_lr:.2e} | ç±»åˆ«æƒé‡: {self.current_weights}")
        
        logger.info(f"{'='*80}\n")
    
    def save_checkpoint(self, epoch, val_metrics, is_best_dice=False, is_best_lesion_dice=False):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_dice': self.best_dice,
            'best_lesion_dice': self.best_lesion_dice,
            'metrics': val_metrics,
            'config': config.__dict__,
            'class_weights': self.current_weights
        }
        
        if (epoch + 1) % config.SAVE_EVERY == 0:
            checkpoint_path = os.path.join(config.RESULTS_DIR, "models", f"checkpoint_epoch_{epoch+1:03d}.pth")
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜ {checkpoint_path}")
        
        if is_best_dice:
            best_path = os.path.join(config.RESULTS_DIR, "models", "best_model_overall_dice.pth")
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ† æœ€ä½³æ•´ä½“DSCæ¨¡å‹å·²ä¿å­˜ï¼mDice: {val_metrics['mDice']:.4f}")
        
        if is_best_lesion_dice:
            best_lesion_path = os.path.join(config.RESULTS_DIR, "models", "best_model_lesion_dice.pth")
            torch.save(checkpoint, best_lesion_path)
            logger.info(f"ğŸ¯ æœ€ä½³ç—…ç¶DSCæ¨¡å‹å·²ä¿å­˜ï¼ç—…ç¶Dice: {val_metrics['lesion_dice']:.4f}")
    
    def save_training_plots(self):
        if not self.history['train_loss']:
            return
        
        plt.figure(figsize=(20, 15))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(2, 3, 1)
        plt.plot(self.history['train_loss'], label='è®­ç»ƒæŸå¤±', color='red', linewidth=2)
        plt.plot(self.history['val_loss'], label='éªŒè¯æŸå¤±', color='blue', linewidth=2)
        plt.title('æŸå¤±æ›²çº¿', fontsize=14)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # DSCæ›²çº¿
        plt.subplot(2, 3, 2)
        plt.plot(self.history['train_dice'], label='è®­ç»ƒmDice', color='red', linewidth=2)
        plt.plot(self.history['val_dice'], label='éªŒè¯mDice', color='blue', linewidth=2)
        plt.title('DSCæ›²çº¿', fontsize=14)
        plt.xlabel('Epoch')
        plt.ylabel('Dice Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ç—…ç¶DSCè¶‹åŠ¿
        plt.subplot(2, 3, 3)
        if self.history['lesion_dice']:
            plt.plot(self.history['lesion_dice'], color='green', linewidth=3)
            plt.title(f'{config.LESION_NAME} DSCè¶‹åŠ¿', fontsize=14)
            plt.xlabel('Validation Step')
            plt.ylabel('Lesion Dice Score')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ ç›®æ ‡çº¿
            plt.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='ç›®æ ‡: 0.8')
            plt.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='ä¼˜ç§€: 0.9')
            plt.legend()
        
        # DSCè¿›åº¦åˆ†æ
        plt.subplot(2, 3, 4)
        if self.history['dsc_progress']:
            # å–æœ€æ–°çš„DSCåˆ†å¸ƒ
            latest_dsc = self.history['dsc_progress'][-1] if self.history['dsc_progress'] else []
            if latest_dsc:
                plt.hist(latest_dsc, bins=25, alpha=0.7, color='skyblue', edgecolor='black')
                mean_dsc = np.mean(latest_dsc)
                plt.axvline(mean_dsc, color='red', linestyle='--', linewidth=2, label=f'å‡å€¼: {mean_dsc:.3f}')
                plt.axvline(0.8, color='orange', linestyle='--', alpha=0.7, label='ç›®æ ‡: 0.8')
                plt.title('æœ€æ–°DSCåˆ†å¸ƒ', fontsize=14)
                plt.xlabel('Dice Score')
                plt.ylabel('é¢‘æ¬¡')
                plt.legend()
                plt.grid(True, alpha=0.3)
        
        # DSCæ”¹è¿›è¶‹åŠ¿
        plt.subplot(2, 3, 5)
        if self.history['lesion_dice']:
            improvements = []
            baseline = self.history['lesion_dice'][0] if self.history['lesion_dice'] else 0
            for dice in self.history['lesion_dice']:
                improvements.append(dice - baseline)
            
            plt.plot(improvements, color='purple', linewidth=2)
            plt.title('DSCæ”¹è¿›è¶‹åŠ¿', fontsize=14)
            plt.xlabel('Validation Step')
            plt.ylabel('Dice Improvement')
            plt.grid(True, alpha=0.3)
            
            if improvements:
                plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)
                final_improvement = improvements[-1]
                plt.text(0.7*len(improvements), max(improvements)*0.8, 
                        f'æ€»æ”¹è¿›: +{final_improvement:.3f}', 
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        
        # æƒé‡å˜åŒ–ï¼ˆå¦‚æœæœ‰è®°å½•ï¼‰
        plt.subplot(2, 3, 6)
        plt.title('ç±»åˆ«æƒé‡å˜åŒ–', fontsize=14)
        plt.xlabel('Epoch')
        plt.ylabel('Weight')
        plt.grid(True, alpha=0.3)
        plt.text(0.5, 0.5, 'æƒé‡å˜åŒ–è®°å½•\n(å¾…å®ç°)', 
                transform=plt.gca().transAxes, ha='center', va='center',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.7))
        
        plt.tight_layout()
        plot_path = os.path.join(config.RESULTS_DIR, "dsc_enhanced_training_curves.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"DSCå¢å¼ºè®­ç»ƒæ›²çº¿å·²ä¿å­˜ {plot_path}")
    
    def train(self):
        logger.info("å¼€å§‹DSCå¢å¼ºè®­ç»ƒï¼")
        
        start_time = time.time()
        
        for epoch in range(config.NUM_EPOCHS):
            logger.info(f"\nğŸ”¥ ç¬¬ {epoch+1}/{config.NUM_EPOCHS} ä¸ªEpochå¼€å§‹ï¼")
            
            train_loss, train_metrics, loss_breakdown = self.train_one_epoch(epoch)
            
            if (epoch + 1) % config.EVAL_EVERY == 0:
                val_loss, val_metrics = self.validate_one_epoch(epoch)
                
                self.print_epoch_summary(epoch, train_loss, train_metrics, val_loss, val_metrics, loss_breakdown)
                
                # åŠ¨æ€æƒé‡è°ƒæ•´
                self.dynamic_weight_adjustment(val_metrics)
                
                # æ›´æ–°å†å²è®°å½•
                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)
                self.history['train_dice'].append(train_metrics['mDice'])
                self.history['val_dice'].append(val_metrics['mDice'])
                self.history['lesion_dice'].append(val_metrics['lesion_dice'])
                
                # è®°å½•DSCè¿›åº¦
                if hasattr(self.metrics, 'dice_scores_per_sample'):
                    self.history['dsc_progress'].append(self.metrics.dice_scores_per_sample.copy())
                
                # æ£€æŸ¥æœ€ä½³æ¨¡å‹
                current_dice = val_metrics['mDice']
                current_lesion_dice = val_metrics['lesion_dice']
                
                is_best_dice = current_dice > self.best_dice
                is_best_lesion_dice = current_lesion_dice > self.best_lesion_dice
                
                if is_best_dice:
                    self.best_dice = current_dice
                    self.patience_counter = 0
                elif is_best_lesion_dice:
                    self.best_lesion_dice = current_lesion_dice
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                
                self.save_checkpoint(epoch, val_metrics, is_best_dice, is_best_lesion_dice)
                
                # æ—©åœæ£€æŸ¥
                if self.patience_counter >= config.EARLY_STOPPING_PATIENCE:
                    logger.info("æ²¡æœ‰æ”¹å–„ï¼Œæå‰ç»“æŸè®­ç»ƒï¼")
                    break
            
            # å®šæœŸä¿å­˜å›¾è¡¨
            if (epoch + 1) % (config.SAVE_EVERY * 2) == 0:
                self.save_training_plots()
            
            torch.cuda.empty_cache()
            gc.collect()
        
        total_time = time.time() - start_time
        logger.info(f"\nğŸ‰ DSCå¢å¼ºè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
        logger.info(f"ğŸ† æœ€ä½³æ•´ä½“DSC: {self.best_dice:.4f}")
        logger.info(f"ğŸ¯ æœ€ä½³ç—…ç¶DSC: {self.best_lesion_dice:.4f}")
        
        # æœ€ç»ˆDSCåˆ†æ
        if self.history['lesion_dice']:
            initial_dice = self.history['lesion_dice'][0]
            final_dice = self.history['lesion_dice'][-1]
            total_improvement = final_dice - initial_dice
            logger.info(f"ğŸ“ˆ æ€»ä½“DSCæå‡: {initial_dice:.4f} â†’ {final_dice:.4f} (+{total_improvement:.4f})")
        
        self.save_training_plots()
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(config.RESULTS_DIR, "dsc_enhanced_training_history.json")
        with open(history_path, 'w', encoding='utf-8') as f:
            # å¤„ç†numpyæ•°ç»„
            history_save = {}
            for key, value in self.history.items():
                if key == 'dsc_progress':
                    history_save[key] = [list(v) if isinstance(v, (list, np.ndarray)) else v for v in value]
                else:
                    history_save[key] = value
            json.dump(history_save, f, indent=2, ensure_ascii=False)

def main():
    logger.info(f"ğŸš¨ SAM {config.LESION_NAME} DSCå¢å¼º2ç±»åˆ†å‰²è®­ç»ƒå¼€å§‹ï¼")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"è£…å¤‡ï¼š{gpu_name} ({gpu_memory:.1f}GB)")
    
    # æ£€æŸ¥è·¯å¾„
    paths_to_check = [
        config.TRAIN_IMAGES_DIR, config.TRAIN_MASKS_DIR,
        config.VAL_IMAGES_DIR, config.VAL_MASKS_DIR,
        config.SAM_MODEL_PATH
    ]
    
    for path in paths_to_check:
        if not os.path.exists(path):
            logger.error(f"è·¯å¾„ä¸å­˜åœ¨ {path}")
            sys.exit(1)
    
    logger.info("ğŸ”¥ è¾¹ç•Œç²¾ç¡®æ€§æ¿€è¿›ä¼˜åŒ–é…ç½®æ€»ç»“ (åŸºäºHD=52.26pxæµ‹è¯•ç»“æœ)ï¼š")
    logger.info(f"  ç›®æ ‡ç—…ç¶: {config.LESION_NAME} (ID: {config.LESION_ID})")
    logger.info(f"  ğŸš¨ æŸå¤±æƒé‡é‡æ–°åˆ†é…: CE={config.CE_WEIGHT}, Focal={config.FOCAL_WEIGHT}, Dice={config.DICE_WEIGHT}")
    logger.info(f"  ğŸ”¥ è¾¹ç•Œæ„ŸçŸ¥Diceæƒé‡: {config.BOUNDARY_AWARE_DICE_WEIGHT} (æ¿€è¿›å¢åŠ !)")
    logger.info(f"  ğŸ†• æ–°å¢æŸå¤±: è¾¹ç•Œè·ç¦»={config.BOUNDARY_LOSS_WEIGHT}, Hausdorff={config.HAUSDORFF_LOSS_WEIGHT}")
    logger.info(f"  âš–ï¸ Tverskyæƒé‡: {config.TVERSKY_WEIGHT} (alpha={config.TVERSKY_ALPHA}, beta={config.TVERSKY_BETA}) - æ¿€è¿›å¹³è¡¡FP/FN")
    logger.info(f"  ğŸ“Š ç±»åˆ«æƒé‡æ¿€è¿›å¹³è¡¡: èƒŒæ™¯={config.INITIAL_CLASS_WEIGHTS[0]}, ç—…ç¶={config.INITIAL_CLASS_WEIGHTS[1]}")
    logger.info(f"  ğŸ¯ é‡‡æ ·ç­–ç•¥æç«¯å¹³è¡¡: ç—…ç¶è¿‡é‡‡æ ·x{config.LESION_OVERSAMPLE_FACTOR}, èƒŒæ™¯æ¬ é‡‡æ ·x{config.BACKGROUND_UNDERSAMPLE_FACTOR}")
    logger.info(f"  âš¡ æ··åˆç²¾åº¦: {config.MIXED_PRECISION}")
    logger.info(f"  ğŸ¤– åŠ¨æ€æƒé‡è°ƒæ•´: {config.DYNAMIC_WEIGHT_UPDATE} - æ¿€è¿›è¾¹ç•Œä¼˜åŒ–ç­–ç•¥")
    logger.info(f"  ğŸ”§ åå¤„ç†ä¼˜åŒ–: å½¢æ€å­¦={config.USE_MORPHOLOGICAL_POSTPROCESS}, æœ€å°åŒºåŸŸ={config.MIN_LESION_SIZE}px")
    logger.info(f"  ğŸ“ˆ ä¼˜åŒ–ç›®æ ‡: Hausdorffè·ç¦» <15px (å½“å‰52.26px â†’ ç›®æ ‡æ”¹å–„70%+)")
    
    trainer = DSCEnhancedTrainer()
    trainer.train()

if __name__ == "__main__":
    main()